% Created 2017-05-31 Wed 08:02
% -*- latex-run-command: pdflatex -*-
\documentclass[presentation,10pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{mathtools}
\newtheorem{mydef}{Definition}
\newtheorem{mythm}{Theorem}
\newcommand{\dx}{\mathrm{d}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\pr}{\mathrm{Pr}}
\newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
\DeclareMathOperator*{\plim}{plim}
\newcommand{\plimn}{\plim_{n \rightarrow \infty}}
\usepackage{booktabs}
\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}
\usetheme{CambridgeUS}
\usecolortheme{beaver}
\author{Zheng Tian}
\date{}
\title{Lecture 11: Assessing Studies Based on Multiple Regression}
\hypersetup{
 pdfauthor={Zheng Tian},
 pdftitle={Lecture 11: Assessing Studies Based on Multiple Regression},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.1 (Org mode 9.0.7)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{frame}{Outline}
\setcounter{tocdepth}{1}
\tableofcontents
\end{frame}


\section{Internal and External Validity}
\label{sec:orgf5f65ae}
\setcounter{tocdepth}{1}
\tableofcontents[currentsection]
\begin{frame}[label={sec:orgecf9c5a}]{An over view of internal and external validity}
\begin{itemize}
\item The concepts of internal and external validity provide a general
framework for assessing whether a statistical or econometric study is
useful for answering a specific question of interest.
\end{itemize}

\vspace{0.1cm} 

\begin{itemize}
\item We focus on regression analysis that have the objective
of estimating the causal effect of a change in some independent
variable on a dependent variable.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgf98264a}]{The population and setting studied versus the population and setting of interest}
\begin{block}{The population and setting studied}
\begin{itemize}
\item The population studied is the population of entities-people,
companies, school districts, and so forth-from which the sample is
drawn.
\item The setting studied refers to as the institutional, legal, social,
and economic environment in which the population studied fits in and
the sample is drawn.
\end{itemize}
\end{block}

\begin{block}{The population and setting of interest}
\begin{itemize}
\item The population and setting of interest is the population
and setting of entities to which the causal inferences from the study
are to be applied.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:org9920855}]{Definition of internal and external validity}
\begin{block}{Internal validity}
The statistical inferences about causal effects are valid for the
population being studied.
\end{block}

\begin{block}{External validity}
The statistical inferences can be generalized from the population and
setting studied to other populations and settings of interest. 
\end{block}
\end{frame}

\begin{frame}[label={sec:org05e3142}]{Threats to internal validity}
\begin{block}{Internal validity consists of two components}
\begin{itemize}
\item The estimator of the causal effect should be unbiased and
consistent.
\item Hypothesis tests should have the desired significance level, and the
confidence intervals should have the desired confidence level.
\end{itemize}
\end{block}

\begin{block}{Internal validity in regression analysis}
\begin{enumerate}
\item the OLS estimator is unbiased and consistent, and
\item the standard errors are computed in the correct way that makes
confidence intervals have the desired confidence level.
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}[label={sec:org3a5804e}]{Threats to external validity}
\begin{block}{Differences in populations}
The causal effect may be different regarding different populations
\begin{itemize}
\item demographic and personal characteristics
\item geographic and climate features
\item timing
\end{itemize}
\end{block}

\begin{block}{Differences in settings}
\begin{itemize}
\item Difference in institutional environment, laws, or physical
environment.
\end{itemize}
\end{block}

\begin{block}{How to assess the external validity of a study}
\begin{itemize}
\item Use specific knowledge.
\item Case-by-case judgment.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:orgca16e2e}]{Threats to Internal Validity of Multiple Regression Analysis}
We introduce five threats to the internal validity of regression studies:
\begin{enumerate}
\item Omitted variable bias
\item Wrong functional form
\item Errors-in-variables bias
\item Sample selection bias
\item Simultaneous causality bias
\end{enumerate}

\vspace{0.3cm} 
All of these imply that \(E(u_i|X_{1i},…,X_{ki}) \neq 0\) so as to make
the OLS estimators biased and inconsistent.  
\end{frame}

\section{Omitted Variable Bias}
\label{sec:orgb2fba87}
\begin{frame}[label={sec:org0e67561}]{Omitted variable bias}
What are the two conditions for omitted variable bias?
\pause
\begin{enumerate}
\item At least one of the included regressors must be correlated with the
omitted variable.
\item The omitted variable must be a determinant of the dependent
variable, \(Y\).
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org6303ef6}]{Solutions to omitted variable bias when the variable is observed or there are adequate control variables}
\begin{itemize}
\item Include the omitted variables or the control variables
\begin{itemize}
\item Avoid the violation of the first least squares assumption, \(E(u |
    X ) = 0\) or to let the conditional mean independence assumption
hold, i.e., \(E(u|X, W) = E(u|X)\)
\end{itemize}
\end{itemize}

\vspace{0.2cm} 

\begin{itemize}
\item Adding an additional independent variable may reduce the precision of the
estimators of the coefficients 
\begin{itemize}
\item when the new variable actually does not belong to the population
regression function,
\item when the new variable is correlated with other regressors.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgf02618b}]{Solutions to omitted variable bias when the variable is observed or there are adequate control variables}
\begin{enumerate}
\item Identify the key coefficient(s) of interest.

\item \emph{a priori} reasoning: before analyzing data, you should consider
\begin{itemize}
\item What are the most likely sources of important omitted variable?
\item Answer the question using economic theory and expert knowledge.
\end{itemize}

\item Result in a base specification and a list of additional
questionable variables that might help mitigate possible omitted
variable bias.

\item Augment your base specification with the additional questionable
control variables.

\item Present an accurate summary of your results in tabular form.
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org567de93}]{Solutions to omitted variable bias when adequate control variables are not available}
\begin{itemize}
\item Panel data regression;
\item Instrumental variables regression;
\item Randomized controlled experiment.
\end{itemize}
\end{frame}

\section{Misspecification of the Functional Form}
\label{sec:orgd5dd68e}

\begin{frame}[label={sec:org1a93d5e}]{Misspecification of the functional form of the regression function}
\begin{itemize}
\item Functional form misspecification arises when the functional form of
the estimated regression function differs from the functional form of
the population regression function. 
\begin{itemize}
\item e.g., nonlinear vs. linear models
\end{itemize}
\end{itemize}

\vspace{0.1cm}

\begin{itemize}
\item Functional form misspecification bias can be considered as a type of
omitted variable bias, in which the omitted variables are the terms
that reflect the missing nonlinear aspects of the regression
function. 
\begin{itemize}
\item e.g., missing the quadratic term
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orge09da65}]{Solutions to functional form misspecification}
\begin{itemize}
\item Plotting the data and the estimated regression function.
\end{itemize}

\vspace{0.1cm}

\begin{itemize}
\item Use a different functional form.
\begin{itemize}
\item Continuous dependent variable:  use the “appropriate” nonlinear
specifications in X (logarithms, interactions, etc.)
\item Discrete (example: binary) dependent variable:  need an extension of
multiple regression methods (“probit” or “logit” analysis for binary
dependent variables)
\end{itemize}
\end{itemize}
\end{frame}

\section{Measurement Errors and Errors-in-Variable Bias}
\label{sec:org98d3394}

\begin{frame}[label={sec:org3c9e060}]{Measurement error and errors-in-variable bias}
Measurement errors often happen in practice. 
\begin{itemize}
\item respondents misstated answers to survey questions
\item typographical errors when data were entered into the database
\item the malfunctions of machines when recording data.
\end{itemize}

\vspace{0.1cm}

Measurement errors in
\begin{itemize}
\item dependent variable
\item independent variable \(\Rightarrow\) \alert{errors-in-variable bias}.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgf411da2}]{Definition of errors-in-variable bias}
\begin{itemize}
\item \alert{Errors-in-variables bias} in the OLS estimator arises when an
independent variable is measured imprecisely.
\end{itemize}

\vspace{0.3cm}

\begin{itemize}
\item This bias depends on the nature of the measurement error and persists
even if the sample size is large.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgf1bf6c7}]{Mathematical illustration of errors-in-variable bias}
\begin{itemize}
\item The population regression model is 
\[ Y_i = \beta_0 + \beta_1 X_i + u_i, \text{ where } E(u_i | X_i) =
  0 \text{ is satisfied}  \]

\item Suppose a regressor \(X_i\) is imprecisely measured by
\(\tilde{X}_i\).
\begin{itemize}
\item The measurement error is \(w_i = \tilde{X}_i - X_i\).
\item Assume \(E(w_i) = 0\) and \(\var(w_i) = \sigma^2_w\).
\end{itemize}

\item Rewrite the model in terms of \(\tilde{X}_i\),
\begin{equation}
\begin{split}
Y_i &= \beta_0 + \beta_1 \tilde{X}_i + [\beta_1 (X_i - \tilde{X}_i) + u_i] \\
    &= \beta_0 + \beta_1 \tilde{X}_i + v_i \label{eq:err-in-var}
\end{split}
\end{equation}
\begin{itemize}
\item The new error term is \(v_i = \beta_1(X_i - \tilde{X}_i) + u_i\)
\end{itemize}

\item If \(\cov(w_i, \tilde{X}_i) \neq 0\), then \(\cov(v_i, \tilde{X}_i)
  \neq 0\) and the OLS estimator \(\hat{\beta}_1\) is biased since
\(E(v_i | \tilde{X}_i) \neq 0\).
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orga496ca0}]{The biased and inconsistent OLS estimator with measurement errors}
\begin{itemize}
\item If \(\cov(w_i, \tilde{X}_i) \neq 0\), then \(\cov(v_i, \tilde{X}_i)
  \neq 0\) and the OLS estimator \(\hat{\beta}_1\) is biased since
\(E(v_i | \tilde{X}_i) \neq 0\).
\end{itemize}

\vspace{0.3cm}

\begin{itemize}
\item The OLS estimator is inconsistent.
\begin{itemize}
\item The precise size and direction of the bias in \(\hat{\beta}_1\) depend
on the correlation between \(\tilde{X}_i\) and the measurement error
\(w_i\). This correlation depends, in turn, on the specific nature of
the measurement error.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org4c90cf8}]{The classical measurement error model}
\begin{itemize}
\item The classical measurement error model assumes that the errors are
purely random. 
$$\corr(w_i, X_i) = 0 \text{ and }\corr(w_i, u_i) = 0$$

\item The errors are correlated with \(\tilde{X}_i\), that is,
\(\corr(\tilde{X}_i, w_i) \neq 0\).

\item In the classical measurement model, the OLS estimator
\(\hat{\beta}_1\) is inconsistent, and its the probability limit is
\begin{equation}
\label{eq:eiv-lim}
\hat{\beta}_1 \rarrowd{p} \frac{\sigma^2_X}{\sigma^2_X + \sigma^2_w}\beta_1
\end{equation}

\item Since \(\frac{\sigma^2_X}{\sigma^2_X + \sigma^2_w} < 1\), Equation
(\ref{eq:eiv-lim}) implies that
\(\hat{\beta}_1\) is biased toward 0.
\begin{itemize}
\item When \(\sigma^2_w\) is very large, then \(\hat{\beta}_1 \rarrowd{p} 0\);
\item When \(\sigma^2_w\) is very small, then \(\hat{\beta}_1 \rarrowd{p} \beta_1\).
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[label={sec:orgbc1fb09}]{Measurement error in Y}
The effect of measurement error in Y is different from that in
X. Generally, measurement in Y that has conditional mean zero given
the regressors will not induce bias in the OLS coefficients, but will
lead to inefficient estimators. 

\begin{itemize}
\item Suppose Y has the classical measurement error, that is, what we
observe, \(\tilde{Y}_i\), is the true value of \(Y_i\) plus a purely
random error \(w_i\). Then, the regression model is 
\[ \tilde{Y}_i = \beta_0 + \beta_1 X_i + v_i, \text{ where } v_i = w_i +
  u_i\]
\item If \(w_i\) and \(X_i\) are independently distributed so that \(E(w_i | X_i)
  = 0\), in which case \(E(v_i | X_i) = 0\), so \(\hat{\beta}_1\) is
unbiased.
\item Since \(\var(v_i) = \var(w_i) + \var(u_i) > \var(u_i)\), the variance
of \(\hat{\beta}_1\) is larger than it would be without measurement
error.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org4057f8b}]{Solutions to errors-in-variable bias}
\begin{itemize}
\item Get an accurate measure of \(X\) as possible as you can.
\item Use an instrumental variable that is correlated with the actual
value of \(X_i\) but is uncorrelated with the measurement error.
\item Develop a mathematical model of the measurement error and use the
resulting formula to adjust the estimates. This requires specific
knowledge of the errors.
\end{itemize}
\end{frame}

\section{Missing data and sample selection}
\label{sec:orgc3ed249}

\begin{frame}[label={sec:orgb2cec50}]{Missing data and sample selection}
Whether missing data pose a threat to internal validity depends on why
the data are missing. We consider three cases of missing data.

\begin{block}{Missing data at random}
When data are missing completely at random, unrelated with \(X\) and
\(Y\), then the effect is to reduce the sample size but not introduce
any estimation bias. 
\end{block}

\begin{block}{Missing data based on \(X\)}
When the data are missing based on the value of a regressor but
unrelated with generating \(Y\), the effect is also to reduce the sample
size but not introduce bias. 
\end{block}

\begin{block}{Sample selection bias}
When the data are missing because of a selection process that is
related with the value of the dependent variable \(Y\), beyond depending
on the regressors \(X\), then this selection process can introduce
correlation between the error term and the regressors, resulting in
\alert{sample selection bias}.
\end{block}
\end{frame}


\begin{frame}[label={sec:org48e4990}]{Sample selection bias: two examples}
The sample selection problem can be cast either as a consequence of
nonrandom sampling or as a missing data problem, illustrated using the
following two examples. 

\begin{block}{Nonrandom sampling: Height of undergraduates}
The professor of Statistics asks you to estimate the mean height of
undergraduate males. You collect your data (obtain your sample) by
standing outside the basketball team’s locker room and recording the
height of the undergraduates who enter.
\end{block}

\begin{block}{Missing data: Trade volume of pairs of countries}
\begin{itemize}
\item The amount of commodities that two countries can trade depends on
GDP of two countries, industrial structures, factor abundance,
etc.
\item We can get the data on trade volume between pairs of countries
from World Bank, Penn World Table, etc.
\item Sample selection bias occurs due to the non-trading pairs.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:orgabd2cad}]{Solutions to sample selection bias}
\begin{itemize}
\item Collect the sample in a way that avoids sample selection.
\item Heckman's two-step method.
\item Randomized controlled experiment.
\item Construct a model of the sample selection problem and estimate that
model.
\end{itemize}
\end{frame}

\section{Simultaneous causality}
\label{sec:orgbf32072}

\begin{frame}[label={sec:orga6f0a9a}]{Simultaneous causality}
Up to now, all we examined is how \(X\) can cause \(Y\). What if \(Y\) causes
\(X\)? If \(Y\) does cause \(X\) in some way, there is \alert{simultaneous
causality} problem, which lead to biased and inconsistent OLS
estimator. 

\vspace{0.3cm}
There are many examples of simultaneous causality in Economics. In the
paper of Acemuglou et al.(2000), \emph{The Colonial Origins of Comparative
Development: An Empirical Investigation}, the authors estimate the
effect of institutions on economic performance. However, the
simultaneous causality (or mutual causality) comes from the fact that
not only do good institutions promote economic performance, but also
countries with high GDP per capita can afford good institutions and
secure property rights, which in turn yield better economic
performance. 
\end{frame}

\begin{frame}[label={sec:org7de73b0}]{Simultaneous causality bias}
Simultaneous causality leads to biased estimates of the effect of \(X\)
on \(Y\), referred to as \alert{simultaneous causality bias}. We can express
the simultaneous causality using a simultaneous equations.
\begin{gather}
Y_i = \beta_0 + \beta_1 X_i + u_i \label{eq:sim-cau-1} \\
X_i = \gamma_0 + \gamma_1 Y_i + v_i \label{eq:sim-cau-2}
\end{gather}

Intuitively, simultaneous causality comes from the following facts. 
\begin{itemize}
\item Large \(u_i\) means large \(Y_i\), which implies large \(X_i\) (if
\(\gamma_1\) > 0).
\item This implies that \(u_i\) and \(X_i\) are correlated, i.e., \(\cov(X_i,
  u_i) \neq 0\).
\item Thus, the OLS estimator of \(\beta_1\) from merely estimating Equation
(\ref{eq:sim-cau-1}) is biased and inconsistent.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org663a255}]{Simultaneous causality bias (cont'd)}
Formally, we can prove that \(\cov(X_i, u_i) \neq 0\), resulting in the
bias in the OLS estimator of \(\beta_1\). 
\begin{proof}
\begin{align*}
\cov(X_i, u_i) &= \cov(\gamma_0 + \gamma_1 Y_i + v_i, u_i) \\
&= \gamma_1\cov(Y_i, u_i) + \cov(v_i, u_i) (\text{ Assuming } \cov(v_i, u_i)=0) \\
&= \gamma_1\cov(\beta_0 + \beta_1 X_i + u_i, u_i) \\
&= \gamma_1\cov(X_i, u_i) + \gamma_1\sigma^2_u
\end{align*}
Solving for $\cov(X_i, u_i)$ yields the result 
$\cov(X_i, u_i) = \gamma_1 \sigma^2_u /(1-\gamma_1\beta_1)$, which is not
equal to zero unless $\gamma_1 = 0$, i.e., the simultaneous causality does
exist. 
\end{proof}
\end{frame}

\begin{frame}[label={sec:org69b64f2}]{Solutions to simultaneous causality bias}
\begin{itemize}
\item Run a randomized controlled experiment.  Because \(X_i\) is chosen at
random by the experimenter, there is no feedback from the outcome
variable to \(Y_i\) (assuming perfect compliance).
\item Develop and estimate a complete model of both directions of
causality.  This is the idea behind many large macro models
(e.g. Federal Reserve Bank-US).  This is extremely difficult in
practice.
\item Use instrumental variables regression to estimate the causal effect
of interest (effect of X on Y, ignoring effect of Y on X)
\end{itemize}
\end{frame}

\section{Inconsistency of Standard Errors}
\label{sec:org282e567}

\begin{frame}[label={sec:org0c88974}]{Sources of inconsistency of OLS standard errors}
Inconsistent standard errors pose a different threat to internal
validity. Even if the OLS estimator is consistent and the sample is
large, inconsistent standard errors will produce hypothesis tests with
size that differs from the desired significance level and "95\%"
confidence intervals that fail to include the true value in 95\% of
repeated samples. 

\vspace{0.3cm}
There are two main reasons for inconsistent standard errors:
improperly handled heteroskedasticity and correlation of the error
term across observations.
\end{frame}

\begin{frame}[label={sec:orgaeb8fe8}]{Heteroskedasticity}
If the errors are heteroskedastic and you mistakenly use the
homoskedasticity-only standard errors that are reported by some
software by default, then the t-test and the F-test based on the wrong
standard errors do not have the desired size. 

\vspace{0.3cm}
The solution to this problem is to use heteroskedasticity-robust
standard errors of the OLS estimators and to construct t- and
F-statistics using a heteroskedasticity-robust variance estimator,
which is provided as an option in modern software packages. 
\end{frame}

\begin{frame}[shrink,label={sec:org05e6dfe}]{The Breusch-Pagan test for heteroskedasticity}
We can test whether heteroskedasticity exists in a regression model
using the Breusch-Pagan test. The test consist of the following steps: 
\begin{enumerate}
\item Estimate a regression model, \(Y = \beta_0 + \beta_1 X_1 + \cdots +
   \beta_k X_k + u\), and obtain the squared OLS residuals,
\(\hat{u}^2\).
\item Run a regression of \(\hat{u}^2 = \delta_0 + \delta_1 X_1 + \cdots +
   \delta_k X_k + v\), and obtain the \(R^2\) of this regression, denoted
as \(R^2_{\hat{u}^2}\).
\item Test the null hypothesis, \(H_0: E(u^2 | X_1, \ldots, X_k) =
   \sigma^2\), i.e., homoskedasticity, against the alternative
hypothesis for heteroskedasticity. The test statistics can be the
overall F statistics for the regression in the second step, which
is
\[ F = \frac{R^2_{\hat{u}^2}/k}{(1 - R^2_{\hat{u}^2})/(n-k-1)} \sim
   F(k, n-k-1)\]
Or we can compute an LM test statistics, which is
\[ LM = n R^2_{\hat{u}^2} \sim \chi^2(k) \]
where \(n\) is the number of observations.
\item Based on the F-statistic or the LM statistic, compute the
p-value. If the p-value is smaller than the significance level, we
can reject the null hypothesis of homoskedasticity.
\end{enumerate}
\end{frame}

\begin{frame}[label={sec:org7ffd06d}]{Correlation of the error term across observations}
In the lease squares assumptions, we assume that \((X_i, Y_i)\) for
\(i=1, \ldots, n\) are i.i.d., which implies that \(u_i\) are uncorrelated
across observations. However, in some setting, the population
regression error can be correlated across observations. There are
mainly two types of correlation in consideration: serial correlation
and spatial correlation. 

\begin{itemize}
\item Serial correlation arises from the repeated observations over the
same entity over time. It is a prevalent problem in time series
data.
\item Spatial correlation arises from the influence of contiguous
(neighboring) observations over geographic units.
\item The OLS estimator with serial correlation or spatial correlation is
still unbiased and consistent, but inference based on no correlation
assumption is not valid.
\end{itemize}
\end{frame}

\begin{frame}[label={sec:org78f3eb3}]{Solutions to correlation of the error term across observations}
\begin{itemize}
\item Use the \alert{heteroskedasticity-and-auto-correlation-consistent
standard errors (HAC)}. We will learn how to handle serial
correlation in time series data in the next two semesters.
\end{itemize}

\vspace{0.3cm}
\begin{itemize}
\item Model the spatial correlation specifically. Spatial econometrics
is a branch of econometrics that deals with spatial correlation.
\end{itemize}
\end{frame}
\end{document}