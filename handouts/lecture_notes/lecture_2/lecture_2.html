<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-02-27 Mon 10:36 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Lecture 2: Review of Probability</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Zheng Tian" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../../css/readtheorg.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Lecture 2: Review of Probability</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orge1d44b5">1. Random Variables and Probability Distributions</a>
<ul>
<li><a href="#org7ebadd5">1.1. Defining probabilities and random variables</a></li>
<li><a href="#orgc63a10a">1.2. Probability distributions</a></li>
</ul>
</li>
<li><a href="#org1cf58f2">2. Expectation, Variance, and Other Moments</a>
<ul>
<li><a href="#org4e8cc09">2.1. The expected value of a random variable</a></li>
<li><a href="#org507a13f">2.2. The variance and standard deviation</a></li>
<li><a href="#org799daff">2.3. Moments of a random variable, skewness and kurtosis</a></li>
</ul>
</li>
<li><a href="#orgc6d58c2">3. Two Random Variables</a>
<ul>
<li><a href="#orgf8b569c">3.1. The joint and marginal distributions</a></li>
<li><a href="#org0833375">3.2. Conditional distributions</a></li>
<li><a href="#orgfbc9529">3.3. Covariance and Correlation</a></li>
</ul>
</li>
<li><a href="#org9282a16">4. Four Specific Distributions</a>
<ul>
<li><a href="#orge34024a">4.1. The normal distribution</a></li>
<li><a href="#org643dfc7">4.2. The chi-squared distribution</a></li>
<li><a href="#org764b71e">4.3. The student t distribution</a></li>
<li><a href="#org5762eb8">4.4. The F distribution</a></li>
</ul>
</li>
<li><a href="#org70dacae">5. Random Sampling and the Distribution of the Sample Average</a>
<ul>
<li><a href="#orga98bab1">5.1. Random sampling</a></li>
<li><a href="#org6258289">5.2. The sampling distribution of the sample average</a></li>
</ul>
</li>
<li><a href="#org1be73ef">6. Large Sample Approximations to Sampling Distributions</a>
<ul>
<li><a href="#org1cc51bb">6.1. The exact distribution and the asymptotic distribution</a></li>
<li><a href="#orgfb3a54a">6.2. The law of large numbers</a></li>
<li><a href="#org115448e">6.3. The central limit theorem</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
This lecture will review the basics in probability theory. The review
is by no means comprehensive. We will just refresh our mind with the
concepts that will be used the lectures that follow.
</p>

<div id="outline-container-orge1d44b5" class="outline-2">
<h2 id="orge1d44b5"><span class="section-number-2">1</span> Random Variables and Probability Distributions</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-org7ebadd5" class="outline-3">
<h3 id="org7ebadd5"><span class="section-number-3">1.1</span> Defining probabilities and random variables</h3>
<div class="outline-text-3" id="text-1-1">
</div><div id="outline-container-org70071ba" class="outline-4">
<h4 id="org70071ba">Experiments, outcomes, sample space, and events</h4>
<div class="outline-text-4" id="text-org70071ba">
<p>
Probabilities are defined with respect to things whose occurrence are
random. We use the idea of an <b>experiment</b> to symbolize the processes
that generate random results. The <b>outcomes</b> of an experiment are its
mutually exclusive potential results. For example, a simple experiment
might be tossing a coin, the outcome of which is either getting a
head(H) or a tail(T) but not both.
</p>

<p>
We can denote all the outcomes from an experiment with a set \(S\), that
is called the <b>sample space</b>. In the tossing-coin experiment, the
sample space is \(\{H, T\}\). Or if we toss a dice, the sample space is
\(\{1, 2, 3, 4, 5, 6\}\). An <b>event</b> is a subset of the sample
space. Getting a head is an event, which is \(\{H\} \subset \{H, T\}\).
</p>
</div>
</div>

<div id="outline-container-org96df7d0" class="outline-4">
<h4 id="org96df7d0">Probability</h4>
<div class="outline-text-4" id="text-org96df7d0">
<p>
The <b>probability</b> of an event is the proportion of the time that the
event will occur in the long run. For example, we toss a coin for \(n\)
times and get \(m\) heads. When \(n\) is very large, we can say that the
probability of getting a head in a toss is \(m/n\). Obviously, we cannot
always repeat an experiment with infinite times. So we need a general
(axiomatic) definition of probability as follows.
</p>
</div>
</div>

<div id="outline-container-orgfd1df83" class="outline-4">
<h4 id="orgfd1df83">An axiomatic definition of probability</h4>
<div class="outline-text-4" id="text-orgfd1df83">
<p>
A probability of an event \(A\) in the sample space \(S\), denoted as
\(\mathrm{Pr}(A)\), is a function that assign \(A\) a real number in \([0,
1]\), satisfying the following three conditions:
</p>
<ol class="org-ol">
<li>\(0 \leq \mathrm{Pr}(A) \leq 1\).</li>
<li>\(\mathrm{Pr}(S) = 1\).</li>
<li>For any disjoint sets, \(A\) and \(B\), that is \(A\) and \(B\) have no
element in common, \(\mathrm{Pr}(A \cup B) = \mathrm{Pr}(A) +
  \mathrm{Pr}(B)\).</li>
</ol>

<p>
Here we use the concept of disjoint (or mutually exclusive) sets. \(A\)
and \(B\) are disjoint if there is no common element between these two
sets, that is, \(A \cap B\) is an empty set.
</p>
</div>
</div>

<div id="outline-container-org3840953" class="outline-4">
<h4 id="org3840953">Random variables</h4>
<div class="outline-text-4" id="text-org3840953">
<p>
Instead of using words or a set symbol to represent an event or an
outcome, we can use numeric value to do so. A <b>random variable</b> is
thus a numerical summary associated with the outcomes of an
experiment. You can also think of a random variable as a function
mapping from an event \(\omega\) in the sample space \(\Omega\) to the
real line, as illustrated in Figure <a href="#org436b048">1</a>.
</p>


<div id="org436b048" class="figure">
<p><img src="figure/random_variable_demo1.png" alt="random_variable_demo1.png" width="600" />
</p>
<p><span class="figure-number">Figure 1: </span>An illustration of random variable</p>
</div>

<p>
Random variables can take different types of values. A <b>discrete</b> random
variables takes on a discrete set of values, like \(0, 1, 2, \ldots, n\)
whereas a <b>continuous</b> random variable takes on a continuum of possble
values, like any value in the interval \((a, b)\).
</p>
</div>
</div>
</div>


<div id="outline-container-orgc63a10a" class="outline-3">
<h3 id="orgc63a10a"><span class="section-number-3">1.2</span> Probability distributions</h3>
<div class="outline-text-3" id="text-1-2">
</div><div id="outline-container-orge8ca469" class="outline-4">
<h4 id="orge8ca469">The probability distribution for a discrete random variable</h4>
<div class="outline-text-4" id="text-orge8ca469">
<p>
The probability distribution of a discrete random variable is the list
of all possible values of the variable and the probability that each
value will occur. These probabilities sum to 1.
</p>
</div>

<ul class="org-ul"><li><a id="org6325a1d"></a>The probability mass function<br  /><div class="outline-text-5" id="text-org6325a1d">
<p>
Let \(X\) be a discrete random variable. The probability distribution of
\(X\) (or the probability mass function), \(p(x)\), is
</p>
\begin{equation*}
p(x) = \mathrm{Pr}(X = x)
\end{equation*}
<p>
where we use \(X\) to denote the random variable and \(x\) to denote a
specific value that \(X\) can take. We denote the set of all possible
value of \(X\) as \(S\).
</p>

<p>
The axioms of probability require that (1) \(0 \leq p(x) \leq
1\) and (2) \( \sum_{i=1}^n p(x_i) = 1\).
</p>

<table id="org86a0386" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> An illustration of the probability distribution of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-right">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.50</td>
<td class="org-right">0.25</td>
<td class="org-right">1.</td>
</tr>
</tbody>
</table>
</div></li>

<li><a id="org3cc3019"></a>The cumulative probability distribution<br  /><div class="outline-text-5" id="text-org3cc3019">
<p>
The <b>cumulative probability distribution</b> (or the cumulative
distribution function, c.d.f.) is the probability that the random variable is
less than or equal to a particular value. Let \(F(x)\) be the c.d.f of
\(X\). Then \(F(x) = \mathrm{Pr}(X \leq x)\).
</p>

<p>
Table <a href="#orgd96c7ac">2</a> and Figure <a href="#org7c8253c">2</a> show that the
c.d.f. of a discrete random variable is a step function of \(x\).
</p>

<table id="orgd96c7ac" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> An illustration of the c.d.f. of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-left">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.50</td>
<td class="org-right">0.25</td>
<td class="org-left">1</td>
</tr>

<tr>
<td class="org-left">C.d.f.</td>
<td class="org-right">0.25</td>
<td class="org-right">0.75</td>
<td class="org-right">1</td>
<td class="org-left">--</td>
</tr>
</tbody>
</table>


<div id="org7c8253c" class="figure">
<p><img src="figure/cdf_discrete_example.png" alt="cdf_discrete_example.png" width="500" height="330" />
</p>
<p><span class="figure-number">Figure 2: </span>The c.d.f. of a discrete random variable</p>
</div>
</div></li>

<li><a id="orga3f855c"></a>Bernouli distribution<br  /><div class="outline-text-5" id="text-orga3f855c">
<p>
Many experiments like tossing a coin generate two outcomes: 1
with the probability of \(p\) and 0 with the probability of \(1-p\). The
random variable generated from such an experiment follows the Bernoulli
distribution.
</p>

<p>
The Bernoulli distribution
</p>
\begin{equation*}
  G =
    \begin{cases}
      1 & \text{with probability } p \\
      0 & \text{with probability } 1-p
    \end{cases}
  \end{equation*}
</div></li></ul>
</div>

<div id="outline-container-org7da8e8c" class="outline-4">
<h4 id="org7da8e8c">The probability distribution of a continuous random variable</h4>
<div class="outline-text-4" id="text-org7da8e8c">
<p>
Unlike a discrete random variable that we can enumerate its values for
each corresponding event, a specific value of a continuous random
variable is just a point in the real line, the probability of which is
zero. Instead, we use the concept of the <b>probability density function
(p.d.f)</b> as the counterpart of the probability mass function. And the
definition of the p.d.f. of a continuous random variable depends on
the definition of its. c.d.f.
</p>

<p>
The cumulative distribution function of a continous random variable
is defined as it is for a discrete random variable. That is, for a
continous random variable, \(X\), the c.d.f. is \(F(x) = \mathrm{Pr}(X
\leq x)\). And the <b>p.d.f.</b> of \(X\) is the function that satisfies
\[ F(x) = \int_{-\infty}^{x} f(t) \mathrm{d}t \text{ for all } x \]
</p>


<div id="org5bbffb7" class="figure">
<p><img src="figure/norm1.png" alt="norm1.png" width="500" height="450" />
</p>
<p><span class="figure-number">Figure 3: </span>The p.d.f. and c.d.f. of a continuous random variable (the normal distribution)</p>
</div>

<p>
For both discrete and continuous random variable, \(F(x)\) must satisfy
the following properties:
</p>
<ol class="org-ol">
<li>\(F(+\infty) = 1 \text{ and } F(-\infty) = 0\) (\(F(x)\) is bounded between 0 and 1)</li>
<li>\(x > y \Rightarrow F(x) \geq F(y)\) (\(F(x)\) is nondecreasing)</li>
</ol>

<p>
By the definition of the c.d.f., we can conveniently calculate
probabilities, such as,
</p>
<ul class="org-ul">
<li>\(\mathrm{P}(x > a) = 1 - \mathrm{P}(x \leq a) = 1 - F(a)\)</li>
<li>\(\mathrm{P}(a < x \leq b) = F(b) - F(a)\).</li>
</ul>

<p>
<b>A note on notation</b>. The c.d.f. and p.d.f. of a random variable \(X\) are
sometimes denoted as \(F_X(x)\) and \(f_X(x)\). In our lecture notes, if
there is no confusion, I will simply use \(F(x)\) and \(f(x)\) without the
subscript. 
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org1cf58f2" class="outline-2">
<h2 id="org1cf58f2"><span class="section-number-2">2</span> Expectation, Variance, and Other Moments</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-org4e8cc09" class="outline-3">
<h3 id="org4e8cc09"><span class="section-number-3">2.1</span> The expected value of a random variable</h3>
<div class="outline-text-3" id="text-2-1">
</div><div id="outline-container-orgc00bf25" class="outline-4">
<h4 id="orgc00bf25">Definition</h4>
<div class="outline-text-4" id="text-orgc00bf25">
<p>
The <b>expected value</b> of a random variable, X, denoted as \(\mathrm{E}(X)\), is
the long-run average of the random variable over many repeated
trials or occurrences, which is also called the <b>expectation</b> or the
<b>mean</b>. The expected value measures the centrality of a random
variable.
</p>

<ul class="org-ul">
<li><p>
For a discrete random variable
\[ \mathrm{E}(X) = \sum_{i=1}^n x_i \mathrm{Pr}(X = x_i) \]
</p>

<p>
e.g. The expectation of a Bernoulli random variable, G
  \[ \mathrm{E}(G) = 1 \cdot p + 0 \cdot (1-p) = p \]
</p></li>

<li>For a continuous random variable
\[ \mathrm{E}(X) = \int_{-\infty}^{\infty} x f(x) \mathrm{d}x\]</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org507a13f" class="outline-3">
<h3 id="org507a13f"><span class="section-number-3">2.2</span> The variance and standard deviation</h3>
<div class="outline-text-3" id="text-2-2">
<p>
The <b>variance</b> of a random variable \(X\) measures its average
deviation from its own expected value. Let \(\mathrm{E}(X) = \mu_X\) and denote
the variance of \(X\), denoted as \(\mathrm{Var}(X)\) or \(\sigma^2_X\), is then
</p>

\begin{equation*}
\mathrm{Var}(X) = \mathrm{E}(X-\mu_X)^{2}=
\begin{cases}
\sum_{i=1}^n (x_i - \mu_X)^{2}\mathrm{Pr}(X = x_i) & \text{if } X \text{ is discrete} \\
\int_{-\infty}^{\infty} (x - \mu_X)^{2}f(x)\mathrm{d} x  & \text{if } X \text{ is continuous}
\end{cases}
\end{equation*}

<p>
The <b>standard deviation</b> of \(X\) is the square root of
\(\mathrm{Var}(X)\) and is denoted as \(\sigma_{X}\). That is,
\(\sigma_{X} = \sqrt{\mathrm{Var}(X)}\)
</p>

<p>
A convenient formula for calculating the variance is
\[ \mathrm{Var}(X) = \mathrm{E}(X - \mu_X)^{2} = \mathrm{E}(X^{2}) - \mu_X^{2} \]
</p>

<p>
The variance of a Bernoulli random variable, \(G\)
\[ \mathrm{Var}(G) = (1-p)^{2}p + (0-p)^{2}(1-p) = p(1-p) \] and \(\sigma_{G} =
\sqrt{p(1-p)}\).
</p>

<p>
From the definition of the expectation and variance, we can compute
the expectation and variance of a linear function of \(X\). Let \(Y = a +
bX\), then
</p>
<ul class="org-ul">
<li>\(\mathrm{E}(Y) = a + b\mathrm{E}(X)\)</li>
<li>\(\mathrm{Var}(Y) = \mathrm{Var}(a + b X) = b^{2} \mathrm{Var}(X)\).</li>
</ul>
</div>
</div>


<div id="outline-container-org799daff" class="outline-3">
<h3 id="org799daff"><span class="section-number-3">2.3</span> Moments of a random variable, skewness and kurtosis</h3>
<div class="outline-text-3" id="text-2-3">
<p>
The expectation and variance are two special cases of the <b>moments</b> of
a distribution.
</p>
</div>

<div id="outline-container-org395e7b0" class="outline-4">
<h4 id="org395e7b0">Definition of the moments of a distribution</h4>
<div class="outline-text-4" id="text-org395e7b0">
<dl class="org-dl">
<dt>k<sup>th</sup> moment</dt><dd>The k<sup>th</sup> <b>moment</b> of the distribution of \(X\) is
\(\mathrm{E}(X^{k})\). So, the expectation is the "first"
moment of \(X\).</dd>

<dt>k<sup>th</sup> central moment</dt><dd>The k<sup>th</sup> central moment of the distribution
of \(X\) with its mean \(\mu_X\) is \(\mathrm{E}(X - \mu_X)^{k}\). So, the
variance is the second central moment of \(X\).</dd>
</dl>

<p>
It is important to remember that not all the moments of a distribution
exist. This is especially true for continuous random variables, for
which the integral to compute the moments may not converge.
</p>
</div>
</div>

<div id="outline-container-org289af29" class="outline-4">
<h4 id="org289af29">Skewness and kurtosis</h4>
<div class="outline-text-4" id="text-org289af29">
<p>
We also use the third and fourth central moments to measure how a
distribution looks like asymmetric and how thick are its tails.
</p>
</div>

<ul class="org-ul"><li><a id="orgec77c5f"></a>Skewness<br  /><div class="outline-text-5" id="text-orgec77c5f">
<p>
The skewness of a distribution provides a mathematical way to describe
how much a distribution deviates from symmetry. It is defined as
\[ \text{Skewness} =  \mathrm{E}(X - \mu_X)^{3}/\sigma_{X}^{3} \]
</p>

<ul class="org-ul">
<li>A symmetric distribution has a skewness of zero.</li>
<li>The skewness can be either positive or negative.</li>
<li>That \(\mathrm{E}(X - \mu_X)^3\) is divided by \(\sigma^3_X\) is to make the
skewness measure unit free. That is, changing the units of Y does
not change its skewness.</li>
</ul>
</div></li>

<li><a id="org1fb7b3c"></a>Kurtosis<br  /><div class="outline-text-5" id="text-org1fb7b3c">
<p>
The kurtosis of the distribution of a random variable \(X\) measures how
much of the variance of \(X\) arises from extreme values, which makes
the distribution have "heavy" tails.
</p>

<p>
The kurtosis of the distribution of \(X\) is
\[ \text{Kurtosis} = \mathrm{E}(X - \mu_X)^{4}/\sigma_{X}^{4} \]
</p>

<ul class="org-ul">
<li>The kurtosis must be positive.</li>
<li>The kurtosis of the normal distribution is 3. So a distribution that
has its kurtosis exceeding 3 is called heavy-tailed, or
<b>leptokurtic</b>.</li>
<li>The kurtosis is also unit free.</li>
</ul>

<p>
Figure <a href="#org5638a4d">4</a> displays four distributions with different
skewness and kurtosis. All four distributions have a mean of zero and
a variance of one, while (a) and (b) are symmetric and (b)-(d) are
heavy-tailed.
</p>


<div id="org5638a4d" class="figure">
<p><img src="figure/fig-2-3.png" alt="fig-2-3.png" width="600" />
</p>
<p><span class="figure-number">Figure 4: </span>Four distributions with different skewness and kurtosis</p>
</div>
</div></li></ul>
</div>
</div>
</div>


<div id="outline-container-orgc6d58c2" class="outline-2">
<h2 id="orgc6d58c2"><span class="section-number-2">3</span> Two Random Variables</h2>
<div class="outline-text-2" id="text-3">
<p>
Econometrics in most cases considers the relations between two or
more variables. We use the concepts of joint, marginal, and
conditional distributions to describe such relations.
</p>
</div>

<div id="outline-container-orgf8b569c" class="outline-3">
<h3 id="orgf8b569c"><span class="section-number-3">3.1</span> The joint and marginal distributions</h3>
<div class="outline-text-3" id="text-3-1">
</div><div id="outline-container-org5f4aba8" class="outline-4">
<h4 id="org5f4aba8">The joint probability functions</h4>
<div class="outline-text-4" id="text-org5f4aba8">
<p>
For two discrete random variables, \(X\) and \(Y\), the joint probability
distribution of \(X\) and \(Y\) is the probability that \(X\) and \(Y\)
simultaneously take on certain values, \(x\) and \(y\), that is
\[ p(x, y) = \mathrm{Pr}(X = x, Y = y)\]
which must satisfy the following
</p>
<ol class="org-ol">
<li>\(p(x, y) \geq 0\)</li>
<li>\(\sum_{i=1}^n\sum_{j=1}^m p(x_i, y_j) = 1\) for all possible
combinations of values of \(X\) and \(Y\).</li>
</ol>

<p>
For two continuous random variables, \(X\) and \(Y\), the counterpart of \(p(x, y)\) is
the joint probability density function, \(f(x, y)\), such that
</p>
<ol class="org-ol">
<li>\(f(x, y) \geq 0\)</li>
<li>\(\int_{-\infty}^{{\infty}} \int_{-\infty}^{\infty} f(x, y)\, dx\, dy= 1\)</li>
</ol>
</div>
</div>

<div id="outline-container-org8283b65" class="outline-4">
<h4 id="org8283b65">The marginal probability distribution</h4>
<div class="outline-text-4" id="text-org8283b65">
<p>
The marginal probability distribution of a random variable \(X\) is
simply the probability distribution of its own. Since it is computed
from the joint probability distribution of \(X\) and \(Y\), we call it as
marginal probability of \(X\).
</p>

<ul class="org-ul">
<li>For a discrete random variable, we can compute the marginal
distribution of \(X\) as
\[ \mathrm{Pr}(X=x) = \sum_{i=1}^n \mathrm{Pr}(X, Y=y_i) = \sum_{i=1}^n p(x, y_i)  \]</li>
<li>For a continuous random variable, the marginal distribution is
\[f_X(x) = \int_{-\infty}^{\infty} f(x, y)\, dy \]</li>
</ul>

<p>
By summing over or integrating out the values of \(Y\), we get the
probability distribution of \(X\) of its own.
</p>
</div>
</div>

<div id="outline-container-org49e44c5" class="outline-4">
<h4 id="org49e44c5">An example of joint and marginal distributions</h4>
<div class="outline-text-4" id="text-org49e44c5">
<p>
Suppose we have two random variables: \(X\) and \(Y\). \(X\) equals 1 if
today is not raining and 0 otherwise, and \(Y\) equals 1 if it takes a short
time for commuting and 0 otherwise. The joint probability distribution
of these two random variables is the distribution that \(X\) takes the
value of 1 or 0 at the same time \(Y\) takes the value of 1 or 0, which
can be represented in the following table.
</p>

<table id="orgf11267d" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> Joint distribution of raining and commuting time</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Rain (\(X=0\))</th>
<th scope="col" class="org-right">No rain (\(X=1\))</th>
<th scope="col" class="org-right">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Long commute (\(Y=0\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.07</td>
<td class="org-right">0.22</td>
</tr>

<tr>
<td class="org-left">Short commute (\(Y=1\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.63</td>
<td class="org-right">0.78</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Total</td>
<td class="org-right">0.30</td>
<td class="org-right">0.70</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

<p>
The four cells in the middle are the joint distribution. For example,
the joint probability of raining and taking a short commute is
0.15. The last row is the marginal distribution of \(X\), indicating
that the probability of raining no matter taking a long or short
commute is 0.30. The marginal distribution of \(X\) is in fact a
Bernoulli distribution. Similarly, the last column is the marginal
distribution of \(Y\), which is also a Bernoulli distribution.
</p>
</div>
</div>
</div>


<div id="outline-container-org0833375" class="outline-3">
<h3 id="org0833375"><span class="section-number-3">3.2</span> Conditional distributions</h3>
<div class="outline-text-3" id="text-3-2">
</div><div id="outline-container-orgb43177e" class="outline-4">
<h4 id="orgb43177e">The conditional probability</h4>
<div class="outline-text-4" id="text-orgb43177e">
<p>
We often say that given one thing happens, what is the probability of
another thing to happen? To answer this question, we need the concept
of conditional probability.
</p>

<p>
For any two events \(A\) and \(B\), the conditional probability of A given
B is defined as
</p>
\begin{equation*}
\mathrm{Pr}(A|B) = \frac{\mathrm{Pr}(A \cap B)}{\mathrm{Pr}(B)}
\end{equation*}

<p>
Figure <a href="#org388bbb3">5</a><sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup> helps us understand the
meaning of conditional probability. When we condition on the set B,
the sample space shrink from the original sample space, S, to a new
sample space, B. Since \(\mathrm{Pr}(A \cap B)\) is defined over S, so
we need to divide it by \(\mathrm{Pr}(B)\) to get \(\mathrm{Pr}(A|B)\).
</p>


<div id="org388bbb3" class="figure">
<p><img src="figure/conditional_probability.png" alt="conditional_probability.png" width="400" height="400" />
</p>
<p><span class="figure-number">Figure 5: </span>An illustration of conditional probability</p>
</div>
</div>
</div>

<div id="outline-container-orgdd0699b" class="outline-4">
<h4 id="orgdd0699b">The conditional probability distribution</h4>
<div class="outline-text-4" id="text-orgdd0699b">
<p>
The conditional distribution of a random variable \(Y\) given another
random variable \(X\) is the distribution of \(Y\) conditional on X taking
a specific value, denoted as \(\mathrm{Pr}(Y | X=x)\). And the formula
to compute it is
\[ \mathrm{Pr}(Y | X=x) = \frac{\mathrm{Pr}(X=x,
Y)}{\mathrm{Pr}(X=x)} \]
</p>

<p>
For continuous random variables \(X\) and \(Y\), we define the conditional
density function as
\[ f(y|x) = \frac{f(x, y)}{f_X(x)} \]
</p>

<p>
In the above example of raining and commuting time, we can compute the
conditional distribution of commuting time given raining or not as
follows
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">\(\mathrm{Pr}(Y=0 \mid X=0)\)</td>
<td class="org-left">0.15/0.30 = 0.5</td>
<td class="org-left">\(\mathrm{Pr}(Y=0 \mid X=1)\)</td>
<td class="org-left">0.07/0.7 = 0.1</td>
</tr>

<tr>
<td class="org-left">\(\mathrm{Pr}(Y=1 \mid X=0)\)</td>
<td class="org-left">0.15/0.30 = 0.5</td>
<td class="org-left">\(\mathrm{Pr}(Y=1 \mid X=1)\)</td>
<td class="org-left">0.63/0.7 = 0.9</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org63f66a7" class="outline-4">
<h4 id="org63f66a7">The conditional expectation</h4>
<div class="outline-text-4" id="text-org63f66a7">
</div><ul class="org-ul"><li><a id="org9be2a72"></a>Definition<br  /><div class="outline-text-5" id="text-org9be2a72">
<p>
The <b>conditional expectation</b> of \(Y\) given \(X\) is the expected value
of the conditional distribution of \(Y\) given \(X\).
</p>

<ul class="org-ul">
<li><p>
For discrete random variables, the conditional mean of \(Y\) given \(X=x\) is
</p>
\begin{equation*}
\mathrm{E}(Y \mid X=x) = \sum_{i=1}^n y_i \mathrm{Pr}(Y=y_i \mid X=x)
\end{equation*}</li>

<li><p>
For continuous random variables, it is computed as
</p>
\begin{equation*}
\int_{-\infty}^{\infty} y f(y \mid x)\, dy
\end{equation*}</li>

<li>The expected mean of commuting time given it is raining is \(0 \times
  0.1 + 1 \times 0.9 = 0.9\).</li>
</ul>
</div></li>

<li><a id="orgf2f58d3"></a>The law of iterated expectation<br  /><div class="outline-text-5" id="text-orgf2f58d3">
<p>
Conditional expectations have a special property called <b>the law of
iterated expectation</b>, \(\mathrm{E}(Y) = E \left[ \mathrm{E}(Y|X) \right]\).
</p>

<p>
It says that the mean of \(Y\) is the weighted average of the
conditional expectation of \(Y\) given \(X\), weighted by the probability
distribution of \(X\). That is,
\[ \mathrm{E}(Y) = \sum_{i=1}^n \mathrm{E}(Y \mid X=x_i) \mathrm{Pr}(X=x_i) \]
</p>

<p>
The proof for the above equation is one question in the homework.
</p>

<p>
If \(\mathrm{E}(X|Y) = 0\), then \(\mathrm{E}(X)=E\left[\mathrm{E}(X|Y)\right]=0\).
</p>
</div></li>

<li><a id="org9d2dc90"></a>Conditional variance<br  /><div class="outline-text-5" id="text-org9d2dc90">
<p>
With the conditional mean of \(Y\) given \(X\), we can compute the
conditional variance as
\[ \mathrm{Var}(Y \mid X=x) = \sum_{i=1}^n \left[ y_i - \mathrm{E}(Y \mid X=x)
\right]^2 \mathrm{Pr}(Y=y_i \mid X=x) \]
</p>

<p>
From the law of iterated expectation, we can get the following
\[ \mathrm{Var}(Y) = \mathrm{E}(\mathrm{Var}(Y \mid X)) + \mathrm{Var}(\mathrm{E}(Y \mid
X)) \]
</p>
</div></li></ul>
</div>

<div id="outline-container-org909ad40" class="outline-4">
<h4 id="org909ad40">Independence</h4>
<div class="outline-text-4" id="text-org909ad40">
</div><ul class="org-ul"><li><a id="org4953a27"></a>The definition of independent random variables<br  /><div class="outline-text-5" id="text-org4953a27">
<p>
Two random variables \(X\) and \(Y\) are <b>independently distributed</b>, or
<b>independent</b>, if knowing the value of one of the variable provides no
information about the other. Mathematically, it means that 
\[ \mathrm{Pr}(Y=y \mid X=x) = \mathrm{Pr}(Y=y)  \]
</p>

<p>
It follows that if \(X\) and \(Y\) are independent
\[ \mathrm{Pr}(Y=y, X=x) = \mathrm{Pr}(X=x) \mathrm{Pr}(Y=y) \]
</p>

<p>
For two continuous random variables, \(X\) and \(Y\), they are
<b>independent</b> if
\[ f(x|y) = f_{X}(x) \text{ or } f(y|x) = f_{Y}(y) \]
</p>

<p>
It follows that if \(X\) and \(Y\) are independent
\[ f(x, y) = f(x|y)f_{Y}(y) = f_{X}(x)f_{Y}(y) \]
</p>
</div></li></ul>
</div>
</div>


<div id="outline-container-orgfbc9529" class="outline-3">
<h3 id="orgfbc9529"><span class="section-number-3">3.3</span> Covariance and Correlation</h3>
<div class="outline-text-3" id="text-3-3">
</div><div id="outline-container-orged0bfeb" class="outline-4">
<h4 id="orged0bfeb">Covariance</h4>
<div class="outline-text-4" id="text-orged0bfeb">
<p>
Covariance and correlation measure the co-movement of two random
variables. The covariance of two random variables \(X\) and \(Y\) is
</p>

\begin{align*}
\mathrm{Cov}(X, Y) & = \sigma_{XY} = \mathrm{E}(X-\mu_{X})(Y-\mu_{Y}) \\
                   & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i, Y=y_j)
\end{align*}

<p>
For continous random variables, the covariance of \(X\) and \(Y\) is
\[ \mathrm{Cov}(X, Y) = \int_{-\infty}^{\infty}
\int_{-\infty}^{\infty} (x-\mu_X)(y-\mu_y)f(x, y) dx dy \]
</p>

<p>
The covariance can also be computed as
\[ \mathrm{Cov}(X, Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y) \]
</p>
</div>
</div>

<div id="outline-container-orgf8fe5b8" class="outline-4">
<h4 id="orgf8fe5b8">Correlation coefficient</h4>
<div class="outline-text-4" id="text-orgf8fe5b8">
<p>
Since the unit of the covariance of \(X\) and \(Y\) is the product of the
unit of \(X\) and that of \(Y\), its meaning is hard to be interpreted. We
often use the correlation coefficient to measure the correlation,
regardless of the units of \(X\) and \(Y\). The <b>correlation coefficient</b>
of \(X\) and \(Y\) is
</p>

<p>
\[ \mathrm{corr}(X, Y) = \rho_{XY} = \frac{\mathrm{Cov}(X, Y)}{\left[\mathrm{Var}(X)\mathrm{Var}(Y)\right]^{1/2}} =
\frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}} \]
</p>

<p>
The absolute value of a correlation coefficient must be less
than 1. That is, \(-1 \leq \mathrm{corr}(X, Y) \leq 1\). 
</p>

<p>
And \(\mathrm{corr}(X, Y)=0\) (or \(\mathrm{Cov}(X,Y)=0\)) means that \(X\)
and \(Y\) are uncorrelated. Since \(\mathrm{Cov}(X, Y) = \mathrm{E}(XY) -
\mathrm{E}(X)\mathrm{E}(Y)\), when \(X\) and \(Y\) are uncorrelated, then \(\mathrm{E}(XY) =
\mathrm{E}(X) \mathrm{E}(Y)\). 
</p>
</div>
</div>

<div id="outline-container-org54e1c15" class="outline-4">
<h4 id="org54e1c15">Independence and uncorrelation</h4>
<div class="outline-text-4" id="text-org54e1c15">
<p>
There is an important relationship between the two concepts of
independence and uncorrelation. If \(X\) and \(Y\) are independent, then
</p>

\begin{align*}
\mathrm{Cov}(X, Y) & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i) \mathrm{Pr}(Y=y_j) \\
                   & = \sum_{i=1}^n (x_i - \mu_X) \mathrm{Pr}(X=x_i) \sum_{j=1}^m (y_j - \mu_y) \mathrm{Pr}(Y=y_j) \\
                   & = 0 \times 0 = 0
\end{align*}

<p>
That is, if \(X\) and \(Y\) are independent, they must be
uncorrelated. However, the converse is not true. If \(X\) and \(Y\) are
uncorrelated, there is a possibility that they are actually
dependent. (Read this article, <a href="http://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf">``uncorrelated-vs-independent''</a>, for a detailed explanation<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>)
</p>

<p>
Accordingly, if \(X\) and \(Y\) are independent, then we must have
</p>

\begin{align*}
\mathrm{E}(Y \mid X) & = \sum_{i=1}^n y_i \mathrm{Pr}(Y=y_i \mid X) \\
                     & = \sum_{i=1}^n y_i \mathrm{Pr}(Y=y_i) \\
                     & = \mathrm{E}(Y) = \mu_Y
\end{align*}

<p>
That is, the conditional mean of \(Y\) given \(X\) does not depend on any
value of \(X\) when \(Y\) is independent of \(X\). Then, we can prove that
\(\mathrm{Cov}(X, Y) = 0\) and \(\mathrm{corr}(X, Y)=0\).
</p>

\begin{align*}
\mathrm{E}(XY) & = \mathrm{E}(\mathrm{E}(XY \mid X)) = \mathrm{E}(X \mathrm{E}(Y \mid X)) \\
               & = \mathrm{E}(X) \mathrm{E}(Y \mid X) = \mathrm{E}(X) \mathrm{E}(Y)
\end{align*}

<p>
It follows that \(\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X)
\mathrm{E}(Y) = 0\) and \(\mathrm{corr}(X, Y)=0\). 
</p>

<p>
Again, the converse is not true. That is, if \(X\) and \(Y\) are
uncorrelated, the conditional mean of \(Y\) given \(X\) may still depend
on \(X\). (See Exercise 2.23 at the end of Chapter 2.)
</p>
</div>
</div>

<div id="outline-container-org8af9a29" class="outline-4">
<h4 id="org8af9a29">Some useful operations</h4>
<div class="outline-text-4" id="text-org8af9a29">
<p>
Let \(X\) and \(Y\) be two random variables, with the means \(\mu_{X}\) and
\(\mu_{Y}\), the variance \(\sigma^{2}_{X}\) and \(\sigma^{2}_{Y}\), and the
covariance \(\sigma_{XY}\), respectively. Then, the following properties
of \(\mathrm{E}(\cdot)\), \(\mathrm{Var}(\cdot)\) and
\(\mathrm{Cov}(\cdot)\) are useful in calculation,
</p>

\begin{align*}
\mathrm{E}(a + bX + cY)      & = a + b \mu_{X} + c \mu_{Y} \\
\mathrm{Var}(aX + bY)        & = a^{2} \sigma^{2}_{X} + b^{2} \sigma^{2}_{Y} + 2ab\sigma_{XY} \\
\mathrm{Cov}(a + bX + cV, Y) & = b\sigma_{XY} + c\sigma_{VY} \\
\end{align*}
</div>
</div>
</div>
</div>


<div id="outline-container-org9282a16" class="outline-2">
<h2 id="org9282a16"><span class="section-number-2">4</span> Four Specific Distributions</h2>
<div class="outline-text-2" id="text-4">
<p>
There are a variety of statistical distributions. In this course, we
will probably use the following four distributions: the normal
distribution, the chi-squared distribution, the student t
distribution, and the F distribution. 
</p>
</div>

<div id="outline-container-orge34024a" class="outline-3">
<h3 id="orge34024a"><span class="section-number-3">4.1</span> The normal distribution</h3>
<div class="outline-text-3" id="text-4-1">
</div><div id="outline-container-org9c33401" class="outline-4">
<h4 id="org9c33401">Definition</h4>
<div class="outline-text-4" id="text-org9c33401">
<p>
The p.d.f. of a normally distributed random variable \(X\) is
\[ f(x) =
\frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right]
\]
for which \(\mathrm{E}(X) = \mu\) and \(\mathrm{Var}(X) = \sigma^{2}\). We
usually write \(X \sim N(\mu, \sigma^{2})\) to say \(X\) has a normal
distribution with mean \(\mu\) and variance \(\sigma^2\). 
</p>

<p>
The standard normal distribution is a special case of the normal
distribution, for which \(\mu = 0\) and \(\sigma = 1\). The p.d.f of the
standard normal distribution is
\[
\phi(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)
\]
The c.d.f of the standard normal distribution is often denoted as
\(\Phi(x)\).
</p>

<p>
The normal distribution is symmetric around its mean, \(\mu\), with the
skewness equal 0, and has 95% of its probability between
\(\mu-1.96\sigma\) and \(\mu+1.96\sigma\), with the kurtosis
equal 3. Figure <a href="#org161ca56">6</a> displays the probability density of a
normal distribution.<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>
</p>


<div id="org161ca56" class="figure">
<p><img src="figure/Normal-distribution-curve.png" alt="Normal-distribution-curve.png" />
</p>
<p><span class="figure-number">Figure 6: </span>The normal probability density</p>
</div>
</div>
</div>

<div id="outline-container-orgf61b1aa" class="outline-4">
<h4 id="orgf61b1aa">Transforming a normally distributed random variable to the standard normal distribution</h4>
<div class="outline-text-4" id="text-orgf61b1aa">
<p>
Let \(X\) be a random variable with a normal distribution, i.e., \(X \sim
N(\mu, \sigma^2)\). Then, we can easily compute the probability of \(X\)
by transforming it into a random variable with the standard normal
distribution. We compute \(Z = (X-\mu)/\sigma\), which follows
the standard normal distribution, \(N(0, 1)\).
</p>

<p>
For example, if \(X \sim N(1, 4)\), then \(Z = (X-1)/2 \sim N(0,
1)\). When we want to find \(\mathrm{Pr}(X \leq 4)\), we only need to
compute \(\Phi(3/2)\).<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>
</p>

<p>
Generally, for any two number \(c_1 < c_2\) and let \(d_1 = (c_1 - \mu)/\sigma\) and
\(d_2 = (c_2 - \mu)/\sigma\), we have
</p>
\begin{align*}
\mathrm{Pr}(X \leq c_2) & = \mathrm{Pr}(Z \leq d_2) = \Phi(d_2) \\
\mathrm{Pr}(X \geq c_1) & = \mathrm{Pr}(Z \geq d_1) = 1 - \Phi(d_1) \\
\mathrm{Pr}(c_1 \leq X \leq c_2) & = \mathrm{Pr}(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1)
\end{align*}
</div>
</div>

<div id="outline-container-org991163e" class="outline-4">
<h4 id="org991163e">The multivariate normal distribution</h4>
<div class="outline-text-4" id="text-org991163e">
<p>
The normal distribution can be generalized to describe the joint
distribution of a set of random variables, which have the multivariate
normal distribution. (See Appendix 17.1 for the p.d.f of this
distribution and the special case of the bivariate normal
distribution.)
</p>
</div>

<ul class="org-ul"><li><a id="orga90ae86"></a>Important properties of the multivariate normal distribution<br  /><div class="outline-text-5" id="text-orga90ae86">
<ol class="org-ol">
<li><p>
If \(X\) and \(Y\) have a bivariate normal distribution with covariance
\(\sigma_{XY}\) and \(a\) and \(b\) are two constants, then
\[
   aX + bY \sim N(a\mu_X + b\mu_Y, a^2\sigma_X + b^2\sigma_Y +
   2ab\sigma_{XY})
   \]
</p>

<p>
More generally, if n random variables, \(x_1, \ldots, x_n\), have a
multivariate normal distribution, then any linear combination of
these variables is normally distributed, for example, \(\sum_{i=1}^n
   x_i\). For any real numbers, \(\alpha_1, \ldots, \alpha_n\), a linear
combination of \({x_i}\) is \(\sum_i \alpha_i x_i\).
</p></li>

<li>If a set of random variables has a multivariate normal
distribution, then the marginal distribution of each of the
variables is normal.</li>

<li><p>
If random variables with a multivariate normal distribution have
covariances that equal zero, then these random variables are
independent.
</p>

<p>
Let \(X\) and \(Y\) be two random variables with a bivariate normal
distribution. The joint p.d.f of \(X\) and \(Y\) is \(f(x, y)\), with the
marginal p.d.f. being \(f_X(x)\) and \(f_Y(y)\), respectively. Then we have
\[ \mathrm{Cov}(X, Y) = 0 \Leftrightarrow f(x, y) = f_X(x)f_Y(y) \]
</p>

<p>
<b>Note</b>: this property only holds for random variables with a
multivariate normal distribution. Generally, uncorrelation does not
imply independence.
</p></li>

<li>If \(X\) and \(Y\) have a bivariate normal distribution, then
\[\mathrm{E}(Y|X = x) = a + bx \]
where \(a\) and \(b\) are constants.</li>
</ol>
</div></li></ul>
</div>
</div>


<div id="outline-container-org643dfc7" class="outline-3">
<h3 id="org643dfc7"><span class="section-number-3">4.2</span> The chi-squared distribution</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Let \(Z_1, \ldots, Z_n\) be n indepenent standard normal distribution,
i.e. \(Z_i \sim N(0, 1)\) for all \(i = 1, \ldots, n\). Then, the random
variable
\[W = \sum_{i=1}^n Z^2_i \]
has a chi-squared distribution with \(n\) degrees of freedom, denoted as
\(W \sim \chi^2(n)\), with \(\mathrm{E}(W) = n\) and \(\mathrm{Var}(W) = 2n\)
</p>

<p>
If \(Z \sim N(0, 1)\), then \(W = Z^2 \sim \chi^2(1)\) with \(\mathrm{E}(W) =
1\) and \(\mathrm{Var}(W) = 2\).
</p>

<p>
Figure <a href="#orga56656d">7</a> shows the p.d.f. of chi-squared distributions
with different degrees of freedom. 
</p>


<div id="orga56656d" class="figure">
<p><img src="figure/chi_squared_pdf.png" alt="chi_squared_pdf.png" width="500" />
</p>
<p><span class="figure-number">Figure 7: </span>The probability density function of chi-squared distributions</p>
</div>
</div>
</div>


<div id="outline-container-org764b71e" class="outline-3">
<h3 id="org764b71e"><span class="section-number-3">4.3</span> The student t distribution</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Let \(Z \sim N(0, 1)\), \(W \sim \chi^2(m)\), and \(Z\) and \(W\) be
independently distributed. Then, the random variable
\[t = \frac{Z}{\sqrt{W/m}} \]
has a student t distribution with \(m\) degrees of freedom, denoted as
\(t \sim t(m)\).
</p>

<p>
As \(n\) increases, \(t\) gets close to a standard normal distribution.
</p>

<p>
Figure <a href="#org09368da">8</a> shows the p.d.f. of student t distributions
with different degrees of freedom. 
</p>


<div id="org09368da" class="figure">
<p><img src="figure/students_t_pdf.png" alt="students_t_pdf.png" width="500" />
</p>
<p><span class="figure-number">Figure 8: </span>The probability density function of student t distributions</p>
</div>
</div>
</div>


<div id="outline-container-org5762eb8" class="outline-3">
<h3 id="org5762eb8"><span class="section-number-3">4.4</span> The F distribution</h3>
<div class="outline-text-3" id="text-4-4">
<p>
Let \(W_1 \sim \chi^2(n_1)\), \(W_2 \sim \chi^2(n_2)\), and \(W_1\) and
\(W_2\) are independent. Then, the random variable
\[ F = \frac{W_1/n_1}{W_2/n_2}\]
has an F distribution with \((n_1, n_2)\) degrees of freedom, denoted as
\(F \sim F(n_1, n_2)\)
</p>

<ul class="org-ul">
<li>If \(t \sim t(n)\), then \(t^2 \sim F(1, n)\)</li>
<li>As \(n_2 \rightarrow \infty\), the \(F(n_1, \infty)\) distribution is the
same as the \(\chi^2(n_1)\) distribution divided by \(n_1\).</li>
</ul>

<p>
Figure <a href="#org56b5c8c">9</a> shows the p.d.f. of F distributions
with different degrees of freedom. 
</p>


<div id="org56b5c8c" class="figure">
<p><img src="figure/fisher_f_pdf.png" alt="fisher_f_pdf.png" width="500" />
</p>
<p><span class="figure-number">Figure 9: </span>The probability density function of F distributions</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org70dacae" class="outline-2">
<h2 id="org70dacae"><span class="section-number-2">5</span> Random Sampling and the Distribution of the Sample Average</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-orga98bab1" class="outline-3">
<h3 id="orga98bab1"><span class="section-number-3">5.1</span> Random sampling</h3>
<div class="outline-text-3" id="text-5-1">
<p>
Almost all the statistical and econometric procedures involve averages
of a sample of data. When samples are randomly drawn from a larger
population, the average itself is a random variable. Therefore, in
the next three sections, we consider the distributions of sample
averages. 
</p>
</div>

<div id="outline-container-org9c393eb" class="outline-4">
<h4 id="org9c393eb">Simple random sampling</h4>
<div class="outline-text-4" id="text-org9c393eb">
<p>
In statistics, a <b>population</b> is a set of similar items or events which
is of interest for some question or experiment. A statistical
population can be a group of actually existing objects (e.g. the set
of all stars within the Milky Way galaxy) or a hypothetical and
potentially infinite group of objects conceived as a generalization
from experience (e.g. the set of all possible hands in a game of
poker).<sup><a id="fnr.5" class="footref" href="#fn.5">5</a></sup>
</p>

<p>
<b>Simple random sampling</b> is a procedure in which \(n\) objects are
selected at random from a population, and each member of the
population is equally likely to be included in the sample. 
</p>

<p>
Let \(Y_1, Y_2, \ldots Y_n\) be the first \(n\) observations in a random
sample. Since they are randomly drawn from a population, \(Y_1, \ldots,
Y_n\) are random variables. And the sample average is also a random
variable. 
</p>
</div>
</div>

<div id="outline-container-org3a4414d" class="outline-4">
<h4 id="org3a4414d">i.i.d draws</h4>
<div class="outline-text-4" id="text-org3a4414d">
<p>
Since \(Y_1, Y_2, \ldots, Y_n\) are drawn from the same population, the
marginal distribution of \(Y_i\) is the same for each \(i=1, \ldots,
n\). And the marginal distribution is the distribution of the
population \(Y\). When \(Y_i\) has the same marginal distribution for
\(i=1, \ldots, n\), then \(Y_1, \ldots, Y_n\) are said to be <b>identically
distributed</b>. 
</p>

<p>
With simple random sampling, the value of \(Y_i\) does not depend on
that of \(Y_j\) for \(i \neq j\). That is, under simple random sampling,
\(Y_1, \ldots, Y_n\) are <b>independent distributed</b>. 
</p>

<p>
Therefore, when \(Y_1, \ldots, Y_n\) are drawn with simple random
sampling from the same distribution of \(Y\), we say that they are
<b>independently and identically distributed</b> or <b>i.i.d</b>, which can be
denoted as 
\[ Y_i \sim IID(\mu_Y, \sigma^2_Y) \text{ for } i = 1, 2, \ldots, n\]
given that the population expectation is \(\mu_Y\) and the variance
is \(\sigma^2_Y\).
</p>
</div>
</div>
</div>


<div id="outline-container-org6258289" class="outline-3">
<h3 id="org6258289"><span class="section-number-3">5.2</span> The sampling distribution of the sample average</h3>
<div class="outline-text-3" id="text-5-2">
</div><div id="outline-container-orga5b7ab0" class="outline-4">
<h4 id="orga5b7ab0">The sample average</h4>
<div class="outline-text-4" id="text-orga5b7ab0">
<p>
The <b>sample average</b> or <b>sample mean</b>, \(\overline{Y}\), of the \(n\)
observations \(Y_1, Y_2, \ldots, Y_n\) is
\[ \overline{Y} = \frac{1}{n}\sum^n_{i=1} Y_i \]
</p>

<p>
When \(Y_1, \ldots, Y_n\) are randomly drawn, \(\overline{Y}\) is also a
random variable that should have its own distribution, called the
<b>sampling distribution</b>. We are mostly interested in the mean,
variance, and the form of the sampling distribution.
</p>
</div>
</div>

<div id="outline-container-org29117e2" class="outline-4">
<h4 id="org29117e2">The mean and variance of \(\overline{Y}\)</h4>
<div class="outline-text-4" id="text-org29117e2">
<p>
Suppose that \(Y_i \sim IID(\mu_Y, \sigma^2_{Y})\) for all \(i = 1,
\ldots, n\). Then, by the definition of \(\overline{Y}\) and the fact
that \(Y_i\) and \(Y_j\) are independent for any \(i \neq j\) and thus,
\(\mathrm{Cov}(Y_i, Y_j)=0\), we have
\[
\mathrm{E}(\overline{Y}) = \mu_{\overline{Y}} =
\frac{1}{n}\sum^n_{i=1}\mathrm{E}(Y_i) = \frac{1}{n} n \mu_Y = \mu_Y
\]
and
\[
\mathrm{Var}(\overline{Y}) = \sigma^2_{\overline{Y}} =  \frac{1}{n^2}\sum^n_{i=1}\mathrm{Var}(Y_i) +
\frac{1}{n^2}\sum^n_{i=1}\sum^n_{j=1}\mathrm{Cov}(Y_i, Y_j) =
\frac{\sigma^2_Y}{n}
\]
And the standard deviation of the sample mean is
\(\sigma_{\overline{Y}} = \sigma_Y / \sqrt{n}\).
</p>
</div>
</div>

<div id="outline-container-org0a87863" class="outline-4">
<h4 id="org0a87863">Sampling distribution of \(\overline{Y}\) when \(Y\) is normally distributed</h4>
<div class="outline-text-4" id="text-org0a87863">
<p>
The sample average \(\overline{Y}\) is a linear combination of \(Y_1,
\ldots, Y_n\). When \(Y_1, \ldots, Y_n\) are i.i.d. draws from \(N(\mu_Y,
\sigma^2_Y)\), from the properties of the multivariate normal
distribution, \(\overline{Y}\) is normally distributed. That is 
\[ \overline{Y} \sim N(\mu_Y, \sigma^2_Y/n) \]
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org1be73ef" class="outline-2">
<h2 id="org1be73ef"><span class="section-number-2">6</span> Large Sample Approximations to Sampling Distributions</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-org1cc51bb" class="outline-3">
<h3 id="org1cc51bb"><span class="section-number-3">6.1</span> The exact distribution and the asymptotic distribution</h3>
<div class="outline-text-3" id="text-6-1">
<p>
The sampling distribution that exactly describes the distribution of
\(\overline{Y}\) for any \(n\) is called the <b>exact distribution</b> or
<b>finite-sample distribution</b>. For example, when \(Y\) is normally
distributed and \(Y_1, \ldots, Y_n\) are i.i.d. draws from \(N(\mu_Y,
\sigma^2_Y)\), we know the exact distribution of \(\overline{Y}\) is
\(N(\mu_Y, \sigma^2_Y/n)\). 
</p>

<p>
However, in most cases, we cannot obtain an exact distribution of
\(\overline{Y}\), for which we can only get an approximation. The
large-sample approximation to the sampling distribution is called the
<b>asymptotic distribution</b>. The existence of the asymptotic
distribution of \(\overline{Y}\) is guaranteed by the <b>law of large
numbers</b> and the <b>central limit theorem</b>. 
</p>
</div>
</div>


<div id="outline-container-orgfb3a54a" class="outline-3">
<h3 id="orgfb3a54a"><span class="section-number-3">6.2</span> The law of large numbers</h3>
<div class="outline-text-3" id="text-6-2">
</div><div id="outline-container-org2c6bf15" class="outline-4">
<h4 id="org2c6bf15">Convergence in probability</h4>
<div class="outline-text-4" id="text-org2c6bf15">
<p>
Let \(S_1, \ldots, S_n\) be a sequence of random variables,
denoted as \(\{S_n\}\). \(\{S_n\}\) is said to converge in probability to a
limit &mu; (denoted as \(S_n \xrightarrow{\text{p}} \mu\)), if and only if
\[ \mathrm{Pr} \left(|S_n-\mu| < \delta \right) \rightarrow 1 \]
as \(n \rightarrow \infty\) for every \(\delta > 0\).
</p>

<p>
For example, \(S_n = \overline{Y}\). That is, \(S_1=Y_1\), \(S_2=1/2(Y_1+Y_2)\),
\(S_n=1/n\sum_i Y_i\), and so forth.
</p>
</div>
</div>

<div id="outline-container-orgf499ade" class="outline-4">
<h4 id="orgf499ade">The law of large numbers</h4>
<div class="outline-text-4" id="text-orgf499ade">
<p>
The law of large numbers (LLN) states that if \(Y_1, \ldots, Y_n\) are i.i.d. with
\(\mathrm{E}(Y_i)=\mu_Y\) and \(\mathrm{Var}(Y_i) < \infty\), then
\(\overline{Y} \xrightarrow{\text{p}} \mu_Y\). 
</p>

<p>
The conditions for the LLN to be held is \(Y_i\) for \(i=1, \ldots, n\)
are i.i.d., and the variance of \(Y_i\) is finite. The latter says that
there is no extremely large outliers in the random samples. The
presence of outliers in the sample can substantially influence the
sample mean, rendering its convergence to be unachievable. 
</p>

<p>
Figure <a href="#org9e0ca15">10</a> shows that the convergence sample mean of random
samples from a Bernoulli distribution that \(\mathrm{Pr}(Y_i = 1) =
0.78\). As the number of sample increases, the sample mean (i.e., the
proportion that \(Y_i = 1\)) get close to the true mean of 0.78. 
</p>


<div id="org9e0ca15" class="figure">
<p><img src="figure/fig-2-8.png" alt="fig-2-8.png" width="600" />
</p>
<p><span class="figure-number">Figure 10: </span>An illustration of the law of large numbers</p>
</div>

<p>
Here is another demonstration of the law of large number,<sup><a id="fnr.6" class="footref" href="#fn.6">6</a></sup>
<a href="IllustratingTheLawOfLargeNumbers.cdf">IllustratingTheLawOfLargeNumbers.cdf</a>. To view this file,
first you need to download them by saving into your disk, then open
them with Wolfram CDF Player that can be downloaded from
<a href="http://www.wolfram.com/cdf-player/">http://www.wolfram.com/cdf-player/</a>.
</p>
</div>
</div>
</div>


<div id="outline-container-org115448e" class="outline-3">
<h3 id="org115448e"><span class="section-number-3">6.3</span> The central limit theorem</h3>
<div class="outline-text-3" id="text-6-3">
<p>
The LLN ensures the convergence of the sample mean from the sampling
distribution, while the <b>central limit theorem</b> (CLT) ensures the
convergence of the sampling distribution itself. 
</p>
</div>

<div id="outline-container-orgb738690" class="outline-4">
<h4 id="orgb738690">Convergence in distribution</h4>
<div class="outline-text-4" id="text-orgb738690">
<p>
Let \(F_1, F_2, \ldots, F_n\) be a sequence of cumulative distribution
functions corresponding to a sequence of random variables, \(S_1, S_2,
\ldots, S_n\). Then the sequence of random variables \({S_n}\) is said to
<b>converge in distribution</b> to a random variable \(S\) (denoted as \(S_n
\xrightarrow{\text{d}} S\)), if the distribution functions \(\{F_n\}\)
converge to \(F\) that is the distribution function of \(S\). We can write
it as
</p>

<p>
\[ S_n \xrightarrow{\text{d}} S \text{ if and only if } \lim_{n
\rightarrow \infty}F_n(x)=F(x) \]
</p>

<p>
where the limit holds at all points \(x\) at which the limiting
distribution \(F\) is continuous. The distribution \(F\) is called the
<b>asymptotic distribution</b> of \(S_n\).
</p>
</div>
</div>

<div id="outline-container-orgd4ed625" class="outline-4">
<h4 id="orgd4ed625">The central limit theorem (Lindeberg-Levy CLT)</h4>
<div class="outline-text-4" id="text-orgd4ed625">
<p>
The CLT states that if \(Y_1, Y_2, \ldots, Y_n\) are i.i.d. random samples from a
probability distribution with finite mean \(\mu_Y\) and finite variance
\(\sigma^2_Y\), i.e., \(0 < \sigma^2_Y < \infty\) and \(\overline{Y} =
(1/n)\sum_i^nY_i\). Then
</p>

<p>
\[ \sqrt{n}(\overline{Y}-\mu_Y) \xrightarrow{\text{d}} N(0,
\sigma^2_Y) \]
</p>

<p>
It follows that since \(\sigma_{\overline{Y}} =
\sqrt{\mathrm{Var}(\overline{Y})} = \sigma_Y/\sqrt{n}\),
\[ \frac{\overline{Y} - \mu_Y}{\sigma_{\overline{Y}}}
\xrightarrow{\text{ d}} N(0, 1) \]
</p>

<p>
Figure <a href="#org0d40cd5">11</a> shows that as \(n\) increases, the standardized sample
average of the samples from a Bernoulli distribution get close to a
normal distribution. 
</p>


<div id="org0d40cd5" class="figure">
<p><img src="figure/fig-2-9.png" alt="fig-2-9.png" width="600" />
</p>
<p><span class="figure-number">Figure 11: </span>An illustration of the central limit theorem</p>
</div>

<p>
Here is the demonstration of the CLT with Wolfram CDF Player,
<a href="IllustratingTheCentralLimitTheoremWithSumsOfBernoulliRandomV.cdf">IllustratingTheCentralLimitTheoremWithSumsOfBernoulliRandomV.cdf</a>.
</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
Source of Figure <a href="#org388bbb3">5</a>: Pishro-Nik, Hossein. (2017). Introduction to
Probability &amp; Statistics. Retrieved from
<a href="https://www.probabilitycourse.com/">https://www.probabilitycourse.com/</a>.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
Shalizi, Cosma. (2013). Reminder No. 1: Uncorrelated
vs. Independent. Retrieved from
<a href="http://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf">http://www.stat.cmu.edu/~cshalizi/uADA/13/reminders/uncorrelated-vs-independent.pdf</a>. 
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">
Source of Figure <a href="#org161ca56">6</a>: Characteristics of the
Normal Distribution. Retrieved from
<a href="http://howmed.net/community-medicine/normal-distribution-curve/">http://howmed.net/community-medicine/normal-distribution-curve/</a>. 
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">
The value of \(\Phi(x)\) can be found in a table for the standard
normal distribution, like the one in the following link,
<a href="https://www.thestudentroom.co.uk/attachment.php?attachmentid=134337&amp;d=1330530156">https://www.thestudentroom.co.uk/attachment.php?attachmentid=134337&amp;d=1330530156</a>. 
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5">5</a></sup> <div class="footpara"><p class="footpara">
Wikipedia. Statistical population. Retrieved from
<a href="https://en.wikipedia.org/wiki/Statistical_population">https://en.wikipedia.org/wiki/Statistical_population</a>. 
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6">6</a></sup> <div class="footpara"><p class="footpara">
This example and the one for the CLT are both retrieved from
<a href="http://demonstrations.wolfram.com/index.html">http://demonstrations.wolfram.com/index.html</a>. 
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Zheng Tian</p>
<p class="date">Created: 2017-02-27 Mon 10:36</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
