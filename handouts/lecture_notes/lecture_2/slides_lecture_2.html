<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 2: Review of Probability</title>
<meta name="author" content="(Zheng Tian)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="../../../reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="../../../reveal.js/css/theme/beige.css" id="theme"/>

<link rel="stylesheet" href="../../../reveal.js/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = '../../../reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1 class="title">Lecture 2: Review of Probability</h1><h2 class="author">Zheng Tian</h2><p class="date">Created: 2017-02-22 Wed 09:35</p>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-orgcd78bbc">Random Variables and Probability Distributions</a></li>
<li><a href="#/slide-orgd73911e">Expectation, Variance, and Other Moments</a></li>
<li><a href="#/slide-orge0c717c">Two Random Variables</a></li>
<li><a href="#/slide-org6a0b281">Four Specific Distributions</a></li>
</ul>
</div>
</div>
</section>


<section>
<section id="slide-orgcd78bbc">
<h2 id="orgcd78bbc">Random Variables and Probability Distributions</h2>
<div class="outline-text-2" id="text-orgcd78bbc">
</div></section>
</section>
<section>
<section id="slide-org27967d8">
<h3 id="org27967d8">Defining probabilities and random variables</h3>
<div class="outline-text-3" id="text-org27967d8">
</div></section>
<section id="slide-orgf4e07c4">
<h4 id="orgf4e07c4">Experiments and outcomes</h4>
<ul>
<li>An <b>experiment</b> is the processes that generate random results</li>
<li>The <b>outcomes</b> of an experiment are its
mutually exclusive potential results.</li>
<li>Example: tossing a coin. The outcome is either getting a head(H) or a tail(T)
but not both.</li>

</ul>

</section>
<section id="slide-org7e083f5">
<h4 id="org7e083f5">Sample space and events</h4>
<ul>
<li>A <b>sample space</b> consists of all the outcomes from an experiment,
denoted with the set \(S\).
<ul>
<li>\(S = \{H, T\}\) in the tossing-coin experiment.</li>

</ul></li>

<li>An <b>event</b> is a subset of the sample 
space. 
<ul>
<li>Getting a head is an event, which is \(\{H\} \subset \{H, T\}\).</li>

</ul></li>

</ul>

</section>
</section>
<section>
<section id="slide-org7162e2d">
<h3 id="org7162e2d">Probability</h3>
<div class="outline-text-3" id="text-org7162e2d">
</div></section>
<section id="slide-org3a661e1">
<h4 id="org3a661e1">An intuitive definition of probability</h4>
<ul>
<li>The <b>probability</b> of an event is the proportion of the time that the
event will occur in the long run.</li>

<li>For example, we toss a coin for \(n\)
times and get \(m\) heads. When \(n\) is very large, we can say that the
probability of getting a head in a toss is \(m/n\).</li>

</ul>

</section>
<section id="slide-org5753a0a">
<h4 id="org5753a0a">An axiomatic definition of probability</h4>
<ul>
<li>A probability of an event \(A\) in the sample space \(S\), denoted as
\(\mathrm{Pr}(A)\), is a function that assign \(A\) a real number in \([0,
  1]\), satisfying the following three conditions:
<ol>
<li>\(0 \leq \mathrm{Pr}(A) \leq 1\).</li>
<li>\(\mathrm{Pr}(S) = 1\).</li>
<li>For any disjoint sets, \(A\) and \(B\), that is \(A\) and \(B\) have no
element in common, \(\mathrm{Pr}(A \cup B) = \mathrm{Pr}(A) +
    \mathrm{Pr}(B)\).</li>

</ol></li>

</ul>

</section>
</section>
<section>
<section id="slide-org4f77f0b">
<h3 id="org4f77f0b">Random variables</h3>
<div class="outline-text-3" id="text-org4f77f0b">
</div></section>
<section id="slide-org680074b">
<h4 id="org680074b">The definition of random variables</h4>
<ul>
<li>A <b>random variable</b> is a numerical summary associated with the
outcomes of an experiment.</li>

<li>You can also think of a random variable as a function
mapping from an event \(\omega\) in the sample space \(\Omega\) to the
real line.</li>

</ul>

</section>
<section id="slide-org01cb744">
<h4 id="org01cb744">An illustration of random variables</h4>

<div id="org3890f23" class="figure">
<p><img src="figure/random_variable_demo1.png" alt="random_variable_demo1.png" width="600" />
</p>
<p><span class="figure-number">Figure 1: </span>An illustration of random variable</p>
</div>

</section>
<section id="slide-org328e6c5">
<h4 id="org328e6c5">Discrete and continuous random variables</h4>
<p>
Random variables can take different types of values
</p>

<ul>
<li>A <b>discrete</b> random
variables takes on a discrete set of values, like \(0, 1, 2, \ldots, n\)</li>
<li>A <b>continuous</b> random variable takes on a continuum of possble
values, like any value in the interval \((a, b)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgeb27a47">
<h3 id="orgeb27a47">Probability distributions</h3>
<div class="outline-text-3" id="text-orgeb27a47">
</div></section>
<section id="slide-org68e7e9d">
<h4 id="org68e7e9d">The probability distribution for a discrete random variable</h4>
<ul>
<li>The probability distribution of a discrete random variable is the list
of all possible values of the variable and the probability that each
value will occur. These probabilities sum to 1.</li>

<li><p>
The probability mass function. Let \(X\) be a discrete random
variable. The probability distribution of \(X\) (or the probability
mass function), \(p(x)\), is
</p>
<div>
\begin{equation*}
p(x) = \mathrm{Pr}(X = x)
\end{equation*}

</div></li>

<li>The axioms of probability require that 
<ol>
<li>\(0 \leq p(x) \leq 1\)</li>
<li>\( \sum_{i=1}^n p(x_i) = 1\).</li>

</ol></li>

</ul>

</section>
<section id="slide-org84bff21">
<h4 id="org84bff21">An example of the probability distribution of a discrete random variable</h4>
<table id="orgd8d10cc" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> An illustration of the probability distribution of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-right">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.75</td>
<td class="org-right">0.25</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

</section>
</section>
<section>
<section id="slide-orge5af8fa">
<h3 id="orge5af8fa">The cumulative probability distribution</h3>
<ul>
<li><p>
The <b>cumulative probability distribution</b> (or the cumulative
distribution function, c.d.f.): 
</p>

<p>
Let \(F(x)\) be the c.d.f of \(X\). Then \(F(x) = \mathrm{Pr}(X \leq x)\).
</p></li>

</ul>

</section>
</section>
<section>
<section id="slide-org6547167">
<h3 id="org6547167">The c.d.f. of a discrete random variable is a step function</h3>
<table id="orgf6a0d06" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> An illustration of the c.d.f. of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-left">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.50</td>
<td class="org-right">0.25</td>
<td class="org-left">1</td>
</tr>

<tr>
<td class="org-left">C.d.f.</td>
<td class="org-right">0.25</td>
<td class="org-right">0.75</td>
<td class="org-right">1</td>
<td class="org-left">--</td>
</tr>
</tbody>
</table>


<div id="orgecfe468" class="figure">
<p><img src="figure/cdf_discrete_example.png" alt="cdf_discrete_example.png" width="450" height="300" />
</p>
<p><span class="figure-number">Figure 2: </span>The c.d.f. of a discrete random variable</p>
</div>

</section>
</section>
<section>
<section id="slide-org75bafa6">
<h3 id="org75bafa6">Bernouli distribution</h3>
<p>
The Bernoulli distribution
</p>
<div>
\begin{equation*}
  G =
    \begin{cases}
      1 & \text{with probability } p \\
      0 & \text{with probability } 1-p
    \end{cases}
  \end{equation*}

</div>

</section>
</section>
<section>
<section id="slide-org6bcd659">
<h3 id="org6bcd659">The probability distribution of a continuous random variable</h3>
<div class="outline-text-3" id="text-org6bcd659">
</div></section>
<section id="slide-org7d2fc2b">
<h4 id="org7d2fc2b">Definition of the c.d.f. and the p.d.f. of a continuous random variable</h4>
<ul>
<li>The cumulative distribution function of a continous random variable
is defined as it is for a discrete random variable. 
\[ F(x) = \mathrm{Pr}(X \leq x) \]</li>

<li>The <b>probability density function (p.d.f.)</b> of \(X\) is the function
that satisfies
\[ F(x) = \int_{-\infty}^{x} f(t) \mathrm{d}t \text{ for all } x \]</li>

</ul>

</section>
<section id="slide-org57a5146">
<h4 id="org57a5146">Properties of the c.d.f.</h4>
<ul>
<li>For both discrete and continuous random variable, \(F(X)\) must satisfy
the following properties:
<ol>
<li>\(F(+\infty) = 1 \text{ and } F(-\infty) = 0\) (\(F(x)\) is bounded between 0 and 1)</li>
<li>\(x > y \Rightarrow F(x) \geq F(y)\) (\(F(x)\) is nondecreasing)</li>

</ol></li>

<li>By the definition of the c.d.f., we can conveniently calculate
probabilities, such as,
<ul>
<li>\(\mathrm{P}(x > a) = 1 - \mathrm{P}(x \leq a) = 1 - F(a)\)</li>
<li>\(\mathrm{P}(a < x \leq b) = F(b) - F(a)\).</li>

</ul></li>

</ul>

</section>
<section id="slide-org4cedac3">
<h4 id="org4cedac3">The c.d.f. and p.d.f. of a normal distribution</h4>

<div id="org6f161d6" class="figure">
<p><img src="figure/norm1.png" alt="norm1.png" width="500" height="450" />
</p>
<p><span class="figure-number">Figure 3: </span>The p.d.f. and c.d.f. of a continuous random variable (the normal distribution)</p>
</div>


</section>
</section>
<section>
<section id="slide-orgd73911e">
<h2 id="orgd73911e">Expectation, Variance, and Other Moments</h2>
<div class="outline-text-2" id="text-orgd73911e">
</div></section>
</section>
<section>
<section id="slide-orgb3203d3">
<h3 id="orgb3203d3">The expected value of a random variable</h3>
<div class="outline-text-3" id="text-orgb3203d3">
</div></section>
<section id="slide-orgea3b8d6">
<h4 id="orgea3b8d6">The expected value</h4>
<ul>
<li>The <b>expected value</b> of a random variable, X, denoted as \(\mathrm{E}(X)\), is
the long-run average of the random variable over many repeated
trials or occurrences, which is also called the <b>expectation</b> or the
<b>mean</b>.</li>

<li>The expected value measures the centrality of a random variable.</li>

</ul>

</section>
<section id="slide-org6755ff5">
<h4 id="org6755ff5">Mathematical definition</h4>
<ul>
<li>For a discrete random variable
\[ \mathrm{E}(X) = \sum_{i=1}^n x_i \mathrm{Pr}(X = x_i) \]</li>

<li>e.g. The expectation of a Bernoulli random variable, \(G\),
\[ \mathrm{E}(G) = 1 \cdot p + 0 \cdot (1-p) = p \]</li>

<li>For a continuous random variable
\[ \mathrm{E}(X) = \int_{-\infty}^{\infty} x f(x) \mathrm{d}x\]</li>

</ul>

</section>
</section>
<section>
<section id="slide-orga22e1fa">
<h3 id="orga22e1fa">The variance and standard deviation</h3>
<div class="outline-text-3" id="text-orga22e1fa">
</div></section>
<section id="slide-org0f7d002">
<h4 id="org0f7d002">Definition of variance and standard deviation</h4>
<ul>
<li>The <b>variance</b> of a random variable \(X\) measures its average
deviation from its own expected value.</li>

<li><p>
Let \(\mathrm{E}(X) = \mu_X\). Then the variance of \(X\),
</p>

<div>
\begin{align*}
\mathrm{Var}(X) & =  \sigma^2_X =  \mathrm{E}(X-\mu_X)^{2} \\
& = 
\begin{cases}
\sum_{i=1}^n (x - \mu_X)^{2}\mathrm{Pr}(X = x_i) & \text{if } X \text{ is discrete} \\
\int_{-\infty}^{\infty} (x - \mu_X)^{2}f(x)\mathrm{d} x  & \text{if } X \text{ is continuous}
\end{cases}
\end{align*}

</div></li>

<li>The <b>standard deviation</b> of \(X\): \(\sigma_{X} = \sqrt{\mathrm{Var}(X)}\)</li>

</ul>

</section>
<section id="slide-orgb74aab5">
<h4 id="orgb74aab5">Computing variance</h4>
<ul>
<li>A convenient formula for calculating the variance is
\[ \mathrm{Var}(X) = \mathrm{E}(X - \mu_X)^{2} = \mathrm{E}(X^{2}) - \mu_X^{2} \]</li>

<li>The variance of a Bernoulli random variable, \(G\)
\[ \mathrm{Var}(G) = (1-p)^{2}p + (0-p)^{2}(1-p) = p(1-p) \]</li>

</ul>

</section>
<section id="slide-orge4c3917">
<h4 id="orge4c3917">The expectation and variance of a linear function of \(X\)</h4>
<p>
Let \(Y = a + bX\), then
</p>
<ul>
<li>\(\mathrm{E}(Y) = a + \mathrm{E}(X)\)</li>
<li>\(\mathrm{Var}(Y) = \mathrm{Var}(a + b X) = b^{2} \mathrm{Var}(X)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgd3ea467">
<h3 id="orgd3ea467">Moments of a random variable, skewness and kurtosis</h3>
<div class="outline-text-3" id="text-orgd3ea467">
</div></section>
<section id="slide-org2fac022">
<h4 id="org2fac022">Definition of the moments of a distribution</h4>
<dl>
<dt>k<sup>th</sup> moment</dt><dd>The k<sup>th</sup> <b>moment</b> of the distribution of \(X\) is
\(\mathrm{E}(X^{k})\). So, the expectation is the "first"
moment of \(X\).</dd>

<dt>k<sup>th</sup> central moment</dt><dd>The k<sup>th</sup> central moment of the distribution
of \(X\) with its mean \(\mu_X\) is \(\mathrm{E}(X - \mu_X)^{k}\). So, the
variance is the second central moment of \(X\).</dd>

</dl>

<ul class="org-ul"><li><a id="org118f1c0"></a>A caveat<br  /><p>
It is important to remember that not all the moments of a distribution
exist. 
</p></li></ul>

</section>
<section id="slide-org3ff384a">
<h4 id="org3ff384a">Skewness</h4>
<ul>
<li><p>
The skewness of a distribution provides a mathematical way to describe
how much a distribution deviates from symmetry.
</p>

<p>
\[ \text{Skewness} =  \mathrm{E}(X - \mu_X)^{3}/\sigma_{X}^{3} \]
</p></li>

<li>A symmetric distribution has a skewness of zero.</li>
<li>The skewness can be either positive or negative.</li>
<li>That \(\mathrm{E}(X - \mu_X)^3\) is divided by \(\sigma^3_X\) is to make
the skewness measure unit free.</li>

</ul>

</section>
<section id="slide-org048246b">
<h4 id="org048246b">Kurtosis</h4>
<ul>
<li><p>
The kurtosis of the distribution of a random variable \(X\) measures how
much of the variance of \(X\) arises from extreme values, which makes
the distribution have "heavy" tails.
</p>

<p>
\[ \text{Kurtosis} = \mathrm{E}(X - \mu_X)^{4}/\sigma_{X}^{4} \]
</p></li>

<li>The kurtosis must be positive.</li>
<li>The kurtosis of the normal distribution is 3. So a distribution that
has its kurtosis exceeding 3 is called heavy-tailed.</li>
<li>The kurtosis is also unit free.</li>

</ul>

</section>
<section id="slide-org8cabed3">
<h4 id="org8cabed3">An illustration of skewness and kurtosis</h4>

<div class="figure">
<p><img src="figure/fig-2-3.png" alt="fig-2-3.png" width="550" height="450" />
</p>
</div>

<ul>
<li>All four distributions have a mean of zero and
a variance of one, while (a) and (b) are symmetric and (b)-(d) are
heavy-tailed.</li>

</ul>


</section>
</section>
<section>
<section id="slide-orge0c717c">
<h2 id="orge0c717c">Two Random Variables</h2>
<div class="outline-text-2" id="text-orge0c717c">
</div></section>
</section>
<section>
<section id="slide-org639f9fe">
<h3 id="org639f9fe">The joint and marginal distributions</h3>
<div class="outline-text-3" id="text-org639f9fe">
</div></section>
<section id="slide-orgc2de412">
<h4 id="orgc2de412">The joint probability function of two discrete random variables</h4>
<ul>
<li>The joint distribution of two random variables \(X\) and \(Y\) is
\[ p(x, y) = \mathrm{Pr}(X = x, Y = y)\]</li>

<li>\(p(x, y)\) must satisfy
<ol>
<li>\(p(x, y) \geq 0\)</li>
<li>\(\sum_{i=1}^n\sum_{j=1}^m p(x_i, y_j) = 1\) for all possible
combinations of values of \(X\) and \(Y\).</li>

</ol></li>

</ul>

</section>
<section id="slide-orgf0c60bf">
<h4 id="orgf0c60bf">The joint probability function of two continuous random variables</h4>
<ul>
<li>For two continuous random variables, \(X\) and \(Y\), the counterpart of \(p(x, y)\) is
the joint probability density function, \(f(x, y)\), such that
<ol>
<li>\(f(x, y) \geq 0\)</li>
<li>\(\int_{-\infty}^{{\infty}} \int_{-\infty}^{\infty} f(x, y)\, dx\, dy= 1\)</li>

</ol></li>

</ul>

</section>
<section id="slide-org203d4b9">
<h4 id="org203d4b9">The marginal probability distribution</h4>
<ul>
<li>The marginal probability distribution of a random variable \(X\) is
simply the probability distribution of its own.</li>

<li>For a discrete random variable, we can compute the marginal
distribution of \(X\) as
\[ \mathrm{Pr}(X=x) = \sum_{i=1}^n \mathrm{Pr}(X, Y=y_i) = \sum_{i=1}^n p(x, y_i)  \]</li>

<li>For a continuous random variable, the marginal distribution is
\[f_X(x) = \int_{-\infty}^{\infty} f(x, y)\, dy \]</li>

</ul>

</section>
<section id="slide-orgae70af6">
<h4 id="orgae70af6">An example of joint and marginal distributions</h4>
<table id="orgb32ab1b" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> Joint and marginal distributions of raining and commuting time</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Rain (\(X=0\))</th>
<th scope="col" class="org-right">No rain (\(X=1\))</th>
<th scope="col" class="org-right">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Long commute (\(Y=0\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.07</td>
<td class="org-right">0.22</td>
</tr>

<tr>
<td class="org-left">Short commute (\(Y=1\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.63</td>
<td class="org-right">0.78</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Total</td>
<td class="org-right">0.30</td>
<td class="org-right">0.70</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

</section>
</section>
<section>
<section id="slide-orgcc3bc51">
<h3 id="orgcc3bc51">Conditional distributions</h3>
<div class="outline-text-3" id="text-orgcc3bc51">
</div></section>
<section id="slide-org2f2b5d9">
<h4 id="org2f2b5d9">The conditional probability</h4>
<ul>
<li><p>
For any two events \(A\) and \(B\), the conditional probability of A given
B is defined as
</p>
<div>
\begin{equation*}
\mathrm{Pr}(A|B) = \frac{\mathrm{Pr}(A \cap B)}{\mathrm{Pr}(B)}
\end{equation*}

</div></li>

</ul>

</section>
<section id="slide-org8697cbf">
<h4 id="org8697cbf">Conditional probability illustrated</h4>

<div id="orgd584ddb" class="figure">
<p><img src="figure/conditional_probability.png" alt="conditional_probability.png" width="400" height="400" />
</p>
<p><span class="figure-number">Figure 5: </span>An illustration of conditional probability</p>
</div>

</section>
</section>
<section>
<section id="slide-org6e40590">
<h3 id="org6e40590">The conditional probability distribution</h3>
<ul>
<li>The conditional distribution of a random variable \(Y\) given another
random variable \(X\) is \(\mathrm{Pr}(Y | X=x)\).</li>

<li>The formula to compute it is
\[ \mathrm{Pr}(Y | X=x) = \frac{\mathrm{Pr}(X=x, Y)}{\mathrm{Pr}(X=x)} \]</li>

<li>For continuous random variables \(X\) and \(Y\), we define the conditional
density function as
\[ f(y|x) = \frac{f(x, y)}{f_X(x)} \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org45af0f8">
<h3 id="org45af0f8">The conditional expectation</h3>
<ul>
<li>The <b>conditional expectation</b> of \(Y\) given \(X\) is the expected value
of the conditional distribution of \(Y\) given \(X\).</li>

<li><p>
For discrete random variables, the conditional mean of \(Y\) given \(X=x\) is
</p>
<div>
\begin{equation*}
\mathrm{E}(Y \mid X=x) = \sum_{i=1}^n y_i \mathrm{Pr}(Y \mid X=x)
\end{equation*}

</div></li>

<li><p>
For continuous random variables, it is computed as
</p>
<div>
\begin{equation*}
\int_{-\infty}^{\infty} y f(y \mid x)\, dy
\end{equation*}

</div></li>

<li>The expected mean of commuting time given it is raining is \(0 \times
  0.1 + 1 \times 0.9 = 0.9\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org967cdff">
<h3 id="org967cdff">The law of iterated expectation</h3>
<ul>
<li><p>
<b>The law of iterated expectation</b>:
</p>

<p>
\[ \mathrm{E}(Y) = E \left[ \mathrm{E}(Y|X) \right] \]
</p></li>

<li>It says that the mean of \(Y\) is the weighted average of the
conditional expectation of \(Y\) given \(X\), weighted by the
probability distribution of \(X\). That is,
\[ \mathrm{E}(Y) = \sum_{i=1}^n \mathrm{E}(Y \mid X=x_i) \mathrm{Pr}(X=x_i) \]</li>

<li>If \(\mathrm{E}(X|Y) = 0\), then \(\mathrm{E}(X)=E\left[\mathrm{E}(X|Y)\right]=0\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org8e33637">
<h3 id="org8e33637">Conditional variance</h3>
<ul>
<li>With the conditional mean of \(Y\) given \(X\), we can compute the
conditional variance as
\[ \mathrm{Var}(Y \mid X=x) = \sum_{i=1}^n \left[ y_i - \mathrm{E}(Y \mid X=x)
  \right]^2 \mathrm{Pr}(Y=y_i \mid X=x) \]</li>

<li>From the law of iterated expectation, we can get the following
\[ \mathrm{Var}(Y) = \mathrm{E}(\mathrm{Var}(Y \mid X)) + \mathrm{Var}(\mathrm{E}(Y \mid
  X)) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgf9f612b">
<h3 id="orgf9f612b">Independent random variables</h3>
<ul>
<li>Two random variables \(X\) and \(Y\) are <b>independently distributed</b>, or
<b>independent</b>, if knowing the value of one of the variable provides no
information about the other.</li>
<li>Mathematically, it means that 
\[ \mathrm{Pr}(Y=y \mid X=x) = \mathrm{Pr}(Y=y)  \]</li>

<li>If \(X\) and \(Y\) are independent
\[ \mathrm{Pr}(Y=y, X=x) = \mathrm{Pr}(X=x) \mathrm{Pr}(Y=y) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgccd4ccb">
<h3 id="orgccd4ccb">Independence between two continuous random variable</h3>
<ul>
<li>For two continuous random variables, \(X\) and \(Y\), they are
<b>independent</b> if
\[ f(x|y) = f_{X}(x) \text{ or } f(y|x) = f_{Y}(y) \]</li>

<li>It follows that if \(X\) and \(Y\) are independent
\[ f(x, y) = f(x|y)f_{Y}(y) = f_{X}(x)f_{Y}(y) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgb00dba9">
<h3 id="orgb00dba9">Covariance and Correlation</h3>
<div class="outline-text-3" id="text-orgb00dba9">
</div></section>
<section id="slide-org30b56b7">
<h4 id="org30b56b7">Covariance</h4>
<ul>
<li><p>
The covariance of two discrete random variables \(X\) and \(Y\) is
</p>
<div>
\begin{align*}
\mathrm{Cov}(X, Y) & = \sigma_{XY} = \mathrm{E}(X-\mu_{X})(Y-\mu_{Y}) \\
                   & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i, Y=y_j)
\end{align*}

</div></li>

<li>For continous random variables, the covariance of \(X\) and \(Y\) is
\[ \mathrm{Cov}(X, Y) = \int_{-\infty}^{\infty}
  \int_{-\infty}^{\infty} (x-\mu_X)(y-\mu_y)f(x, y) dx dy \]</li>

<li>The covariance can also be computed as
\[ \mathrm{Cov}(X, Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y) \]</li>

</ul>

</section>
<section id="slide-org167b097">
<h4 id="org167b097">Correlation coefficient</h4>
<ul>
<li><p>
The <b>correlation coefficient</b> of \(X\) and \(Y\) is
</p>

<p>
\[ \mathrm{corr}(X, Y) = \rho_{XY} = \frac{\mathrm{Cov}(X, Y)}{\left[\mathrm{Var}(X)\mathrm{Var}(Y)\right]^{1/2}} =
  \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}} \]
</p></li>

<li>\(-1 \leq \mathrm{corr}(X, Y) \leq 1\).</li>

<li>\(\mathrm{corr}(X, Y)=0\) (or \(\mathrm{Cov}(X,Y)=0\)) means that \(X\)
and \(Y\) are uncorrelated.</li>

<li>Since \(\mathrm{Cov}(X, Y) = \mathrm{E}(XY) -
  \mathrm{E}(X)\mathrm{E}(Y)\), when \(X\) and \(Y\) are uncorrelated, then \(\mathrm{E}(XY) =
  \mathrm{E}(X) \mathrm{E}(Y)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgce6566f">
<h3 id="orgce6566f">Independence and uncorrelation</h3>
<ul>
<li><p>
If \(X\) and \(Y\) are independent, then
</p>
<div>
\begin{align*}
\mathrm{Cov}(X, Y) & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i) \mathrm{Pr}(Y=y_j) \\
                   & = \sum_{i=1}^n (x_i - \mu_X) \mathrm{Pr}(X=x_i) \sum_{j=1}^m (y_j - \mu_y) \mathrm{Pr}(Y=y_j) \\
                   & = 0 \times 0 = 0
\end{align*}

</div></li>

<li>That is, if \(X\) and \(Y\) are independent, they must be
uncorrelated.</li>

<li>However, the converse is not true. If \(X\) and \(Y\) are
uncorrelated, there is a possibility that they are actually
dependent.</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgbf55aa3">
<h3 id="orgbf55aa3">Conditional mean and correlation</h3>
<ul>
<li>If \(X\) and \(Y\) are independent, then we must have 
\(\mathrm{E}(Y \mid X) = \mathrm{E}(Y) = \mu_Y\)</li>

<li><p>
Then, we can prove that
\(\mathrm{Cov}(X, Y) = 0\) and \(\mathrm{corr}(X, Y)=0\).
</p>

<div>
\begin{align*}
\mathrm{E}(XY) & = \mathrm{E}(\mathrm{E}(XY \mid X)) = \mathrm{E}(X \mathrm{E}(Y \mid X)) \\
               & = \mathrm{E}(X) \mathrm{E}(Y \mid X) = \mathrm{E}(X) \mathrm{E}(Y)
\end{align*}

</div>

<p>
It follows that \(\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X)
   \mathrm{E}(Y) = 0\) and \(\mathrm{corr}(X, Y)=0\). 
</p></li>

</ul>

</section>
</section>
<section>
<section id="slide-orge0f636e">
<h3 id="orge0f636e">Some useful operations</h3>
<p>
The following properties
of \(\mathrm{E}(\cdot)\), \(\mathrm{Var}(\cdot)\) and
\(\mathrm{Cov}(\cdot)\) are useful in calculation,
</p>

<div>
\begin{align*}
\mathrm{E}(a + bX + cY)      & = a + b \mu_{X} + c \mu_{Y} \\
\mathrm{Var}(aX + bY)        & = a^{2} \sigma^{2}_{X} + b^{2} \sigma^{2}_{Y} + 2ab\sigma_{XY} \\
\mathrm{Cov}(a + bX + cV, Y) & = b\sigma_{XY} + c\sigma_{VY} \\
\end{align*}

</div>


</section>
</section>
<section>
<section id="slide-org6a0b281">
<h2 id="org6a0b281">Four Specific Distributions</h2>
<div class="outline-text-2" id="text-org6a0b281">
</div></section>
</section>
<section>
<section id="slide-orgb5bf722">
<h3 id="orgb5bf722">The normal distribution</h3>
<div class="outline-text-3" id="text-orgb5bf722">
</div></section>
<section id="slide-orgc38c447">
<h4 id="orgc38c447">The normal distribution</h4>
<ul>
<li>The p.d.f. of a normally distributed random variable \(X\) is
\[ f(x) =
  \frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right]
  \]</li>
<li>\(\mathrm{E}(X) = \mu\) and \(\mathrm{Var}(X) = \sigma^{2}\).</li>
<li>We write \(X \sim N(\mu, \sigma^{2})\)</li>

</ul>

</section>
<section id="slide-org9c68943">
<h4 id="org9c68943">The standard normal distribution</h4>
<ul>
<li>The standard normal distribution is a special case of the normal
distribution, for which \(\mu = 0\) and \(\sigma = 0\). The p.d.f of the
standard normal distribution is
\[
  \phi(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)
  \]</li>
<li>The c.d.f of the standard normal distribution is often denoted as
\(\Phi(x)\).</li>

</ul>

</section>
<section id="slide-orgc6a0466">
<h4 id="orgc6a0466">Symmetric and skinny tails</h4>
<ul>
<li>The normal distribution is symmetric around its mean, \(\mu\), with the
skewness equal 0</li>
<li>It has 95% of its probability between
\(\mu-1.96\sigma\) and \(\mu+1.96\sigma\), with the kurtosis
equal 3.</li>

</ul>


<div id="orgacfaf89" class="figure">
<p><img src="figure/Normal-distribution-curve.jpg" alt="Normal-distribution-curve.jpg" />
</p>
<p><span class="figure-number">Figure 6: </span>The normal probability density</p>
</div>

</section>
<section id="slide-org25e728a">
<h4 id="org25e728a">Transforming a normally distributed random variable to the standard normal distribution</h4>
<ul>
<li>Let \(X\) be a random variable with a normal distribution, i.e., \(X \sim
  N(\mu, \sigma^2)\).</li>
<li>We compute \(Z = (X-\mu)/\sigma\), which follows
the standard normal distribution, \(N(0, 1)\).</li>
<li>For example, if \(X \sim N(1, 4)\), then \(Z = (X-1)/2 \sim N(0,
  1)\). When we want to find \(\mathrm{Pr}(X \leq 4)\), we only need to
compute \(\Phi(3/2)\)</li>
<li><p>
Generally, for any two number \(c_1 < c_2\) and let \(d_1 = (c_1 - \mu)/\sigma\) and
\(d_2 = (c_2 - \mu)/\sigma\), we have
</p>
<div>
\begin{align*}
\mathrm{Pr}(X \leq c_2) & = \mathrm{Pr}(Z \leq d_2) = \Phi(d_2) \\
\mathrm{Pr}(X \geq c_1) & = \mathrm{Pr}(Z \geq d_1) = 1 - \Phi(d_1) \\
\mathrm{Pr}(c_1 \leq X \leq c_2) & = \mathrm{Pr}(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1)
\end{align*}

</div></li>

</ul>

</section>
<section id="slide-orgab0d946">
<h4 id="orgab0d946">The multivariate normal distribution</h4>
<ul>
<li>The multivariate normal distribution is the joint
distribution of a set of random variables.</li>

<li>The p.d.f. of the multivariate normal distribution is beyond the
scope of this course, but the following properties make this
distribution handy in analysis.</li>

</ul>

</section>
<section id="slide-orgeda228c">
<h4 id="orgeda228c">Important properties of the multivariate normal distribution</h4>
<ul>
<li>If n random variables, \(x_1, \ldots, x_n\), have a multivariate
normal distribution, then any linear combination of these variables
is normally distributed. For any real numbers, \(\alpha_1, \ldots,
  \alpha_n\), a linear combination of \({x_i}\) is \(\sum_i \alpha_i x_i\).</li>

<li>If a set of random variables has a multivariate normal
distribution, then the marginal distribution of each of the
variables is normal.</li>

<li>If random variables with a multivariate normal distribution have
covariances that equal zero, then these random variables are
independent.</li>

<li>If \(X\) and \(Y\) have a bivariate normal distribution, then
\(\mathrm{E}(Y|X = x) = a + bx\), where \(a\) and \(b\) are constants.</li>

</ul>

</section>
</section>
<section>
<section id="slide-org1384d82">
<h3 id="org1384d82">The chi-squared distribution</h3>
<ul>
<li>Let \(Z_1, \ldots, Z_n\) be n indepenent standard normal distribution,
i.e. \(Z_i \sim N(0, 1)\) for all \(i = 1, \ldots, n\). Then, the random
variable
\[W = \sum_{i=1}^n Z^2_i \]
has a chi-squared distribution with \(n\) degrees of freedom, denoted as
\(W \sim \chi^2(n)\), with \(\mathrm{E}(W) = n\) and \(\mathrm{Var}(W) = 2n\)</li>

<li>If \(Z \sim N(0, 1)\), then \(W = Z^2 \sim \chi^2(1)\) with \(\mathrm{E}(W) =
  1\) and \(\mathrm{Var}(W) = 2\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org3b0d849">
<h3 id="org3b0d849">The student t distribution</h3>
<ul>
<li>Let \(Z \sim N(0, 1)\), \(W \sim \chi^2(m)\), and \(Z\) and \(W\) be
independently distributed. Then, the random variable
\[t = \frac{Z}{\sqrt{W/m}} \]
has a student t distribution with \(m\) degrees of freedom, denoted as
\(t \sim t(m)\).</li>

<li>As \(n\) increases, \(t\) gets close to a standard normal distribution.</li>

</ul>

</section>
</section>
<section>
<section id="slide-org1247cf0">
<h3 id="org1247cf0">The F distribution</h3>
<ul>
<li>Let \(W_1 \sim \chi^2(n_1)\), \(W_2 \sim \chi^2(n_2)\), and \(W_1\) and
\(W_2\) are independent. Then, the random variable
\[ F = \frac{W_1/n_1}{W_2/n_2}\]
has an F distribution with \((n_1, n_2)\) degrees of freedom, denoted as
\(F \sim F(n_1, n_2)\)</li>

<li>If \(t \sim t(n)\), then \(t^2 \sim F(1, n)\)</li>

<li>As \(n_2 \rightarrow \infty\), the \(F(n_1, \infty)\) distribution is the
same as the \(\chi^2(n_1)\) distribution divided by \(n_1\).</li>

</ul>
</section>
</section>
</div>
</div>
<script src="../../../reveal.js/lib/js/head.min.js"></script>
<script src="../../../reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: true,
keyboard: true,
overview: true,
width: 1000,
height: 800,
margin: 0.20,
minScale: 0.50,
maxScale: 2.50,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'cube', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
{ src: '../../../reveal.js/plugin/menu/menu.js' },
 { src: '../../../reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: '../../../reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: '../../../reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
