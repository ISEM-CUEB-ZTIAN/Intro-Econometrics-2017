<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 2: Review of Probability</title>
<meta name="author" content="(Zheng Tian)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="../../../reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="../../../reveal.js/css/theme/beige.css" id="theme"/>

<link rel="stylesheet" href="../../../reveal.js/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = '../../../reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1 class="title">Lecture 2: Review of Probability</h1><h2 class="author">Zheng Tian</h2><p class="date">Created: 2017-02-26 Sun 21:20</p>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-org0568a54">Random Variables and Probability Distributions</a></li>
<li><a href="#/slide-orgbbdc63a">Expectation, Variance, and Other Moments</a></li>
<li><a href="#/slide-org8dacfd1">Two Random Variables</a></li>
<li><a href="#/slide-org26a7942">Four Specific Distributions</a></li>
<li><a href="#/slide-orgb349d41">Random Sampling and the Distribution of the Sample Average</a></li>
<li><a href="#/slide-org5c00560">Large Sample Approximations to Sampling Distributions</a></li>
</ul>
</div>
</div>
</section>


<section>
<section id="slide-org0568a54">
<h2 id="org0568a54">Random Variables and Probability Distributions</h2>
<div class="outline-text-2" id="text-org0568a54">
</div></section>
</section>
<section>
<section id="slide-org17bc99b">
<h3 id="org17bc99b">Defining probabilities and random variables</h3>
<div class="outline-text-3" id="text-org17bc99b">
</div></section>
<section id="slide-orgdf135a0">
<h4 id="orgdf135a0">Experiments and outcomes</h4>
<ul>
<li>An <b>experiment</b> is the processes that generate random results</li>
<li>The <b>outcomes</b> of an experiment are its
mutually exclusive potential results.</li>
<li>Example: tossing a coin. The outcome is either getting a head(H) or a tail(T)
but not both.</li>

</ul>

</section>
<section id="slide-orge773062">
<h4 id="orge773062">Sample space and events</h4>
<ul>
<li>A <b>sample space</b> consists of all the outcomes from an experiment,
denoted with the set \(S\).
<ul>
<li>\(S = \{H, T\}\) in the tossing-coin experiment.</li>

</ul></li>

<li>An <b>event</b> is a subset of the sample 
space. 
<ul>
<li>Getting a head is an event, which is \(\{H\} \subset \{H, T\}\).</li>

</ul></li>

</ul>

</section>
</section>
<section>
<section id="slide-orge50f67e">
<h3 id="orge50f67e">Probability</h3>
<div class="outline-text-3" id="text-orge50f67e">
</div></section>
<section id="slide-orga0584df">
<h4 id="orga0584df">An intuitive definition of probability</h4>
<ul>
<li>The <b>probability</b> of an event is the proportion of the time that the
event will occur in the long run.</li>

<li>For example, we toss a coin for \(n\)
times and get \(m\) heads. When \(n\) is very large, we can say that the
probability of getting a head in a toss is \(m/n\).</li>

</ul>

</section>
<section id="slide-org96773b8">
<h4 id="org96773b8">An axiomatic definition of probability</h4>
<ul>
<li>A probability of an event \(A\) in the sample space \(S\), denoted as
\(\mathrm{Pr}(A)\), is a function that assign \(A\) a real number in \([0,
  1]\), satisfying the following three conditions:
<ol>
<li>\(0 \leq \mathrm{Pr}(A) \leq 1\).</li>
<li>\(\mathrm{Pr}(S) = 1\).</li>
<li>For any disjoint sets, \(A\) and \(B\), that is \(A\) and \(B\) have no
element in common, \(\mathrm{Pr}(A \cup B) = \mathrm{Pr}(A) +
    \mathrm{Pr}(B)\).</li>

</ol></li>

</ul>

</section>
</section>
<section>
<section id="slide-org2089aec">
<h3 id="org2089aec">Random variables</h3>
<div class="outline-text-3" id="text-org2089aec">
</div></section>
<section id="slide-orgd1c0c21">
<h4 id="orgd1c0c21">The definition of random variables</h4>
<ul>
<li>A <b>random variable</b> is a numerical summary associated with the
outcomes of an experiment.</li>

<li>You can also think of a random variable as a function
mapping from an event \(\omega\) in the sample space \(\Omega\) to the
real line.</li>

</ul>

</section>
<section id="slide-orge84149b">
<h4 id="orge84149b">An illustration of random variables</h4>

<div id="org83aec0f" class="figure">
<p><img src="figure/random_variable_demo1.png" alt="random_variable_demo1.png" width="600" />
</p>
<p><span class="figure-number">Figure 1: </span>An illustration of random variable</p>
</div>

</section>
<section id="slide-org7f9b728">
<h4 id="org7f9b728">Discrete and continuous random variables</h4>
<p>
Random variables can take different types of values
</p>

<ul>
<li>A <b>discrete</b> random
variables takes on a discrete set of values, like \(0, 1, 2, \ldots, n\)</li>
<li>A <b>continuous</b> random variable takes on a continuum of possble
values, like any value in the interval \((a, b)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org563411e">
<h3 id="org563411e">Probability distributions</h3>
<div class="outline-text-3" id="text-org563411e">
</div></section>
<section id="slide-org9468fc2">
<h4 id="org9468fc2">The probability distribution for a discrete random variable</h4>
<ul>
<li>The probability distribution of a discrete random variable is the list
of all possible values of the variable and the probability that each
value will occur. These probabilities sum to 1.</li>

<li><p>
The probability mass function. Let \(X\) be a discrete random
variable. The probability distribution of \(X\) (or the probability
mass function), \(p(x)\), is
</p>
<div>
\begin{equation*}
p(x) = \mathrm{Pr}(X = x)
\end{equation*}

</div></li>

<li>The axioms of probability require that 
<ol>
<li>\(0 \leq p(x) \leq 1\)</li>
<li>\( \sum_{i=1}^n p(x_i) = 1\).</li>

</ol></li>

</ul>

</section>
<section id="slide-orge066dc0">
<h4 id="orge066dc0">An example of the probability distribution of a discrete random variable</h4>
<table id="orgffddb0b" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> An illustration of the probability distribution of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-right">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.50</td>
<td class="org-right">0.25</td>
<td class="org-right">1.</td>
</tr>
</tbody>
</table>

</section>
</section>
<section>
<section id="slide-org7d5cdfd">
<h3 id="org7d5cdfd">The cumulative probability distribution</h3>
<ul>
<li><p>
The <b>cumulative probability distribution</b> (or the cumulative
distribution function, c.d.f.): 
</p>

<p>
Let \(F(x)\) be the c.d.f of \(X\). Then \(F(x) = \mathrm{Pr}(X \leq x)\).
</p></li>

</ul>

</section>
</section>
<section>
<section id="slide-org89eebd6">
<h3 id="org89eebd6">The c.d.f. of a discrete random variable is a step function</h3>
<table id="org18ed324" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> An illustration of the c.d.f. of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-left">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.50</td>
<td class="org-right">0.25</td>
<td class="org-left">1</td>
</tr>

<tr>
<td class="org-left">C.d.f.</td>
<td class="org-right">0.25</td>
<td class="org-right">0.75</td>
<td class="org-right">1</td>
<td class="org-left">--</td>
</tr>
</tbody>
</table>


<div id="org6b79e3d" class="figure">
<p><img src="figure/cdf_discrete_example.png" alt="cdf_discrete_example.png" width="450" height="300" />
</p>
<p><span class="figure-number">Figure 2: </span>The c.d.f. of a discrete random variable</p>
</div>

</section>
</section>
<section>
<section id="slide-orgc43eb92">
<h3 id="orgc43eb92">Bernouli distribution</h3>
<p>
The Bernoulli distribution
</p>
<div>
\begin{equation*}
  G =
    \begin{cases}
      1 & \text{with probability } p \\
      0 & \text{with probability } 1-p
    \end{cases}
  \end{equation*}

</div>

</section>
</section>
<section>
<section id="slide-orgd86d5b4">
<h3 id="orgd86d5b4">The probability distribution of a continuous random variable</h3>
<div class="outline-text-3" id="text-orgd86d5b4">
</div></section>
<section id="slide-org2f2d872">
<h4 id="org2f2d872">Definition of the c.d.f. and the p.d.f. of a continuous random variable</h4>
<ul>
<li>The cumulative distribution function of a continous random variable
is defined as it is for a discrete random variable. 
\[ F(x) = \mathrm{Pr}(X \leq x) \]</li>

<li>The <b>probability density function (p.d.f.)</b> of \(X\) is the function
that satisfies
\[ F(x) = \int_{-\infty}^{x} f(t) \mathrm{d}t \text{ for all } x \]</li>

</ul>

</section>
<section id="slide-orga1f8ffe">
<h4 id="orga1f8ffe">Properties of the c.d.f.</h4>
<ul>
<li>For both discrete and continuous random variable, \(F(X)\) must satisfy
the following properties:
<ol>
<li>\(F(+\infty) = 1 \text{ and } F(-\infty) = 0\) (\(F(x)\) is bounded between 0 and 1)</li>
<li>\(x > y \Rightarrow F(x) \geq F(y)\) (\(F(x)\) is nondecreasing)</li>

</ol></li>

<li>By the definition of the c.d.f., we can conveniently calculate
probabilities, such as,
<ul>
<li>\(\mathrm{P}(x > a) = 1 - \mathrm{P}(x \leq a) = 1 - F(a)\)</li>
<li>\(\mathrm{P}(a < x \leq b) = F(b) - F(a)\).</li>

</ul></li>

</ul>

</section>
<section id="slide-org4615dcc">
<h4 id="org4615dcc">The c.d.f. and p.d.f. of a normal distribution</h4>

<div id="org12b6525" class="figure">
<p><img src="figure/norm1.png" alt="norm1.png" width="500" height="450" />
</p>
<p><span class="figure-number">Figure 3: </span>The p.d.f. and c.d.f. of a continuous random variable (the normal distribution)</p>
</div>


</section>
</section>
<section>
<section id="slide-orgbbdc63a">
<h2 id="orgbbdc63a">Expectation, Variance, and Other Moments</h2>
<div class="outline-text-2" id="text-orgbbdc63a">
</div></section>
</section>
<section>
<section id="slide-orgcd5114b">
<h3 id="orgcd5114b">The expected value of a random variable</h3>
<div class="outline-text-3" id="text-orgcd5114b">
</div></section>
<section id="slide-orgb0672d5">
<h4 id="orgb0672d5">The expected value</h4>
<ul>
<li>The <b>expected value</b> of a random variable, X, denoted as \(\mathrm{E}(X)\), is
the long-run average of the random variable over many repeated
trials or occurrences, which is also called the <b>expectation</b> or the
<b>mean</b>.</li>

<li>The expected value measures the centrality of a random variable.</li>

</ul>

</section>
<section id="slide-orga639738">
<h4 id="orga639738">Mathematical definition</h4>
<ul>
<li>For a discrete random variable
\[ \mathrm{E}(X) = \sum_{i=1}^n x_i \mathrm{Pr}(X = x_i) \]</li>

<li>e.g. The expectation of a Bernoulli random variable, \(G\),
\[ \mathrm{E}(G) = 1 \cdot p + 0 \cdot (1-p) = p \]</li>

<li>For a continuous random variable
\[ \mathrm{E}(X) = \int_{-\infty}^{\infty} x f(x) \mathrm{d}x\]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org849fe29">
<h3 id="org849fe29">The variance and standard deviation</h3>
<div class="outline-text-3" id="text-org849fe29">
</div></section>
<section id="slide-org8455830">
<h4 id="org8455830">Definition of variance and standard deviation</h4>
<ul>
<li>The <b>variance</b> of a random variable \(X\) measures its average
deviation from its own expected value.</li>

<li><p>
Let \(\mathrm{E}(X) = \mu_X\). Then the variance of \(X\),
</p>

<div>
\begin{align*}
\mathrm{Var}(X) & =  \sigma^2_X =  \mathrm{E}(X-\mu_X)^{2} \\
& = 
\begin{cases}
\sum_{i=1}^n (x_i - \mu_X)^{2}\mathrm{Pr}(X = x_i) & \text{if } X \text{ is discrete} \\
\int_{-\infty}^{\infty} (x - \mu_X)^{2}f(x)\mathrm{d} x  & \text{if } X \text{ is continuous}
\end{cases}
\end{align*}

</div></li>

<li>The <b>standard deviation</b> of \(X\): \(\sigma_{X} = \sqrt{\mathrm{Var}(X)}\)</li>

</ul>

</section>
<section id="slide-orgb4e3968">
<h4 id="orgb4e3968">Computing variance</h4>
<ul>
<li>A convenient formula for calculating the variance is
\[ \mathrm{Var}(X) = \mathrm{E}(X - \mu_X)^{2} = \mathrm{E}(X^{2}) - \mu_X^{2} \]</li>

<li>The variance of a Bernoulli random variable, \(G\)
\[ \mathrm{Var}(G) = (1-p)^{2}p + (0-p)^{2}(1-p) = p(1-p) \]</li>

</ul>

</section>
<section id="slide-org1a54a0f">
<h4 id="org1a54a0f">The expectation and variance of a linear function of \(X\)</h4>
<p>
Let \(Y = a + bX\), then
</p>
<ul>
<li>\(\mathrm{E}(Y) = a + b\mathrm{E}(X)\)</li>
<li>\(\mathrm{Var}(Y) = \mathrm{Var}(a + b X) = b^{2} \mathrm{Var}(X)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-orge3a84b4">
<h3 id="orge3a84b4">Moments of a random variable, skewness and kurtosis</h3>
<div class="outline-text-3" id="text-orge3a84b4">
</div></section>
<section id="slide-org61073e2">
<h4 id="org61073e2">Definition of the moments of a distribution</h4>
<dl>
<dt>k<sup>th</sup> moment</dt><dd>The k<sup>th</sup> <b>moment</b> of the distribution of \(X\) is
\(\mathrm{E}(X^{k})\). So, the expectation is the "first"
moment of \(X\).</dd>

<dt>k<sup>th</sup> central moment</dt><dd>The k<sup>th</sup> central moment of the distribution
of \(X\) with its mean \(\mu_X\) is \(\mathrm{E}(X - \mu_X)^{k}\). So, the
variance is the second central moment of \(X\).</dd>

</dl>

<ul class="org-ul"><li><a id="orgda7431c"></a>A caveat<br  /><p>
It is important to remember that not all the moments of a distribution
exist. 
</p></li></ul>

</section>
<section id="slide-org2be55e5">
<h4 id="org2be55e5">Skewness</h4>
<ul>
<li><p>
The skewness of a distribution provides a mathematical way to describe
how much a distribution deviates from symmetry.
</p>

<p>
\[ \text{Skewness} =  \mathrm{E}(X - \mu_X)^{3}/\sigma_{X}^{3} \]
</p></li>

<li>A symmetric distribution has a skewness of zero.</li>
<li>The skewness can be either positive or negative.</li>
<li>That \(\mathrm{E}(X - \mu_X)^3\) is divided by \(\sigma^3_X\) is to make
the skewness measure unit free.</li>

</ul>

</section>
<section id="slide-org6dc88eb">
<h4 id="org6dc88eb">Kurtosis</h4>
<ul>
<li><p>
The kurtosis of the distribution of a random variable \(X\) measures how
much of the variance of \(X\) arises from extreme values, which makes
the distribution have "heavy" tails.
</p>

<p>
\[ \text{Kurtosis} = \mathrm{E}(X - \mu_X)^{4}/\sigma_{X}^{4} \]
</p></li>

<li>The kurtosis must be positive.</li>
<li>The kurtosis of the normal distribution is 3. So a distribution that
has its kurtosis exceeding 3 is called heavy-tailed.</li>
<li>The kurtosis is also unit free.</li>

</ul>

</section>
<section id="slide-org5bb56a7">
<h4 id="org5bb56a7">An illustration of skewness and kurtosis</h4>

<div class="figure">
<p><img src="figure/fig-2-3.png" alt="fig-2-3.png" width="550" height="450" />
</p>
</div>

<ul>
<li>All four distributions have a mean of zero and
a variance of one, while (a) and (b) are symmetric and (b)-(d) are
heavy-tailed.</li>

</ul>


</section>
</section>
<section>
<section id="slide-org8dacfd1">
<h2 id="org8dacfd1">Two Random Variables</h2>
<div class="outline-text-2" id="text-org8dacfd1">
</div></section>
</section>
<section>
<section id="slide-org19039aa">
<h3 id="org19039aa">The joint and marginal distributions</h3>
<div class="outline-text-3" id="text-org19039aa">
</div></section>
<section id="slide-org485b417">
<h4 id="org485b417">The joint probability function of two discrete random variables</h4>
<ul>
<li>The joint distribution of two random variables \(X\) and \(Y\) is
\[ p(x, y) = \mathrm{Pr}(X = x, Y = y)\]</li>

<li>\(p(x, y)\) must satisfy
<ol>
<li>\(p(x, y) \geq 0\)</li>
<li>\(\sum_{i=1}^n\sum_{j=1}^m p(x_i, y_j) = 1\) for all possible
combinations of values of \(X\) and \(Y\).</li>

</ol></li>

</ul>

</section>
<section id="slide-org714876b">
<h4 id="org714876b">The joint probability function of two continuous random variables</h4>
<ul>
<li>For two continuous random variables, \(X\) and \(Y\), the counterpart of \(p(x, y)\) is
the joint probability density function, \(f(x, y)\), such that
<ol>
<li>\(f(x, y) \geq 0\)</li>
<li>\(\int_{-\infty}^{{\infty}} \int_{-\infty}^{\infty} f(x, y)\, dx\, dy= 1\)</li>

</ol></li>

</ul>

</section>
<section id="slide-org2556c2f">
<h4 id="org2556c2f">The marginal probability distribution</h4>
<ul>
<li>The marginal probability distribution of a random variable \(X\) is
simply the probability distribution of its own.</li>

<li>For a discrete random variable, we can compute the marginal
distribution of \(X\) as
\[ \mathrm{Pr}(X=x) = \sum_{i=1}^n \mathrm{Pr}(X, Y=y_i) = \sum_{i=1}^n p(x, y_i)  \]</li>

<li>For a continuous random variable, the marginal distribution is
\[f_X(x) = \int_{-\infty}^{\infty} f(x, y)\, dy \]</li>

</ul>

</section>
<section id="slide-org8cbc4bb">
<h4 id="org8cbc4bb">An example of joint and marginal distributions</h4>
<table id="orgd73419e" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> Joint and marginal distributions of raining and commuting time</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Rain (\(X=0\))</th>
<th scope="col" class="org-right">No rain (\(X=1\))</th>
<th scope="col" class="org-right">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Long commute (\(Y=0\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.07</td>
<td class="org-right">0.22</td>
</tr>

<tr>
<td class="org-left">Short commute (\(Y=1\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.63</td>
<td class="org-right">0.78</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Total</td>
<td class="org-right">0.30</td>
<td class="org-right">0.70</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

</section>
</section>
<section>
<section id="slide-org8c8a24b">
<h3 id="org8c8a24b">Conditional distributions</h3>
<div class="outline-text-3" id="text-org8c8a24b">
</div></section>
<section id="slide-org0c06b71">
<h4 id="org0c06b71">The conditional probability</h4>
<ul>
<li><p>
For any two events \(A\) and \(B\), the conditional probability of A given
B is defined as
</p>
<div>
\begin{equation*}
\mathrm{Pr}(A|B) = \frac{\mathrm{Pr}(A \cap B)}{\mathrm{Pr}(B)}
\end{equation*}

</div></li>

</ul>

</section>
<section id="slide-orge972d46">
<h4 id="orge972d46">Conditional probability illustrated</h4>

<div id="orgfc74e3c" class="figure">
<p><img src="figure/conditional_probability.png" alt="conditional_probability.png" width="400" height="400" />
</p>
<p><span class="figure-number">Figure 5: </span>An illustration of conditional probability</p>
</div>

</section>
</section>
<section>
<section id="slide-orgfc27996">
<h3 id="orgfc27996">The conditional probability distribution</h3>
<ul>
<li>The conditional distribution of a random variable \(Y\) given another
random variable \(X\) is \(\mathrm{Pr}(Y | X=x)\).</li>

<li>The formula to compute it is
\[ \mathrm{Pr}(Y | X=x) = \frac{\mathrm{Pr}(X=x, Y)}{\mathrm{Pr}(X=x)} \]</li>

<li>For continuous random variables \(X\) and \(Y\), we define the conditional
density function as
\[ f(y|x) = \frac{f(x, y)}{f_X(x)} \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org99ff9d9">
<h3 id="org99ff9d9">The conditional expectation</h3>
<ul>
<li>The <b>conditional expectation</b> of \(Y\) given \(X\) is the expected value
of the conditional distribution of \(Y\) given \(X\).</li>

<li><p>
For discrete random variables, the conditional mean of \(Y\) given \(X=x\) is
</p>
<div>
\begin{equation*}
\mathrm{E}(Y \mid X=x) = \sum_{i=1}^n y_i \mathrm{Pr}(Y=y_i \mid X=x)
\end{equation*}

</div></li>

<li><p>
For continuous random variables, it is computed as
</p>
<div>
\begin{equation*}
\int_{-\infty}^{\infty} y f(y \mid x)\, dy
\end{equation*}

</div></li>

<li>The expected mean of commuting time given it is raining is \(0 \times
  0.1 + 1 \times 0.9 = 0.9\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org2a847e9">
<h3 id="org2a847e9">The law of iterated expectation</h3>
<ul>
<li><p>
<b>The law of iterated expectation</b>:
</p>

<p>
\[ \mathrm{E}(Y) = E \left[ \mathrm{E}(Y|X) \right] \]
</p></li>

<li>It says that the mean of \(Y\) is the weighted average of the
conditional expectation of \(Y\) given \(X\), weighted by the
probability distribution of \(X\). That is,
\[ \mathrm{E}(Y) = \sum_{i=1}^n \mathrm{E}(Y \mid X=x_i) \mathrm{Pr}(X=x_i) \]</li>

<li>If \(\mathrm{E}(X|Y) = 0\), then \(\mathrm{E}(X)=E\left[\mathrm{E}(X|Y)\right]=0\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org0300d16">
<h3 id="org0300d16">Conditional variance</h3>
<ul>
<li>With the conditional mean of \(Y\) given \(X\), we can compute the
conditional variance as
\[ \mathrm{Var}(Y \mid X=x) = \sum_{i=1}^n \left[ y_i - \mathrm{E}(Y \mid X=x)
  \right]^2 \mathrm{Pr}(Y=y_i \mid X=x) \]</li>

<li>From the law of iterated expectation, we can get the following
\[ \mathrm{Var}(Y) = \mathrm{E}(\mathrm{Var}(Y \mid X)) + \mathrm{Var}(\mathrm{E}(Y \mid
  X)) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org2f531a3">
<h3 id="org2f531a3">Independent random variables</h3>
<ul>
<li>Two random variables \(X\) and \(Y\) are <b>independently distributed</b>, or
<b>independent</b>, if knowing the value of one of the variable provides no
information about the other.</li>
<li>Mathematically, it means that 
\[ \mathrm{Pr}(Y=y \mid X=x) = \mathrm{Pr}(Y=y)  \]</li>

<li>If \(X\) and \(Y\) are independent
\[ \mathrm{Pr}(Y=y, X=x) = \mathrm{Pr}(X=x) \mathrm{Pr}(Y=y) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgf0883c4">
<h3 id="orgf0883c4">Independence between two continuous random variable</h3>
<ul>
<li>For two continuous random variables, \(X\) and \(Y\), they are
<b>independent</b> if
\[ f(x|y) = f_{X}(x) \text{ or } f(y|x) = f_{Y}(y) \]</li>

<li>It follows that if \(X\) and \(Y\) are independent
\[ f(x, y) = f(x|y)f_{Y}(y) = f_{X}(x)f_{Y}(y) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org1b10c97">
<h3 id="org1b10c97">Covariance and Correlation</h3>
<div class="outline-text-3" id="text-org1b10c97">
</div></section>
<section id="slide-org2d5397a">
<h4 id="org2d5397a">Covariance</h4>
<ul>
<li><p>
The covariance of two discrete random variables \(X\) and \(Y\) is
</p>
<div>
\begin{align*}
\mathrm{Cov}(X, Y) & = \sigma_{XY} = \mathrm{E}(X-\mu_{X})(Y-\mu_{Y}) \\
                   & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i, Y=y_j)
\end{align*}

</div></li>

<li>For continous random variables, the covariance of \(X\) and \(Y\) is
\[ \mathrm{Cov}(X, Y) = \int_{-\infty}^{\infty}
  \int_{-\infty}^{\infty} (x-\mu_X)(y-\mu_y)f(x, y) dx dy \]</li>

<li>The covariance can also be computed as
\[ \mathrm{Cov}(X, Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y) \]</li>

</ul>

</section>
<section id="slide-org4f21857">
<h4 id="org4f21857">Correlation coefficient</h4>
<ul>
<li><p>
The <b>correlation coefficient</b> of \(X\) and \(Y\) is
</p>

<p>
\[ \mathrm{corr}(X, Y) = \rho_{XY} = \frac{\mathrm{Cov}(X, Y)}{\left[\mathrm{Var}(X)\mathrm{Var}(Y)\right]^{1/2}} =
  \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}} \]
</p></li>

<li>\(-1 \leq \mathrm{corr}(X, Y) \leq 1\).</li>

<li>\(\mathrm{corr}(X, Y)=0\) (or \(\mathrm{Cov}(X,Y)=0\)) means that \(X\)
and \(Y\) are uncorrelated.</li>

<li>Since \(\mathrm{Cov}(X, Y) = \mathrm{E}(XY) -
  \mathrm{E}(X)\mathrm{E}(Y)\), when \(X\) and \(Y\) are uncorrelated, then \(\mathrm{E}(XY) =
  \mathrm{E}(X) \mathrm{E}(Y)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgaaaf028">
<h3 id="orgaaaf028">Independence and uncorrelation</h3>
<ul>
<li><p>
If \(X\) and \(Y\) are independent, then
</p>
<div>
\begin{align*}
\mathrm{Cov}(X, Y) & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i) \mathrm{Pr}(Y=y_j) \\
                   & = \sum_{i=1}^n (x_i - \mu_X) \mathrm{Pr}(X=x_i) \sum_{j=1}^m (y_j - \mu_y) \mathrm{Pr}(Y=y_j) \\
                   & = 0 \times 0 = 0
\end{align*}

</div></li>

<li>That is, if \(X\) and \(Y\) are independent, they must be
uncorrelated.</li>

<li>However, the converse is not true. If \(X\) and \(Y\) are
uncorrelated, there is a possibility that they are actually
dependent.</li>

</ul>

</section>
</section>
<section>
<section id="slide-org650b11b">
<h3 id="org650b11b">Conditional mean and correlation</h3>
<ul>
<li>If \(X\) and \(Y\) are independent, then we must have 
\(\mathrm{E}(Y \mid X) = \mathrm{E}(Y) = \mu_Y\)</li>

<li><p>
Then, we can prove that
\(\mathrm{Cov}(X, Y) = 0\) and \(\mathrm{corr}(X, Y)=0\).
</p>

<div>
\begin{align*}
\mathrm{E}(XY) & = \mathrm{E}(\mathrm{E}(XY \mid X)) = \mathrm{E}(X \mathrm{E}(Y \mid X)) \\
               & = \mathrm{E}(X) \mathrm{E}(Y \mid X) = \mathrm{E}(X) \mathrm{E}(Y)
\end{align*}

</div>

<p>
It follows that \(\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X)
   \mathrm{E}(Y) = 0\) and \(\mathrm{corr}(X, Y)=0\). 
</p></li>

</ul>

</section>
</section>
<section>
<section id="slide-orgfc8bcf3">
<h3 id="orgfc8bcf3">Some useful operations</h3>
<p>
The following properties
of \(\mathrm{E}(\cdot)\), \(\mathrm{Var}(\cdot)\) and
\(\mathrm{Cov}(\cdot)\) are useful in calculation,
</p>

<div>
\begin{align*}
\mathrm{E}(a + bX + cY)      & = a + b \mu_{X} + c \mu_{Y} \\
\mathrm{Var}(aX + bY)        & = a^{2} \sigma^{2}_{X} + b^{2} \sigma^{2}_{Y} + 2ab\sigma_{XY} \\
\mathrm{Cov}(a + bX + cV, Y) & = b\sigma_{XY} + c\sigma_{VY} \\
\end{align*}

</div>


</section>
</section>
<section>
<section id="slide-org26a7942">
<h2 id="org26a7942">Four Specific Distributions</h2>
<div class="outline-text-2" id="text-org26a7942">
</div></section>
</section>
<section>
<section id="slide-org8f0e68c">
<h3 id="org8f0e68c">The normal distribution</h3>
<div class="outline-text-3" id="text-org8f0e68c">
</div></section>
<section id="slide-org60e56d3">
<h4 id="org60e56d3">The normal distribution</h4>
<ul>
<li>The p.d.f. of a normally distributed random variable \(X\) is
\[ f(x) =
  \frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right]
  \]</li>
<li>\(\mathrm{E}(X) = \mu\) and \(\mathrm{Var}(X) = \sigma^{2}\).</li>
<li>We write \(X \sim N(\mu, \sigma^{2})\)</li>

</ul>

</section>
<section id="slide-orge3b28e8">
<h4 id="orge3b28e8">The standard normal distribution</h4>
<ul>
<li>The standard normal distribution is a special case of the normal
distribution, for which \(\mu = 0\) and \(\sigma = 0\). The p.d.f of the
standard normal distribution is
\[
  \phi(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)
  \]</li>
<li>The c.d.f of the standard normal distribution is often denoted as
\(\Phi(x)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgce14706">
<h3 id="orgce14706">Symmetric and skinny tails</h3>
<ul>
<li>The normal distribution is symmetric around its mean, \(\mu\), with the
skewness equal 0</li>
<li>It has 95% of its probability between
\(\mu-1.96\sigma\) and \(\mu+1.96\sigma\), with the kurtosis
equal 3.</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgcf4439a">
<h3 id="orgcf4439a">The p.d.f. of the normal distribution</h3>

<div id="org1d3532c" class="figure">
<p><img src="figure/Normal-distribution-curve.png" alt="Normal-distribution-curve.png" />
</p>
<p><span class="figure-number">Figure 6: </span>The normal probability density</p>
</div>

</section>
</section>
<section>
<section id="slide-org83c593a">
<h3 id="org83c593a">Transforming a normally distributed random variable to the standard normal distribution</h3>
<ul>
<li>Let \(X\) be a random variable with a normal distribution, i.e., \(X \sim
  N(\mu, \sigma^2)\).</li>
<li>Standardization of \(X\). We compute \(Z = (X-\mu)/\sigma\), which
follows the standard normal distribution, \(N(0, 1)\).</li>
<li>For example, if \(X \sim N(1, 4)\), then \(Z = (X-1)/2 \sim N(0,
  1)\). When we want to find \(\mathrm{Pr}(X \leq 4)\), we only need to
compute \(\Phi(3/2)\)</li>

</ul>

</section>
<section id="slide-orgb484ee0">
<h4 id="orgb484ee0">The general rule for transforming a normally distributed random variable</h4>
<ul>
<li><p>
Generally, for any two number \(c_1 < c_2\) and let \(d_1 = (c_1 - \mu)/\sigma\) and
\(d_2 = (c_2 - \mu)/\sigma\), we have
</p>
<div>
\begin{align*}
\mathrm{Pr}(X \leq c_2) & = \mathrm{Pr}(Z \leq d_2) = \Phi(d_2) \\
\mathrm{Pr}(X \geq c_1) & = \mathrm{Pr}(Z \geq d_1) = 1 - \Phi(d_1) \\
\mathrm{Pr}(c_1 \leq X \leq c_2) & = \mathrm{Pr}(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1)
\end{align*}

</div></li>

</ul>

</section>
</section>
<section>
<section id="slide-orga7bc0f0">
<h3 id="orga7bc0f0">The multivariate normal distribution</h3>
<ul>
<li>The multivariate normal distribution is the joint
distribution of a set of random variables.</li>

<li>The p.d.f. of the multivariate normal distribution is beyond the
scope of this course, but the following properties make this
distribution handy in analysis.</li>

</ul>

</section>
<section id="slide-orga5ad466">
<h4 id="orga5ad466">Important properties of the multivariate normal distribution</h4>
<ul>
<li>If n random variables, \(x_1, \ldots, x_n\), have a multivariate
normal distribution, then any linear combination of these variables
is normally distributed. For any real numbers, \(\alpha_1, \ldots,
  \alpha_n\), a linear combination of \({x_i}\) is \(\sum_i \alpha_i x_i\).</li>

<li>If a set of random variables has a multivariate normal
distribution, then the marginal distribution of each of the
variables is normal.</li>

<li>If random variables with a multivariate normal distribution have
covariances that equal zero, then these random variables are
independent.</li>

<li>If \(X\) and \(Y\) have a bivariate normal distribution, then
\(\mathrm{E}(Y|X = x) = a + bx\), where \(a\) and \(b\) are constants.</li>

</ul>

</section>
</section>
<section>
<section id="slide-org1ea9e01">
<h3 id="org1ea9e01">The chi-squared distribution</h3>
<ul>
<li>Let \(Z_1, \ldots, Z_n\) be n indepenent standard normal distribution,
i.e. \(Z_i \sim N(0, 1)\) for all \(i = 1, \ldots, n\). Then, the random
variable
\[W = \sum_{i=1}^n Z^2_i \]
has a chi-squared distribution with \(n\) degrees of freedom, denoted as
\(W \sim \chi^2(n)\), with \(\mathrm{E}(W) = n\) and \(\mathrm{Var}(W) = 2n\)</li>

<li>If \(Z \sim N(0, 1)\), then \(W = Z^2 \sim \chi^2(1)\) with \(\mathrm{E}(W) =
  1\) and \(\mathrm{Var}(W) = 2\).</li>

</ul>

</section>
<section id="slide-orgdc805c8">
<h4 id="orgdc805c8">The p.d.f. of chi-squared distributions</h4>

<div id="org4677f40" class="figure">
<p><img src="figure/chi_squared_pdf.png" alt="chi_squared_pdf.png" width="700" />
</p>
<p><span class="figure-number">Figure 7: </span>The probability density function of chi-squared distributions</p>
</div>

</section>
</section>
<section>
<section id="slide-org1b909da">
<h3 id="org1b909da">The student t distribution</h3>
<ul>
<li>Let \(Z \sim N(0, 1)\), \(W \sim \chi^2(m)\), and \(Z\) and \(W\) be
independently distributed. Then, the random variable
\[t = \frac{Z}{\sqrt{W/m}} \]
has a student t distribution with \(m\) degrees of freedom, denoted as
\(t \sim t(m)\).</li>

<li>As \(n\) increases, \(t\) gets close to a standard normal distribution.</li>

</ul>

</section>
<section id="slide-org1f95c88">
<h4 id="org1f95c88">The p.d.f. of student t distributions</h4>

<div id="org2225016" class="figure">
<p><img src="figure/students_t_pdf.png" alt="students_t_pdf.png" width="700" />
</p>
<p><span class="figure-number">Figure 8: </span>The probability density function of student t distributions</p>
</div>

</section>
</section>
<section>
<section id="slide-org68ad18d">
<h3 id="org68ad18d">The F distribution</h3>
<ul>
<li>Let \(W_1 \sim \chi^2(n_1)\), \(W_2 \sim \chi^2(n_2)\), and \(W_1\) and
\(W_2\) are independent. Then, the random variable
\[ F = \frac{W_1/n_1}{W_2/n_2}\]
has an F distribution with \((n_1, n_2)\) degrees of freedom, denoted as
\(F \sim F(n_1, n_2)\)</li>

<li>If \(t \sim t(n)\), then \(t^2 \sim F(1, n)\)</li>

<li>As \(n_2 \rightarrow \infty\), the \(F(n_1, \infty)\) distribution is the
same as the \(\chi^2(n_1)\) distribution divided by \(n_1\).</li>

</ul>

</section>
<section id="slide-org427ce19">
<h4 id="org427ce19">The p.d.f. of F distributions</h4>

<div id="orga414cf1" class="figure">
<p><img src="figure/fisher_f_pdf.png" alt="fisher_f_pdf.png" width="700" />
</p>
<p><span class="figure-number">Figure 9: </span>The probability density function of F distributions</p>
</div>


</section>
</section>
<section>
<section id="slide-orgb349d41">
<h2 id="orgb349d41">Random Sampling and the Distribution of the Sample Average</h2>
<div class="outline-text-2" id="text-orgb349d41">
</div></section>
</section>
<section>
<section id="slide-orge61a756">
<h3 id="orge61a756">Random sampling</h3>
<div class="outline-text-3" id="text-orge61a756">
</div></section>
<section id="slide-org64296bc">
<h4 id="org64296bc">Simple random sampling</h4>
<ul>
<li>A <b>population</b> is a set of similar items or events which
is of interest for some question or experiment.</li>

<li><b>Simple random sampling</b> is a procedure in which \(n\) objects are
selected at random from a population, and each member of the
population is equally likely to be included in the sample.</li>

<li>Let \(Y_1, Y_2, \ldots Y_n\) be the first \(n\) observations in a random
sample. Since they are randomly drawn from a population, \(Y_1, \ldots,
  Y_n\) are random variables.</li>

</ul>

</section>
<section id="slide-org26b0464">
<h4 id="org26b0464">i.i.d draws</h4>
<ul>
<li>Since \(Y_1, Y_2, \ldots, Y_n\) are drawn from the same population,
the marginal distribution of \(Y_i\) is the same for each \(i=1,
  \ldots, n\), which are said to be <b>identically distributed</b>.</li>

<li>With simple random sampling, the value of \(Y_i\) does not depend on
that of \(Y_j\) for \(i \neq j\), which are said to <b>independent
distributed</b>.</li>

<li>Therefore, when \(Y_1, \ldots, Y_n\) are drawn with simple random
sampling from the same distribution of \(Y\), we say that they are
<b>independently and identically distributed</b> or <b>i.i.d</b>, which is
denoted as 
\[ Y_i \sim IID(\mu_Y, \sigma^2_Y) \text{ for } i = 1, 2, \ldots, n\]
given that the population expectation is \(\mu_Y\) and the variance
is \(\sigma^2_Y\).</li>

</ul>


</section>
</section>
<section>
<section id="slide-orga8b033e">
<h3 id="orga8b033e">The sampling distribution of the sample average</h3>
<div class="outline-text-3" id="text-orga8b033e">
</div></section>
<section id="slide-orgf6e8b8b">
<h4 id="orgf6e8b8b">The sample average</h4>
<ul>
<li>The <b>sample average</b> or <b>sample mean</b>, \(\overline{Y}\), of the \(n\)
observations \(Y_1, Y_2, \ldots, Y_n\) is
\[ \overline{Y} = \frac{1}{n}\sum^n_{i=1} Y_i \]</li>

<li>When \(Y_1, \ldots, Y_n\) are randomly drawn, \(\overline{Y}\) is also a
random variable that should have its own distribution, called the
<b>sampling distribution</b>.</li>

</ul>

</section>
<section id="slide-org324696a">
<h4 id="org324696a">The mean and variance of \(\overline{Y}\)</h4>
<ul>
<li>Suppose that \(Y_i \sim IID(\mu_Y, \sigma^2_{Y})\) for all \(i = 1,
  \ldots, n\). Then
\[
  \mathrm{E}(\overline{Y}) = \mu_{\overline{Y}} =
  \frac{1}{n}\sum^n_{i=1}\mathrm{E}(Y_i) = \frac{1}{n} n \mu_Y = \mu_Y
  \]
and
\[
  \mathrm{Var}(\overline{Y}) = \sigma^2_{\overline{Y}} =  \frac{1}{n^2}\sum^n_{i=1}\mathrm{Var}(Y_i) +
  \frac{1}{n^2}\sum^n_{i=1}\sum^n_{j=1}\mathrm{Cov}(Y_i, Y_j) =
  \frac{\sigma^2_Y}{n}
  \]</li>
<li>The standard deviation of the sample mean is
\(\sigma_{\overline{Y}} = \sigma_Y / \sqrt{n}\).</li>

</ul>

</section>
<section id="slide-org5200e79">
<h4 id="org5200e79">Sampling distribution of \(\overline{Y}\) when \(Y\) is normally distributed</h4>
<ul>
<li>When \(Y_1, \ldots, Y_n\) are i.i.d. draws from \(N(\mu_Y,
  \sigma^2_Y)\), from the properties of the multivariate normal
distribution, \(\overline{Y}\) is normally distributed. That is 
\[ \overline{Y} \sim N(\mu_Y, \sigma^2_Y/n) \]</li>

</ul>


</section>
</section>
<section>
<section id="slide-org5c00560">
<h2 id="org5c00560">Large Sample Approximations to Sampling Distributions</h2>
<div class="outline-text-2" id="text-org5c00560">
</div></section>
</section>
<section>
<section id="slide-orgaa04283">
<h3 id="orgaa04283">The exact distribution and the asymptotic distribution</h3>
<ul>
<li>The sampling distribution that exactly describes the distribution of
\(\overline{Y}\) for any \(n\) is called the <b>exact distribution</b> or
<b>finite-sample distribution</b>.</li>

<li>However, in most cases, we cannot obtain an exact distribution of
\(\overline{Y}\), for which we can only get an approximation.</li>

<li>The large-sample approximation to the sampling distribution is called the
<b>asymptotic distribution</b>.</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgd9c000e">
<h3 id="orgd9c000e">The law of large numbers</h3>
<div class="outline-text-3" id="text-orgd9c000e">
</div></section>
<section id="slide-orgc3383c9">
<h4 id="orgc3383c9">Convergence in probability</h4>
<ul>
<li>Let \(S_1, \ldots, S_n\) be a sequence of random variables,
denoted as \(\{S_n\}\). \(\{S_n\}\) is said to converge in probability to a
limit &mu; (denoted as \(S_n \xrightarrow{\text{p}} \mu\)), if and only if
\[ \mathrm{Pr} \left(|S_n-\mu| < \delta \right) \rightarrow 1 \]
as \(n \rightarrow \infty\) for every \(\delta > 0\).</li>

<li>For example, \(S_n = \overline{Y}\). That is, \(S_1=Y_1\), \(S_2=1/2(Y_1+Y_2)\),
\(S_n=1/n\sum_i Y_i\), and so forth.</li>

</ul>

</section>
<section id="slide-orgbc59764">
<h4 id="orgbc59764">The law of large numbers</h4>
<ul>
<li>The law of large numbers (LLN) states that if \(Y_1, \ldots, Y_n\) are i.i.d. with
\(\mathrm{E}(Y_i)=\mu_Y\) and \(\mathrm{Var}(Y_i) < \infty\), then
\(\overline{Y} \xrightarrow{\text{p}} \mu_Y\).</li>

<li>The conditions for the LLN to be held is \(Y_i\) for \(i=1, \ldots, n\)
are i.i.d., and the variance of \(Y_i\) is finite. The latter says that
there is no extremely large outliers in the random samples.</li>

</ul>

</section>
<section id="slide-orgf6ab86c">
<h4 id="orgf6ab86c">The LLN illustrated</h4>

<div id="org0adf86d" class="figure">
<p><img src="figure/fig-2-8.png" alt="fig-2-8.png" width="600" />
</p>
<p><span class="figure-number">Figure 10: </span>An illustration of the law of large numbers</p>
</div>

</section>
</section>
<section>
<section id="slide-org34f4a81">
<h3 id="org34f4a81">The central limit theorem</h3>
<div class="outline-text-3" id="text-org34f4a81">
</div></section>
<section id="slide-org59e21b4">
<h4 id="org59e21b4">Convergence in distribution</h4>
<ul>
<li><p>
Let \(F_1, F_2, \ldots, F_n\) be a sequence of cumulative distribution
functions corresponding to a sequence of random variables, \(S_1, S_2,
  \ldots, S_n\). Then the sequence of random variables \({S_n}\) is said to
<b>converge in distribution</b> to a random variable \(S\) (denoted as \(S_n
  \xrightarrow{\text{d}} S\)), if the distribution functions \(\{F_n\}\)
converge to \(F\) that is the distribution function of \(S\). We can write
it as
</p>

<p>
\[ S_n \xrightarrow{\text{d}} S \text{ if and only if } \lim_{n
  \rightarrow \infty}F_n(x)=F(x) \]
</p></li>

<li>The distribution \(F\) is called the <b>asymptotic distribution</b> of \(S_n\).</li>

</ul>

</section>
<section id="slide-orgb8729c3">
<h4 id="orgb8729c3">The central limit theorem (Lindeberg-Levy CLT)</h4>
<ul>
<li><p>
The CLT states that if \(Y_1, Y_2, \ldots, Y_n\) are i.i.d. random samples from a
probability distribution with finite mean \(\mu_Y\) and finite variance
\(\sigma^2_Y\), i.e., \(0 < \sigma^2_Y < \infty\) and \(\overline{Y} =
  (1/n)\sum_i^nY_i\). Then
</p>

<p>
\[ \sqrt{n}(\overline{Y}-\mu_Y) \xrightarrow{\text{d}} N(0,
  \sigma^2_Y) \]
</p></li>

<li><p>
It follows that since \(\sigma_{\overline{Y}} =
  \sqrt{\mathrm{Var}(\overline{Y})} = \sigma_Y/\sqrt{n}\),
</p>

<p>
\[ \frac{\overline{Y} - \mu_Y}{\sigma_{\overline{Y}}}
  \xrightarrow{\text{ d}} N(0, 1) \]
</p></li>

</ul>

</section>
<section id="slide-orgc7b7caf">
<h4 id="orgc7b7caf">The CLT illustrated</h4>

<div id="org27df11c" class="figure">
<p><img src="figure/fig-2-9.png" alt="fig-2-9.png" width="600" />
</p>
<p><span class="figure-number">Figure 11: </span>An illustration of the central limit theorem</p>
</div>

</section>
</section>
<section>
<section id="slide-org9c58a45">
<h3 id="org9c58a45">Illustrations with Wolfram CDF player</h3>
<ul>
<li>To view the following demonstrations,
first you need to download them by saving into your disk, then open
them with Wolfram CDF Player that can be downloaded from
<a href="http://www.wolfram.com/cdf-player/">http://www.wolfram.com/cdf-player/</a>.</li>

<li>Here is another demonstration of the law of large number,
<a href="IllustratingTheLawOfLargeNumbers.cdf">IllustratingTheLawOfLargeNumbers.cdf</a>.</li>

<li>Here is the demonstration of the CLT with Wolfram CDF Player,
<a href="IllustratingTheCentralLimitTheoremWithSumsOfBernoulliRandomV.cdf">IllustratingTheCentralLimitTheoremWithSumsOfBernoulliRandomV.cdf</a>.</li>

</ul>
</section>
</section>
</div>
</div>
<script src="../../../reveal.js/lib/js/head.min.js"></script>
<script src="../../../reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: true,
keyboard: true,
overview: true,
width: 1000,
height: 800,
margin: 0.20,
minScale: 0.50,
maxScale: 2.50,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'cube', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
{ src: '../../../reveal.js/plugin/menu/menu.js' },
 { src: '../../../reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: '../../../reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: '../../../reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
