<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 2: Review of Probability</title>
<meta name="author" content="(Zheng Tian)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="../../../reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="../../../reveal.js/css/theme/beige.css" id="theme"/>

<link rel="stylesheet" href="../../../reveal.js/lib/css/zenburn.css"/>
<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = '../../../reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1 class="title">Lecture 2: Review of Probability</h1><h2 class="author">Zheng Tian</h2><p class="date">Created: 2017-02-22 Wed 08:42</p>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-org8df5890">Random Variables and Probability Distributions</a></li>
<li><a href="#/slide-org850e40b">Expectation, Variance, and Other Moments</a></li>
<li><a href="#/slide-orgbcf4a5a">Two Random Variables</a></li>
<li><a href="#/slide-orgab52495">Four Specific Distributions</a></li>
</ul>
</div>
</div>
</section>


<section>
<section id="slide-org8df5890">
<h2 id="org8df5890">Random Variables and Probability Distributions</h2>
<div class="outline-text-2" id="text-org8df5890">
</div></section>
</section>
<section>
<section id="slide-org59b6215">
<h3 id="org59b6215">Defining probabilities and random variables</h3>
<div class="outline-text-3" id="text-org59b6215">
</div></section>
<section id="slide-org891c8ed">
<h4 id="org891c8ed">Experiments and outcomes</h4>
<ul>
<li>An <b>experiment</b> is the processes that generate random results</li>
<li>The <b>outcomes</b> of an experiment are its
mutually exclusive potential results.</li>
<li>Example: tossing a coin. The outcome is either getting a head(H) or a tail(T)
but not both.</li>

</ul>

</section>
<section id="slide-orgc6ce93e">
<h4 id="orgc6ce93e">Sample space and events</h4>
<ul>
<li>A <b>sample space</b> consists of all the outcomes from an experiment,
denoted with the set \(S\).
<ul>
<li>\(S = \{H, T\}\) in the tossing-coin experiment.</li>

</ul></li>

<li>An <b>event</b> is a subset of the sample 
space. 
<ul>
<li>Getting a head is an event, which is \(\{H\} \subset \{H, T\}\).</li>

</ul></li>

</ul>

</section>
</section>
<section>
<section id="slide-org2894c40">
<h3 id="org2894c40">Probability</h3>
<div class="outline-text-3" id="text-org2894c40">
</div></section>
<section id="slide-org12a92f1">
<h4 id="org12a92f1">An intuitive definition of probability</h4>
<ul>
<li>The <b>probability</b> of an event is the proportion of the time that the
event will occur in the long run.</li>

<li>For example, we toss a coin for \(n\)
times and get \(m\) heads. When \(n\) is very large, we can say that the
probability of getting a head in a toss is \(m/n\).</li>

</ul>

</section>
<section id="slide-orgd0f0ab4">
<h4 id="orgd0f0ab4">An axiomatic definition of probability</h4>
<ul>
<li>A probability of an event \(A\) in the sample space \(S\), denoted as
\(\mathrm{Pr}(A)\), is a function that assign \(A\) a real number in \([0,
  1]\), satisfying the following three conditions:
<ol>
<li>\(0 \leq \mathrm{Pr}(A) \leq 1\).</li>
<li>\(\mathrm{Pr}(S) = 1\).</li>
<li>For any disjoint sets, \(A\) and \(B\), that is \(A\) and \(B\) have no
element in common, \(\mathrm{Pr}(A \cup B) = \mathrm{Pr}(A) +
    \mathrm{Pr}(B)\).</li>

</ol></li>

</ul>

</section>
</section>
<section>
<section id="slide-org7081740">
<h3 id="org7081740">Random variables</h3>
<div class="outline-text-3" id="text-org7081740">
</div></section>
<section id="slide-org3a5f806">
<h4 id="org3a5f806">The definition of random variables</h4>
<ul>
<li>A <b>random variable</b> is a numerical summary associated with the
outcomes of an experiment.</li>

<li>You can also think of a random variable as a function
mapping from an event \(\omega\) in the sample space \(\Omega\) to the
real line.</li>

</ul>

</section>
<section id="slide-org745e41e">
<h4 id="org745e41e">An illustration of random variables</h4>

<div id="org31f84d1" class="figure">
<p><img src="figure/random_variable_demo1.png" alt="random_variable_demo1.png" width="600" />
</p>
<p><span class="figure-number">Figure 1: </span>An illustration of random variable</p>
</div>

</section>
<section id="slide-org8c5b480">
<h4 id="org8c5b480">Discrete and continuous random variables</h4>
<p>
Random variables can take different types of values
</p>

<ul>
<li>A <b>discrete</b> random
variables takes on a discrete set of values, like \(0, 1, 2, \ldots, n\)</li>
<li>A <b>continuous</b> random variable takes on a continuum of possble
values, like any value in the interval \((a, b)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org313aa21">
<h3 id="org313aa21">Probability distributions</h3>
<div class="outline-text-3" id="text-org313aa21">
</div></section>
<section id="slide-org2bc6498">
<h4 id="org2bc6498">The probability distribution for a discrete random variable</h4>
<ul>
<li>The probability distribution of a discrete random variable is the list
of all possible values of the variable and the probability that each
value will occur. These probabilities sum to 1.</li>

<li><p>
The probability mass function. Let \(X\) be a discrete random
variable. The probability distribution of \(X\) (or the probability
mass function), \(p(x)\), is
</p>
<div>
\begin{equation*}
p(x) = \mathrm{Pr}(X = x)
\end{equation*}

</div></li>

<li>The axioms of probability require that 
<ol>
<li>\(0 \leq p(x) \leq 1\)</li>
<li>\( \sum_{i=1}^n p(x_i) = 1\).</li>

</ol></li>

</ul>

</section>
<section id="slide-org944cf3b">
<h4 id="org944cf3b">An example of the probability distribution of a discrete random variable</h4>
<table id="orgd43bf76" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> An illustration of the probability distribution of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-right">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.75</td>
<td class="org-right">0.25</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

</section>
</section>
<section>
<section id="slide-org1e23631">
<h3 id="org1e23631">The cumulative probability distribution</h3>
<ul>
<li><p>
The <b>cumulative probability distribution</b> (or the cumulative
distribution function, c.d.f.): 
</p>

<p>
Let \(F(x)\) be the c.d.f of \(X\). Then \(F(x) = \mathrm{Pr}(X \leq x)\).
</p></li>

</ul>

</section>
</section>
<section>
<section id="slide-org0de0c50">
<h3 id="org0de0c50">The c.d.f. of a discrete random variable is a step function</h3>
<table id="org162338d" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> An illustration of the c.d.f. of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-left">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.50</td>
<td class="org-right">0.25</td>
<td class="org-left">1</td>
</tr>

<tr>
<td class="org-left">C.d.f.</td>
<td class="org-right">0.25</td>
<td class="org-right">0.75</td>
<td class="org-right">1</td>
<td class="org-left">--</td>
</tr>
</tbody>
</table>


<div id="org7361a8e" class="figure">
<p><img src="figure/cdf_discrete_example.png" alt="cdf_discrete_example.png" width="450" height="300" />
</p>
<p><span class="figure-number">Figure 2: </span>The c.d.f. of a discrete random variable</p>
</div>

</section>
</section>
<section>
<section id="slide-org0f62625">
<h3 id="org0f62625">Bernouli distribution</h3>
<p>
The Bernoulli distribution
</p>
<div>
\begin{equation*}
  G =
    \begin{cases}
      1 & \text{with probability } p \\
      0 & \text{with probability } 1-p
    \end{cases}
  \end{equation*}

</div>

</section>
</section>
<section>
<section id="slide-orgf61d8fd">
<h3 id="orgf61d8fd">The probability distribution of a continuous random variable</h3>
<div class="outline-text-3" id="text-orgf61d8fd">
</div></section>
<section id="slide-org04a60ff">
<h4 id="org04a60ff">Definition of the c.d.f. and the p.d.f. of a continuous random variable</h4>
<ul>
<li>The cumulative distribution function of a continous random variable
is defined as it is for a discrete random variable. 
\[ F(x) = \mathrm{Pr}(X \leq x) \]</li>

<li>The <b>probability density function (p.d.f.)</b> of \(X\) is the function
that satisfies
\[ F(x) = \int_{-\infty}^{x} f(t) \mathrm{d}t \text{ for all } x \]</li>

</ul>

</section>
<section id="slide-orgf2e4a95">
<h4 id="orgf2e4a95">Properties of the c.d.f.</h4>
<ul>
<li>For both discrete and continuous random variable, \(F(X)\) must satisfy
the following properties:
<ol>
<li>\(F(+\infty) = 1 \text{ and } F(-\infty) = 0\) (\(F(x)\) is bounded between 0 and 1)</li>
<li>\(x > y \Rightarrow F(x) \geq F(y)\) (\(F(x)\) is nondecreasing)</li>

</ol></li>

<li>By the definition of the c.d.f., we can conveniently calculate
probabilities, such as,
<ul>
<li>\(\mathrm{P}(x > a) = 1 - \mathrm{P}(x \leq a) = 1 - F(a)\)</li>
<li>\(\mathrm{P}(a < x \leq b) = F(b) - F(a)\).</li>

</ul></li>

</ul>

</section>
<section id="slide-org1b824aa">
<h4 id="org1b824aa">The c.d.f. and p.d.f. of a normal distribution</h4>

<div id="orgc35dfc1" class="figure">
<p><img src="figure/norm1.png" alt="norm1.png" width="500" height="450" />
</p>
<p><span class="figure-number">Figure 3: </span>The p.d.f. and c.d.f. of a continuous random variable (the normal distribution)</p>
</div>


</section>
</section>
<section>
<section id="slide-org850e40b">
<h2 id="org850e40b">Expectation, Variance, and Other Moments</h2>
<div class="outline-text-2" id="text-org850e40b">
</div></section>
</section>
<section>
<section id="slide-org06c9c15">
<h3 id="org06c9c15">The expected value of a random variable</h3>
<div class="outline-text-3" id="text-org06c9c15">
</div></section>
<section id="slide-org2bac3d8">
<h4 id="org2bac3d8">The expected value</h4>
<ul>
<li>The <b>expected value</b> of a random variable, X, denoted as \(\mathrm{E}(X)\), is
the long-run average of the random variable over many repeated
trials or occurrences, which is also called the <b>expectation</b> or the
<b>mean</b>.</li>

<li>The expected value measures the centrality of a random variable.</li>

</ul>

</section>
<section id="slide-org94255f0">
<h4 id="org94255f0">Mathematical definition</h4>
<ul>
<li>For a discrete random variable
\[ \mathrm{E}(X) = \sum_{i=1}^n x_i \mathrm{Pr}(X = x_i) \]</li>

<li>e.g. The expectation of a Bernoulli random variable, \(G\),
\[ \mathrm{E}(G) = 1 \cdot p + 0 \cdot (1-p) = p \]</li>

<li>For a continuous random variable
\[ \mathrm{E}(X) = \int_{-\infty}^{\infty} x f(x) \mathrm{d}x\]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org594ab31">
<h3 id="org594ab31">The variance and standard deviation</h3>
<div class="outline-text-3" id="text-org594ab31">
</div></section>
<section id="slide-orgd86f2dc">
<h4 id="orgd86f2dc">Definition of variance and standard deviation</h4>
<ul>
<li>The <b>variance</b> of a random variable \(X\) measures its average
deviation from its own expected value.</li>

<li><p>
Let \(\mathrm{E}(X) = \mu_X\). Then the variance of \(X\),
</p>

<div>
\begin{align*}
\mathrm{Var}(X) & =  \sigma^2_X =  \mathrm{E}(X-\mu_X)^{2} \\
& = 
\begin{cases}
\sum_{i=1}^n (x - \mu_X)^{2}\mathrm{Pr}(X = x_i) & \text{if } X \text{ is discrete} \\
\int_{-\infty}^{\infty} (x - \mu_X)^{2}f(x)\mathrm{d} x  & \text{if } X \text{ is continuous}
\end{cases}
\end{align*}

</div></li>

<li>The <b>standard deviation</b> of \(X\): &sigma;<sub>X</sub> = \sqrt{\mathrm{Var}(X)}$</li>

</ul>

</section>
<section id="slide-org0b30f05">
<h4 id="org0b30f05">Computing variance</h4>
<ul>
<li>A convenient formula for calculating the variance is
\[ \mathrm{Var}(X) = \mathrm{E}(X - \mu_X)^{2} = \mathrm{E}(X^{2}) - \mu_X^{2} \]</li>

<li>The variance of a Bernoulli random variable, \(G\)
\[ \mathrm{Var}(G) = (1-p)^{2}p + (0-p)^{2}(1-p) = p(1-p) \]</li>

<li>The expectation and variance of a linear function of \(X\). Let \(Y = a +
  bX\), then
<ul>
<li>\(\mathrm{E}(Y) = a + \mathrm{E}(X)\)</li>
<li>\(\mathrm{Var}(Y) = \mathrm{Var}(a + b X) = b^{2} \mathrm{Var}(X)\).</li>

</ul></li>

</ul>

</section>
</section>
<section>
<section id="slide-org5a8fd8e">
<h3 id="org5a8fd8e">Moments of a random variable, skewness and kurtosis</h3>
<div class="outline-text-3" id="text-org5a8fd8e">
</div></section>
<section id="slide-org5a9c21f">
<h4 id="org5a9c21f">Definition of the moments of a distribution</h4>
<dl>
<dt>k<sup>th</sup> moment</dt><dd>The k<sup>th</sup> <b>moment</b> of the distribution of \(X\) is
\(\mathrm{E}(X^{k})\). So, the expectation is the "first"
moment of \(X\).</dd>

<dt>k<sup>th</sup> central moment</dt><dd>The k<sup>th</sup> central moment of the distribution
of \(X\) with its mean \(\mu_X\) is \(\mathrm{E}(X - \mu_X)^{k}\). So, the
variance is the second central moment of \(X\).</dd>

</dl>

<ul class="org-ul"><li><a id="org1b55fa5"></a>A caveat<br  /><p>
It is important to remember that not all the moments of a distribution
exist. 
</p></li></ul>

</section>
<section id="slide-org44be25b">
<h4 id="org44be25b">Skewness</h4>
<ul>
<li><p>
The skewness of a distribution provides a mathematical way to describe
how much a distribution deviates from symmetry.
</p>

<p>
\[ \text{Skewness} =  \mathrm{E}(X - \mu_X)^{3}/\sigma_{X}^{3} \]
</p></li>

<li>A symmetric distribution has a skewness of zero.</li>
<li>The skewness can be either positive or negative.</li>
<li>That \(\mathrm{E}(X - \mu_X)^3\) is divided by \(\sigma^3_X\) is to make
the skewness measure unit free.</li>

</ul>

</section>
<section id="slide-org5187cf1">
<h4 id="org5187cf1">Kurtosis</h4>
<ul>
<li><p>
The kurtosis of the distribution of a random variable \(X\) measures how
much of the variance of \(X\) arises from extreme values, which makes
the distribution have "heavy" tails.
</p>

<p>
\[ \text{Kurtosis} = \mathrm{E}(X - \mu_X)^{4}/\sigma_{X}^{4} \]
</p></li>

<li>The kurtosis must be positive.</li>
<li>The kurtosis of the normal distribution is 3. So a distribution that
has its kurtosis exceeding 3 is called heavy-tailed.</li>
<li>The kurtosis is also unit free.</li>

</ul>

</section>
<section id="slide-org286c215">
<h4 id="org286c215">An illustration of skewness and kurtosis</h4>

<div class="figure">
<p><img src="figure/fig-2-3.png" alt="fig-2-3.png" width="550" height="450" />
</p>
</div>

<ul>
<li>All four distributions have a mean of zero and
a variance of one, while (a) and (b) are symmetric and (b)-(d) are
heavy-tailed.</li>

</ul>


</section>
</section>
<section>
<section id="slide-orgbcf4a5a">
<h2 id="orgbcf4a5a">Two Random Variables</h2>
<div class="outline-text-2" id="text-orgbcf4a5a">
</div></section>
</section>
<section>
<section id="slide-orga6a07f9">
<h3 id="orga6a07f9">The joint and marginal distributions</h3>
<div class="outline-text-3" id="text-orga6a07f9">
</div></section>
<section id="slide-org7297fb8">
<h4 id="org7297fb8">The joint probability function of two discrete random variables</h4>
<ul>
<li>The joint distribution of two random variables \(X\) and \(Y\) is
\[ p(x, y) = \mathrm{Pr}(X = x, Y = y)\]</li>

<li>\(p(x, y)\) must satisfy
<ol>
<li>\(p(x, y) \geq 0\)</li>
<li>\(\sum_{i=1}^n\sum_{j=1}^m p(x_i, y_j) = 1\) for all possible
combinations of values of \(X\) and \(Y\).</li>

</ol></li>

</ul>

</section>
<section id="slide-org3067836">
<h4 id="org3067836">The joint probability function of two continuous random variables</h4>
<ul>
<li>For two continuous random variables, \(X\) and \(Y\), the counterpart of \(p(x, y)\) is
the joint probability density function, \(f(x, y)\), such that
<ol>
<li>\(f(x, y) \geq 0\)</li>
<li>\(\int_{-\infty}^{{\infty}} \int_{-\infty}^{\infty} f(x, y)\, dx\, dy= 1\)</li>

</ol></li>

</ul>

</section>
<section id="slide-org026ebd3">
<h4 id="org026ebd3">The marginal probability distribution</h4>
<ul>
<li>The marginal probability distribution of a random variable \(X\) is
simply the probability distribution of its own.</li>

<li>For a discrete random variable, we can compute the marginal
distribution of \(X\) as
\[ \mathrm{Pr}(X=x) = \sum_{i=1}^n \mathrm{Pr}(X, Y=y_i) = \sum_{i=1}^n p(x, y_i)  \]</li>

<li>For a continuous random variable, the marginal distribution is
\[f_X(x) = \int_{-\infty}^{\infty} f(x, y)\, dy \]</li>

</ul>

</section>
<section id="slide-orgd3c5fea">
<h4 id="orgd3c5fea">An example of joint and marginal distributions</h4>
<table id="org00e2c50" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> Joint and marginal distributions of raining and commuting time</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Rain (\(X=0\))</th>
<th scope="col" class="org-right">No rain (\(X=1\))</th>
<th scope="col" class="org-right">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Long commute (\(Y=0\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.07</td>
<td class="org-right">0.22</td>
</tr>

<tr>
<td class="org-left">Short commute (\(Y=1\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.63</td>
<td class="org-right">0.78</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Total</td>
<td class="org-right">0.30</td>
<td class="org-right">0.70</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

</section>
</section>
<section>
<section id="slide-org517925f">
<h3 id="org517925f">Conditional distributions</h3>
<div class="outline-text-3" id="text-org517925f">
</div></section>
<section id="slide-org5549699">
<h4 id="org5549699">The conditional probability</h4>
<ul>
<li><p>
For any two events \(A\) and \(B\), the conditional probability of A given
B is defined as
</p>
<div>
\begin{equation*}
\mathrm{Pr}(A|B) = \frac{\mathrm{Pr}(A \cap B)}{\mathrm{Pr}(B)}
\end{equation*}

</div></li>

</ul>

</section>
<section id="slide-orge7e3d1f">
<h4 id="orge7e3d1f">Conditional probability illustrated</h4>

<div id="org447dff1" class="figure">
<p><img src="figure/conditional_probability.png" alt="conditional_probability.png" width="400" height="400" />
</p>
<p><span class="figure-number">Figure 5: </span>An illustration of conditional probability</p>
</div>

</section>
</section>
<section>
<section id="slide-org21ee3bd">
<h3 id="org21ee3bd">The conditional probability distribution</h3>
<ul>
<li>The conditional distribution of a random variable \(Y\) given another
random variable \(X\) is \(\mathrm{Pr}(Y | X=x)\).</li>

<li>The formula to compute it is
\[ \mathrm{Pr}(Y | X=x) = \frac{\mathrm{Pr}(X=x, Y)}{\mathrm{Pr}(X=x)} \]</li>

<li>For continuous random variables \(X\) and \(Y\), we define the conditional
density function as
\[ f(y|x) = \frac{f(x, y)}{f_X(x)} \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org09b9f23">
<h3 id="org09b9f23">The conditional expectation</h3>
<ul>
<li>The <b>conditional expectation</b> of \(Y\) given \(X\) is the expected value
of the conditional distribution of \(Y\) given \(X\).</li>

<li><p>
For discrete random variables, the conditional mean of \(Y\) given \(X=x\) is
</p>
<div>
\begin{equation*}
\mathrm{E}(Y \mid X=x) = \sum_{i=1}^n y_i \mathrm{Pr}(Y \mid X=x)
\end{equation*}

</div></li>

<li><p>
For continuous random variables, it is computed as
</p>
<div>
\begin{equation*}
\int_{-\infty}^{\infty} y f(y \mid x)\, dy
\end{equation*}

</div></li>

<li>The expected mean of commuting time given it is raining is \(0 \times
  0.1 + 1 \times 0.9 = 0.9\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgebb59d4">
<h3 id="orgebb59d4">The law of iterated expectation</h3>
<ul>
<li><p>
<b>The law of iterated expectation</b>:
</p>

<p>
\[ \mathrm{E}(Y) = E \left[ \mathrm{E}(Y|X) \right] \]
</p></li>

<li>It says that the mean of \(Y\) is the weighted average of the
conditional expectation of \(Y\) given \(X\), weighted by the
probability distribution of \(X\). That is,
\[ \mathrm{E}(Y) = \sum_{i=1}^n \mathrm{E}(Y \mid X=x_i) \mathrm{Pr}(X=x_i) \]</li>

<li>If \(\mathrm{E}(X|Y) = 0\), then \(\mathrm{E}(X)=E\left[\mathrm{E}(X|Y)\right]=0\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org127663b">
<h3 id="org127663b">Conditional variance</h3>
<ul>
<li>With the conditional mean of \(Y\) given \(X\), we can compute the
conditional variance as
\[ \mathrm{Var}(Y \mid X=x) = \sum_{i=1}^n \left[ y_i - \mathrm{E}(Y \mid X=x)
  \right]^2 \mathrm{Pr}(Y=y_i \mid X=x) \]</li>

<li>From the law of iterated expectation, we can get the following
\[ \mathrm{Var}(Y) = \mathrm{E}(\mathrm{Var}(Y \mid X)) + \mathrm{Var}(\mathrm{E}(Y \mid
  X)) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org0bef10e">
<h3 id="org0bef10e">Independent random variables</h3>
<ul>
<li>Two random variables \(X\) and \(Y\) are <b>independently distributed</b>, or
<b>independent</b>, if knowing the value of one of the variable provides no
information about the other.</li>
<li>Mathematically, it means that 
\[ \mathrm{Pr}(Y=y \mid X=x) = \mathrm{Pr}(Y=y)  \]</li>

<li>If \(X\) and \(Y\) are independent
\[ \mathrm{Pr}(Y=y, X=x) = \mathrm{Pr}(X=x) \mathrm{Pr}(Y=y) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org1201032">
<h3 id="org1201032">Independence between two continuous random variable</h3>
<ul>
<li>For two continuous random variables, \(X\) and \(Y\), they are
<b>independent</b> if
\[ f(x|y) = f_{X}(x) \text{ or } f(y|x) = f_{Y}(y) \]</li>

<li>It follows that if \(X\) and \(Y\) are independent
\[ f(x, y) = f(x|y)f_{Y}(y) = f_{X}(x)f_{Y}(y) \]</li>

</ul>

</section>
</section>
<section>
<section id="slide-org5976390">
<h3 id="org5976390">Covariance and Correlation</h3>
<div class="outline-text-3" id="text-org5976390">
</div></section>
<section id="slide-orgb3f764d">
<h4 id="orgb3f764d">Covariance</h4>
<ul>
<li><p>
The covariance of two discrete random variables \(X\) and \(Y\) is
</p>
<div>
\begin{align*}
\mathrm{Cov}(X, Y) & = \sigma_{XY} = \mathrm{E}(X-\mu_{X})(Y-\mu_{Y}) \\
                   & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i, Y=y_j)
\end{align*}

</div></li>

<li>For continous random variables, the covariance of \(X\) and \(Y\) is
\[ \mathrm{Cov}(X, Y) = \int_{-\infty}^{\infty}
  \int_{-\infty}^{\infty} (x-\mu_X)(y-\mu_y)f(x, y) dx dy \]</li>

<li>The covariance can also be computed as
\[ \mathrm{Cov}(X, Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y) \]</li>

</ul>

</section>
<section id="slide-orgb335c96">
<h4 id="orgb335c96">Correlation coefficient</h4>
<ul>
<li><p>
The <b>correlation coefficient</b> of \(X\) and \(Y\) is
</p>

<p>
\[ \mathrm{corr}(X, Y) = \rho_{XY} = \frac{\mathrm{Cov}(X, Y)}{\left[\mathrm{Var}(X)\mathrm{Var}(Y)\right]^{1/2}} =
  \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}} \]
</p></li>

<li>\(-1 \leq \mathrm{corr}(X, Y) \leq 1\).</li>

<li>\(\mathrm{corr}(X, Y)=0\) (or \(\mathrm{Cov}(X,Y)=0\)) means that \(X\)
and \(Y\) are uncorrelated.</li>

<li>Since \(\mathrm{Cov}(X, Y) = \mathrm{E}(XY) -
  \mathrm{E}(X)\mathrm{E}(Y)\), when \(X\) and \(Y\) are uncorrelated, then \(\mathrm{E}(XY) =
  \mathrm{E}(X) \mathrm{E}(Y)\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-org5a24a02">
<h3 id="org5a24a02">Independence and uncorrelation</h3>
<ul>
<li><p>
If \(X\) and \(Y\) are independent, then
</p>
<div>
\begin{align*}
\mathrm{Cov}(X, Y) & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i) \mathrm{Pr}(Y=y_j) \\
                   & = \sum_{i=1}^n (x_i - \mu_X) \mathrm{Pr}(X=x_i) \sum_{j=1}^m (y_j - \mu_y) \mathrm{Pr}(Y=y_j) \\
                   & = 0 \times 0 = 0
\end{align*}

</div></li>

<li>That is, if \(X\) and \(Y\) are independent, they must be
uncorrelated.</li>

<li>However, the converse is not true. If \(X\) and \(Y\) are
uncorrelated, there is a possibility that they are actually
dependent.</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgb7f982f">
<h3 id="orgb7f982f">Conditional mean and correlation</h3>
<ul>
<li>If \(X\) and \(Y\) are independent, then we must have 
\(\mathrm{E}(Y \mid X) = \mathrm{E}(Y) = \mu_Y\)</li>

<li><p>
Then, we can prove that
\(\mathrm{Cov}(X, Y) = 0\) and \(\mathrm{corr}(X, Y)=0\).
</p>

<div>
\begin{align*}
\mathrm{E}(XY) & = \mathrm{E}(\mathrm{E}(XY \mid X)) = \mathrm{E}(X \mathrm{E}(Y \mid X)) \\
               & = \mathrm{E}(X) \mathrm{E}(Y \mid X) = \mathrm{E}(X) \mathrm{E}(Y)
\end{align*}

</div>

<p>
It follows that \(\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X)
   \mathrm{E}(Y) = 0\) and \(\mathrm{corr}(X, Y)=0\). 
</p></li>

</ul>

</section>
</section>
<section>
<section id="slide-org27727af">
<h3 id="org27727af">Some useful operations</h3>
<p>
The following properties
of \(\mathrm{E}(\cdot)\), \(\mathrm{Var}(\cdot)\) and
\(\mathrm{Cov}(\cdot)\) are useful in calculation,
</p>

<div>
\begin{align*}
\mathrm{E}(a + bX + cY)      & = a + b \mu_{X} + c \mu_{Y} \\
\mathrm{Var}(aX + bY)        & = a^{2} \sigma^{2}_{X} + b^{2} \sigma^{2}_{Y} + 2ab\sigma_{XY} \\
\mathrm{Cov}(a + bX + cV, Y) & = b\sigma_{XY} + c\sigma_{VY} \\
\end{align*}

</div>


</section>
</section>
<section>
<section id="slide-orgab52495">
<h2 id="orgab52495">Four Specific Distributions</h2>
<div class="outline-text-2" id="text-orgab52495">
</div></section>
</section>
<section>
<section id="slide-orga9ab640">
<h3 id="orga9ab640">The normal distribution</h3>
<div class="outline-text-3" id="text-orga9ab640">
</div></section>
<section id="slide-orgb592ebd">
<h4 id="orgb592ebd">The normal distribution</h4>
<ul>
<li>The p.d.f. of a normally distributed random variable \(X\) is
\[ f(x) =
  \frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right]
  \]</li>
<li>\(\mathrm{E}(X) = \mu\) and \(\mathrm{Var}(X) = \sigma^{2}\).</li>
<li>We write \(X \sim N(\mu, \sigma^{2})\)</li>

</ul>

</section>
<section id="slide-orgb42d7fb">
<h4 id="orgb42d7fb">The standard normal distribution</h4>
<ul>
<li>The standard normal distribution is a special case of the normal
distribution, for which \(\mu = 0\) and \(\sigma = 0\). The p.d.f of the
standard normal distribution is
\[
  \phi(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)
  \]</li>
<li>The c.d.f of the standard normal distribution is often denoted as
\(\Phi(x)\).</li>

</ul>

</section>
<section id="slide-org3dd33e1">
<h4 id="org3dd33e1">Symmetric and skinny tails</h4>
<ul>
<li>The normal distribution is symmetric around its mean, \(\mu\), with the
skewness equal 0</li>
<li>It has 95% of its probability between
\(\mu-1.96\sigma\) and \(\mu+1.96\sigma\), with the kurtosis
equal 3.</li>

</ul>


<div id="org2b2feb6" class="figure">
<p><img src="figure/Normal-distribution-curve.jpg" alt="Normal-distribution-curve.jpg" />
</p>
<p><span class="figure-number">Figure 6: </span>The normal probability density</p>
</div>

</section>
<section id="slide-org46c3270">
<h4 id="org46c3270">Transforming a normally distributed random variable to the standard normal distribution</h4>
<ul>
<li>Let \(X\) be a random variable with a normal distribution, i.e., \(X \sim
  N(\mu, \sigma^2)\).</li>
<li>We compute \(Z = (X-\mu)/\sigma\), which follows
the standard normal distribution, \(N(0, 1)\).</li>
<li>For example, if \(X \sim N(1, 4)\), then \(Z = (X-1)/2 \sim N(0,
  1)\). When we want to find \(\mathrm{Pr}(X \leq 4)\), we only need to
compute \(\Phi(3/2)\)</li>
<li><p>
Generally, for any two number \(c_1 < c_2\) and let \(d_1 = (c_1 - \mu)/\sigma\) and
\(d_2 = (c_2 - \mu)/\sigma\), we have
</p>
<div>
\begin{align*}
\mathrm{Pr}(X \leq c_2) & = \mathrm{Pr}(Z \leq d_2) = \Phi(d_2) \\
\mathrm{Pr}(X \geq c_1) & = \mathrm{Pr}(Z \geq d_1) = 1 - \Phi(d_1) \\
\mathrm{Pr}(c_1 \leq X \leq c_2) & = \mathrm{Pr}(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1)
\end{align*}

</div></li>

</ul>

</section>
<section id="slide-org708fe15">
<h4 id="org708fe15">The multivariate normal distribution</h4>
<ul>
<li>The multivariate normal distribution is the joint
distribution of a set of random variables.</li>

<li>The p.d.f. of the multivariate normal distribution is beyond the
scope of this course, but the following properties make this
distribution handy in analysis.</li>

</ul>

</section>
<section id="slide-org801c953">
<h4 id="org801c953">Important properties of the multivariate normal distribution</h4>
<ul>
<li>If n random variables, \(x_1, \ldots, x_n\), have a multivariate
normal distribution, then any linear combination of these variables
is normally distributed. For any real numbers, \(\alpha_1, \ldots,
  \alpha_n\), a linear combination of \({x_i}\) is \(\sum_i \alpha_i x_i\).</li>

<li>If a set of random variables has a multivariate normal
distribution, then the marginal distribution of each of the
variables is normal.</li>

<li>If random variables with a multivariate normal distribution have
covariances that equal zero, then these random variables are
independent.</li>

<li>If \(X\) and \(Y\) have a bivariate normal distribution, then
\(\mathrm{E}(Y|X = x) = a + bx\), where \(a\) and \(b\) are constants.</li>

</ul>

</section>
</section>
<section>
<section id="slide-org15d8f6f">
<h3 id="org15d8f6f">The chi-squared distribution</h3>
<ul>
<li>Let \(Z_1, \ldots, Z_n\) be n indepenent standard normal distribution,
i.e. \(Z_i \sim N(0, 1)\) for all \(i = 1, \ldots, n\). Then, the random
variable
\[W = \sum_{i=1}^n Z^2_i \]
has a chi-squared distribution with \(n\) degrees of freedom, denoted as
\(W \sim \chi^2(n)\), with \(\mathrm{E}(W) = n\) and \(\mathrm{Var}(W) = 2n\)</li>

<li>If \(Z \sim N(0, 1)\), then \(W = Z^2 \sim \chi^2(1)\) with \(\mathrm{E}(W) =
  1\) and \(\mathrm{Var}(W) = 2\).</li>

</ul>

</section>
</section>
<section>
<section id="slide-orgf5f3102">
<h3 id="orgf5f3102">The student t distribution</h3>
<ul>
<li>Let \(Z \sim N(0, 1)\), \(W \sim \chi^2(m)\), and \(Z\) and \(W\) be
independently distributed. Then, the random variable
\[t = \frac{Z}{\sqrt{W/m}} \]
has a student t distribution with \(m\) degrees of freedom, denoted as
\(t \sim t(m)\).</li>

<li>As \(n\) increases, \(t\) gets close to a standard normal distribution.</li>

</ul>

</section>
</section>
<section>
<section id="slide-org462b0b8">
<h3 id="org462b0b8">The F distribution</h3>
<ul>
<li>Let \(W_1 \sim \chi^2(n_1)\), \(W_2 \sim \chi^2(n_2)\), and \(W_1\) and
\(W_2\) are independent. Then, the random variable
\[ F = \frac{W_1/n_1}{W_2/n_2}\]
has an F distribution with \((n_1, n_2)\) degrees of freedom, denoted as
\(F \sim F(n_1, n_2)\)</li>

<li>If \(t \sim t(n)\), then \(t^2 \sim F(1, n)\)</li>

<li>As \(n_2 \rightarrow \infty\), the \(F(n_1, \infty)\) distribution is the
same as the \(\chi^2(n_1)\) distribution divided by \(n_1\).</li>

</ul>
</section>
</section>
</div>
</div>
<script src="../../../reveal.js/lib/js/head.min.js"></script>
<script src="../../../reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: true,
keyboard: true,
overview: true,
width: 1000,
height: 800,
margin: 0.20,
minScale: 0.50,
maxScale: 2.50,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'cube', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
{ src: '../../../reveal.js/plugin/menu/menu.js' },
 { src: '../../../reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
 { src: '../../../reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: '../../../reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
