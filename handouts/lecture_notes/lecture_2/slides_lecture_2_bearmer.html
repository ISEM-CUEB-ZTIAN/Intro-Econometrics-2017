<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>Lecture 2: Review of Probability</title>
<meta name="author" content="(Zheng Tian)"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="file:////Users/ztian/reveal.js/css/reveal.css"/>

<link rel="stylesheet" href="file:////Users/ztian/reveal.js/css/theme/moon.css" id="theme"/>


<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
    if( window.location.search.match( /print-pdf/gi ) ) {
        var link = document.createElement( 'link' );
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = 'file:////Users/ztian/reveal.js/css/print/pdf.css';
        document.getElementsByTagName( 'head' )[0].appendChild( link );
    }
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide"><h1 class="title">Lecture 2: Review of Probability</h1><h2 class="author">Zheng Tian</h2><p class="date">Created: 2017-02-26 Sun 21:18</p>
</section>
<section id="table-of-contents">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#/slide-org81e3dde">1. Random Variables and Probability Distributions</a></li>
<li><a href="#/slide-org9448cb0">2. Expectation, Variance, and Other Moments</a></li>
<li><a href="#/slide-org80f0c1e">3. Two Random Variables</a></li>
<li><a href="#/slide-orgd56373d">4. Four Specific Distributions</a></li>
<li><a href="#/slide-orga139986">5. Random Sampling and the Distribution of the Sample Average</a></li>
<li><a href="#/slide-org80496cb">6. Large Sample Approximations to Sampling Distributions</a></li>
</ul>
</div>
</div>
</section>

<section>
<section id="slide-org81e3dde">
<h2 id="org81e3dde"><span class="section-number-2">1</span> Random Variables and Probability Distributions</h2>
<div class="outline-text-2" id="text-1">
</div></section>
<section id="slide-orgc1cace7">
<h3 id="orgc1cace7">Defining probabilities and random variables</h3>
<div class="outline-text-3" id="text-orgc1cace7">
</div><ul class="org-ul"><li><a id="org59c4e37"></a>Experiments and outcomes&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>An <b>experiment</b> is the processes that generate random results</li>
<li>The <b>outcomes</b> of an experiment are its
mutually exclusive potential results.</li>
<li>Example: tossing a coin. The outcome is either getting a head(H) or a tail(T)
but not both.</li>

</ul></li>

<li><a id="org311b980"></a>Sample space and events&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>A <b>sample space</b> consists of all the outcomes from an experiment,
denoted with the set \(S\).
<ul>
<li>\(S = \{H, T\}\) in the tossing-coin experiment.</li>

</ul></li>

<li>An <b>event</b> is a subset of the sample 
space.</li>

<li>Getting a head is an event, which is \(\{H\} \subset \{H, T\}\).</li>

</ul></li></ul>

</section>
<section id="slide-orgd40f8c8">
<h3 id="orgd40f8c8">Probability</h3>
<div class="outline-text-3" id="text-orgd40f8c8">
</div><ul class="org-ul"><li><a id="org408db46"></a>An intuitive definition of probability&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>The <b>probability</b> of an event is the proportion of the time that the
event will occur in the long run.</li>

<li>For example, we toss a coin for \(n\)
times and get \(m\) heads. When \(n\) is very large, we can say that the
probability of getting a head in a toss is \(m/n\).</li>

</ul></li>

<li><a id="orgca90050"></a>An axiomatic definition of probability&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>A probability of an event \(A\) in the sample space \(S\), denoted as
\(\mathrm{Pr}(A)\), is a function that assign \(A\) a real number in \([0,
  1]\), satisfying the following three conditions:
<ol>
<li>\(0 \leq \mathrm{Pr}(A) \leq 1\).</li>
<li>\(\mathrm{Pr}(S) = 1\).</li>
<li>For any disjoint sets, \(A\) and \(B\), that is \(A\) and \(B\) have no
element in common, \(\mathrm{Pr}(A \cup B) = \mathrm{Pr}(A) +
    \mathrm{Pr}(B)\).</li>

</ol></li>

</ul></li></ul>

</section>
<section id="slide-org9a83f50">
<h3 id="org9a83f50">Random variables</h3>
<div class="outline-text-3" id="text-org9a83f50">
</div><ul class="org-ul"><li><a id="orgcdc2aac"></a>The definition of random variables&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>A <b>random variable</b> is a numerical summary associated with the
outcomes of an experiment.</li>

<li>You can also think of a random variable as a function
mapping from an event \(\omega\) in the sample space \(\Omega\) to the
real line.</li>

</ul></li>

<li><a id="org01ddcc1"></a>An illustration of random variables&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><div id="orgf4efdbd" class="figure">
<p><img src="figure/random_variable_demo1.png" alt="random_variable_demo1.png" width="600" />
</p>
<p><span class="figure-number">Figure 1: </span>An illustration of random variable</p>
</div></li>

<li><a id="orgc4b19e8"></a>Discrete and continuous random variables&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><p>
Random variables can take different types of values
</p>

<ul>
<li>A <b>discrete</b> random
variables takes on a discrete set of values, like \(0, 1, 2, \ldots, n\)</li>
<li>A <b>continuous</b> random variable takes on a continuum of possble
values, like any value in the interval \((a, b)\).</li>

</ul></li></ul>

</section>
<section id="slide-orgf8001bd">
<h3 id="orgf8001bd">Probability distributions</h3>
<div class="outline-text-3" id="text-orgf8001bd">
</div><ul class="org-ul"><li><a id="orge2e56c6"></a>The probability distribution for a discrete random variable&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>The probability distribution of a discrete random variable is the list
of all possible values of the variable and the probability that each
value will occur. These probabilities sum to 1.</li>

<li><p>
The probability mass function. Let \(X\) be a discrete random
variable. The probability distribution of \(X\) (or the probability
mass function), \(p(x)\), is
</p>
<div>
\begin{equation*}
p(x) = \mathrm{Pr}(X = x)
\end{equation*}

</div></li>

<li>The axioms of probability require that 
<ol>
<li>\(0 \leq p(x) \leq  1\)</li>
<li>2) \( \sum_{i=1}^n p(x_i) =  1\).</li>

</ol></li>

</ul></li>

<li><a id="org38c8a48"></a>An example of the probability distribution of a discrete random variable&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><table id="org283f492" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> An illustration of the probability distribution of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-right">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.50</td>
<td class="org-right">0.25</td>
<td class="org-right">1.</td>
</tr>
</tbody>
</table></li></ul>

</section>
<section id="slide-org2e8fec0">
<h3 id="org2e8fec0">The cumulative probability distribution</h3>
<div class="outline-text-3" id="text-org2e8fec0">
</div><ul class="org-ul"><li><a id="org0a3e2f7"></a>Definition of the c.d.f.&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li><p>
The <b>cumulative probability distribution</b> (or the cumulative
distribution function, c.d.f.): 
</p>

<p>
Let \(F(x)\) be the c.d.f of \(X\). Then \(F(x) = \mathrm{Pr}(X \leq x)\).
</p></li>

</ul>

<table id="orge1bdcad" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> An illustration of the c.d.f. of a discrete random variable</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">\(X\)</th>
<th scope="col" class="org-right">1</th>
<th scope="col" class="org-right">2</th>
<th scope="col" class="org-right">3</th>
<th scope="col" class="org-left">Sum</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(\mathrm{P}(x)\)</td>
<td class="org-right">0.25</td>
<td class="org-right">0.50</td>
<td class="org-right">0.25</td>
<td class="org-left">1</td>
</tr>

<tr>
<td class="org-left">C.d.f.</td>
<td class="org-right">0.25</td>
<td class="org-right">0.75</td>
<td class="org-right">1</td>
<td class="org-left">--</td>
</tr>
</tbody>
</table></li>

<li><a id="org7e2a97c"></a>An illustration of the c.d.f. of a discrete random variable&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><div id="orgb34aa65" class="figure">
<p><img src="figure/cdf_discrete_example.png" alt="cdf_discrete_example.png" width="500" height="450" />
</p>
<p><span class="figure-number">Figure 2: </span>The c.d.f. of a discrete random variable</p>
</div></li></ul>

</section>
<section id="slide-org9882890">
<h3 id="org9882890">Bernouli distribution</h3>
<p>
The Bernoulli distribution
</p>
<div>
\begin{equation*}
  G =
    \begin{cases}
      1 & \text{with probability } p \\
      0 & \text{with probability } 1-p
    \end{cases}
  \end{equation*}

</div>

</section>
<section id="slide-orga00dc9a">
<h3 id="orga00dc9a">The probability distribution of a continuous random variable</h3>
<div class="outline-text-3" id="text-orga00dc9a">
</div><ul class="org-ul"><li><a id="orgdee9bf0"></a>Definition of the c.d.f. and the p.d.f.&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>The cumulative distribution function of a continous random variable
is defined as it is for a discrete random variable. 
\[ F(x) = \mathrm{Pr}(X \leq x) \]</li>

<li>The <b>probability density function (p.d.f.)</b> of \(X\) is the function
that satisfies
\[ F(x) = \int_{-\infty}^{x} f(t) \mathrm{d}t \text{ for all } x \]</li>

</ul></li>

<li><a id="org98305f4"></a>Properties of the c.d.f.&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>For both discrete and continuous random variable, \(F(X)\) must satisfy
the following properties:
<ol>
<li>\(F(+\infty) = 1 \text{ and } F(-\infty) = 0\) (\(F(x)\) is bounded between 0 and 1)</li>
<li>\(x > y \Rightarrow F(x) \geq F(y)\) (\(F(x)\) is nondecreasing)</li>

</ol></li>

<li>By the definition of the c.d.f., we can conveniently calculate
probabilities, such as,
<ul>
<li>\(\mathrm{P}(x > a) = 1 - \mathrm{P}(x \leq a) = 1 - F(a)\)</li>
<li>\(\mathrm{P}(a < x \leq b) = F(b) - F(a)\).</li>

</ul></li>

</ul></li>

<li><a id="orgd7354cd"></a>The c.d.f. and p.d.f. of a normal distribution&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><div id="org724de17" class="figure">
<p><img src="figure/norm1.png" alt="norm1.png" width="500" height="450" />
</p>
<p><span class="figure-number">Figure 3: </span>The p.d.f. and c.d.f. of a continuous random variable (the normal distribution)</p>
</div></li></ul>


</section>
</section>
<section>
<section id="slide-org9448cb0">
<h2 id="org9448cb0"><span class="section-number-2">2</span> Expectation, Variance, and Other Moments</h2>
<div class="outline-text-2" id="text-2">
</div></section>
<section id="slide-org3292f06">
<h3 id="org3292f06">The expected value of a random variable</h3>
<div class="outline-text-3" id="text-org3292f06">
</div><ul class="org-ul"><li><a id="orgd35ee18"></a>The expected value&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>The <b>expected value</b> of a random variable, X, denoted as \(\mathrm{E}(X)\), is
the long-run average of the random variable over many repeated
trials or occurrences, which is also called the <b>expectation</b> or the
<b>mean</b>.</li>

<li>The expected value measures the centrality of a random variable.</li>

</ul></li>

<li><a id="org69107c5"></a>Mathematical definition&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>For a discrete random variable
\[ \mathrm{E}(X) = \sum_{i=1}^n x_i \mathrm{Pr}(X = x_i) \]</li>

<li>e.g. The expectation of a Bernoulli random variable, \(G\),
\[ \mathrm{E}(G) = 1 \cdot p + 0 \cdot (1-p) = p \]</li>

<li>For a continuous random variable
\[ \mathrm{E}(X) = \int_{-\infty}^{\infty} x f(x) \mathrm{d}x\]</li>

</ul></li></ul>

</section>
<section id="slide-org26cd3c9">
<h3 id="org26cd3c9">The variance and standard deviation</h3>
<div class="outline-text-3" id="text-org26cd3c9">
</div><ul class="org-ul"><li><a id="org1bfb1d9"></a>Definition of variance and standard deviation&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>The <b>variance</b> of a random variable \(X\) measures its average
deviation from its own expected value.</li>

<li><p>
Let \(\mathrm{E}(X) = \mu_X\). Then the variance of \(X\),
</p>

<div>
\begin{align*}
\mathrm{Var}(X) & =  \sigma^2_X =  \mathrm{E}(X-\mu_X)^{2} \\
& = 
\begin{cases}
\sum_{i=1}^n (x_i - \mu_X)^{2}\mathrm{Pr}(X = x_i) & \text{if } X \text{ is discrete} \\
\int_{-\infty}^{\infty} (x - \mu_X)^{2}f(x)\mathrm{d} x  & \text{if } X \text{ is continuous}
\end{cases}
\end{align*}

</div></li>

<li>The <b>standard deviation</b> of \(X\): \(\sigma_{X} = \sqrt{\mathrm{Var}(X)}\)</li>

</ul></li>

<li><a id="orgf3b5dd8"></a>Computing variance&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>A convenient formula for calculating the variance is
\[ \mathrm{Var}(X) = \mathrm{E}(X - \mu_X)^{2} = \mathrm{E}(X^{2}) - \mu_X^{2} \]</li>

<li>The variance of a Bernoulli random variable, \(G\)
\[ \mathrm{Var}(G) = (1-p)^{2}p + (0-p)^{2}(1-p) = p(1-p) \]</li>

<li>The expectation and variance of a linear function of \(X\). Let \(Y = a +
  bX\), then
<ul>
<li>\(\mathrm{E}(Y) = a + b\mathrm{E}(X)\)</li>
<li>\(\mathrm{Var}(Y) = \mathrm{Var}(a + b X) = b^{2} \mathrm{Var}(X)\).</li>

</ul></li>

</ul></li></ul>

</section>
<section id="slide-orgac84cb4">
<h3 id="orgac84cb4">Moments of a random variable, skewness and kurtosis</h3>
<div class="outline-text-3" id="text-orgac84cb4">
</div><ul class="org-ul"><li><a id="org2fad741"></a>Definition of the moments of a distribution&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><dl>
<dt>k<sup>th</sup> moment</dt><dd>The k<sup>th</sup> <b>moment</b> of the distribution of \(X\) is
\(\mathrm{E}(X^{k})\). So, the expectation is the "first"
moment of \(X\).</dd>

<dt>k<sup>th</sup> central moment</dt><dd>The k<sup>th</sup> central moment of the distribution
of \(X\) with its mean \(\mu_X\) is \(\mathrm{E}(X - \mu_X)^{k}\). So, the
variance is the second central moment of \(X\).</dd>

</dl>

<ul class="org-ul"><li><a id="orgcf3a6c6"></a>A caveat<br  /><p>
It is important to remember that not all the moments of a distribution
exist. 
</p></li></ul></li>

<li><a id="orge506172"></a>Skewness&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li><p>
The skewness of a distribution provides a mathematical way to describe
how much a distribution deviates from symmetry.
</p>

<p>
\[ \text{Skewness} =  \mathrm{E}(X - \mu_X)^{3}/\sigma_{X}^{3} \]
</p></li>

<li>A symmetric distribution has a skewness of zero.</li>
<li>The skewness can be either positive or negative.</li>
<li>That \(\mathrm{E}(X - \mu_X)^3\) is divided by \(\sigma^3_X\) is to make
the skewness measure unit free.</li>

</ul></li>

<li><a id="orga634109"></a>Kurtosis&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li><p>
The kurtosis of the distribution of a random variable \(X\) measures how
much of the variance of \(X\) arises from extreme values, which makes
the distribution have "heavy" tails.
</p>

<p>
\[ \text{Kurtosis} = \mathrm{E}(X - \mu_X)^{4}/\sigma_{X}^{4} \]
</p></li>

<li>The kurtosis must be positive.</li>
<li>The kurtosis of the normal distribution is 3. So a distribution that
has its kurtosis exceeding 3 is called heavy-tailed.</li>
<li>The kurtosis is also unit free.</li>

</ul></li>

<li><a id="org74f4945"></a>An illustration of skewness and kurtosis&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><div class="figure">
<p><img src="figure/fig-2-3.png" alt="fig-2-3.png" />
</p>
</div>

<ul>
<li>All four distributions have a mean of zero and
a variance of one, while (a) and (b) are symmetric and (b)-(d) are
heavy-tailed.</li>

</ul></li></ul>


</section>
</section>
<section>
<section id="slide-org80f0c1e">
<h2 id="org80f0c1e"><span class="section-number-2">3</span> Two Random Variables</h2>
<div class="outline-text-2" id="text-3">
</div></section>
<section id="slide-orgdb1fd42">
<h3 id="orgdb1fd42">The joint and marginal distributions</h3>
<div class="outline-text-3" id="text-orgdb1fd42">
</div><ul class="org-ul"><li><a id="org14dda08"></a>The joint probability function of two discrete random variables<br  /><ul>
<li>The joint distribution of two random variables \(X\) and \(Y\) is
\[ p(x, y) = \mathrm{Pr}(X = x, Y = y)\]</li>

<li>\(p(x, y)\) must satisfy
<ol>
<li>\(p(x, y) \geq 0\)</li>
<li>\(\sum_{i=1}^n\sum_{j=1}^m p(x_i, y_j) = 1\) for all possible
combinations of values of \(X\) and \(Y\).</li>

</ol></li>

</ul></li>

<li><a id="org986e709"></a>The joint probability function of two continuous random variables<br  /><ul>
<li>For two continuous random variables, \(X\) and \(Y\), the counterpart of \(p(x, y)\) is
the joint probability density function, \(f(x, y)\), such that
<ol>
<li>\(f(x, y) \geq 0\)</li>
<li>\(\int_{-\infty}^{{\infty}} \int_{-\infty}^{\infty} f(x, y)\, dx\, dy= 1\)</li>

</ol></li>

</ul></li></ul>

</section>
<section id="slide-org32d146b">
<h3 id="org32d146b">The marginal probability distribution</h3>
<ul>
<li>The marginal probability distribution of a random variable \(X\) is
simply the probability distribution of its own.</li>

<li>For a discrete random variable, we can compute the marginal
distribution of \(X\) as
\[ \mathrm{Pr}(X=x) = \sum_{i=1}^n \mathrm{Pr}(X, Y=y_i) = \sum_{i=1}^n p(x, y_i)  \]</li>

<li>For a continuous random variable, the marginal distribution is
\[f_X(x) = \int_{-\infty}^{\infty} f(x, y)\, dy \]</li>

</ul>

</section>
<section id="slide-orge7fee7b">
<h3 id="orge7fee7b">An example of joint and marginal distributions</h3>
<table id="org2ea6520" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> Joint and marginal distributions of raining and commuting time</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Rain (\(X=0\))</th>
<th scope="col" class="org-right">No rain (\(X=1\))</th>
<th scope="col" class="org-right">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Long commute (\(Y=0\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.07</td>
<td class="org-right">0.22</td>
</tr>

<tr>
<td class="org-left">Short commute (\(Y=1\))</td>
<td class="org-right">0.15</td>
<td class="org-right">0.63</td>
<td class="org-right">0.78</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Total</td>
<td class="org-right">0.30</td>
<td class="org-right">0.70</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>

</section>
<section id="slide-org50caacc">
<h3 id="org50caacc">Conditional probability</h3>
<ul>
<li><p>
For any two events \(A\) and \(B\), the conditional probability of \(A\) given
\(B\) is defined as
</p>
<div>
\begin{equation*}
\mathrm{Pr}(A|B) = \frac{\mathrm{Pr}(A \cap B)}{\mathrm{Pr}(B)}
\end{equation*}

</div></li>

</ul>


<div class="figure">
<p><img src="figure/conditional_probability.png" alt="conditional_probability.png" />
</p>
</div>

</section>
<section id="slide-orge1ee4a5">
<h3 id="orge1ee4a5">The conditional probability distribution</h3>
<ul>
<li>The conditional distribution of a random variable \(Y\) given another
random variable \(X\) is \(\mathrm{Pr}(Y | X=x)\).</li>

<li>The formula to compute it is
\[ \mathrm{Pr}(Y | X=x) = \frac{\mathrm{Pr}(X=x, Y)}{\mathrm{Pr}(X=x)} \]</li>

<li>For continuous random variables \(X\) and \(Y\), we define the conditional
density function as
\[ f(y|x) = \frac{f(x, y)}{f_X(x)} \]</li>

</ul>

</section>
<section id="slide-orgebe0746">
<h3 id="orgebe0746">The conditional expectation</h3>
<ul>
<li>The <b>conditional expectation</b> of \(Y\) given \(X\) is the expected value
of the conditional distribution of \(Y\) given \(X\).</li>

<li><p>
For discrete random variables, the conditional mean of \(Y\) given \(X=x\) is
</p>
<div>
\begin{equation*}
\mathrm{E}(Y \mid X=x) = \sum_{i=1}^n y_i \mathrm{Pr}(Y=y_i \mid X=x)
\end{equation*}

</div></li>

<li><p>
For continuous random variables, it is computed as
</p>
<div>
\begin{equation*}
\int_{-\infty}^{\infty} y f(y \mid x)\, dy
\end{equation*}

</div></li>

<li>The expected mean of commuting time given it is raining is \(0 \times
  0.1 + 1 \times 0.9 = 0.9\).</li>

</ul>

</section>
<section id="slide-org97dae2b">
<h3 id="org97dae2b">The law of iterated expectation</h3>
<ul>
<li><p>
<b>The law of iterated expectation</b>:
</p>

<p>
\[ \mathrm{E}(Y) = E \left[ \mathrm{E}(Y|X) \right] \]
</p></li>

<li>It says that the mean of \(Y\) is the weighted average of the
conditional expectation of \(Y\) given \(X\), weighted by the
probability distribution of \(X\). That is,
\[ \mathrm{E}(Y) = \sum_{i=1}^n \mathrm{E}(Y \mid X=x_i) \mathrm{Pr}(X=x_i) \]</li>

<li>If \(\mathrm{E}(X|Y) = 0\), then \(\mathrm{E}(X)=E\left[\mathrm{E}(X|Y)\right]=0\).</li>

</ul>

</section>
<section id="slide-org8b94ea0">
<h3 id="org8b94ea0">Conditional variance</h3>
<ul>
<li>With the conditional mean of \(Y\) given \(X\), we can compute the
conditional variance as
\[ \mathrm{Var}(Y \mid X=x) = \sum_{i=1}^n \left[ y_i - \mathrm{E}(Y \mid X=x)
  \right]^2 \mathrm{Pr}(Y=y_i \mid X=x) \]</li>

<li>From the law of iterated expectation, we can get the following
\[ \mathrm{Var}(Y) = \mathrm{E}(\mathrm{Var}(Y \mid X)) + \mathrm{Var}(\mathrm{E}(Y \mid
  X)) \]</li>

</ul>

</section>
<section id="slide-org9788079">
<h3 id="org9788079">Independent random variables</h3>
<ul>
<li>Two random variables \(X\) and \(Y\) are <b>independently distributed</b>, or
<b>independent</b>, if knowing the value of one of the variable provides no
information about the other.</li>
<li>Mathematically, it means that 
\[ \mathrm{Pr}(Y=y \mid X=x) = \mathrm{Pr}(Y=y)  \]</li>

<li>If \(X\) and \(Y\) are independent
\[ \mathrm{Pr}(Y=y, X=x) = \mathrm{Pr}(X=x) \mathrm{Pr}(Y=y) \]</li>

</ul>

</section>
<section id="slide-org323a6d2">
<h3 id="org323a6d2">Independence between two continuous random variable</h3>
<ul>
<li>For two continuous random variables, \(X\) and \(Y\), they are
<b>independent</b> if
\[ f(x|y) = f_{X}(x) \text{ or } f(y|x) = f_{Y}(y) \]</li>

<li>It follows that if \(X\) and \(Y\) are independent
\[ f(x, y) = f(x|y)f_{Y}(y) = f_{X}(x)f_{Y}(y) \]</li>

</ul>

</section>
<section id="slide-org342f045">
<h3 id="org342f045">Covariance and Correlation</h3>
<div class="outline-text-3" id="text-org342f045">
</div><ul class="org-ul"><li><a id="org69fc4c2"></a>Covariance&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li><p>
The covariance of two discrete random variables \(X\) and \(Y\) is
</p>
<div>
\begin{align*}
\mathrm{Cov}(X, Y) & = \sigma_{XY} = \mathrm{E}(X-\mu_{X})(Y-\mu_{Y}) \\
                   & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i, Y=y_j)
\end{align*}

</div></li>

<li>For continous random variables, the covariance of \(X\) and \(Y\) is
\[ \mathrm{Cov}(X, Y) = \int_{-\infty}^{\infty}
  \int_{-\infty}^{\infty} (x-\mu_X)(y-\mu_y)f(x, y) dx dy \]</li>

<li>The covariance can also be computed as
\[ \mathrm{Cov}(X, Y) = \mathrm{E}(XY) - \mathrm{E}(X)\mathrm{E}(Y) \]</li>

</ul></li>

<li><a id="orgb41e424"></a>Correlation coefficient&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li><p>
The <b>correlation coefficient</b> of \(X\) and \(Y\) is
</p>

<p>
\[ \mathrm{corr}(X, Y) = \rho_{XY} = \frac{\mathrm{Cov}(X, Y)}{\left[\mathrm{Var}(X)\mathrm{Var}(Y)\right]^{1/2}} =
  \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}} \]
</p></li>

<li>\(-1 \leq \mathrm{corr}(X, Y) \leq 1\).</li>

<li>\(\mathrm{corr}(X, Y)=0\) (or \(\mathrm{Cov}(X,Y)=0\)) means that \(X\)
and \(Y\) are uncorrelated.</li>

<li>Since \(\mathrm{Cov}(X, Y) = \mathrm{E}(XY) -
  \mathrm{E}(X)\mathrm{E}(Y)\), when \(X\) and \(Y\) are uncorrelated, then \(\mathrm{E}(XY) =
  \mathrm{E}(X) \mathrm{E}(Y)\).</li>

</ul></li></ul>

</section>
<section id="slide-orgd2590df">
<h3 id="orgd2590df">Independence and uncorrelation</h3>
<ul>
<li><p>
If \(X\) and \(Y\) are independent, then
</p>
<div>
\begin{align*}
\mathrm{Cov}(X, Y) & = \sum_{i=1}^n \sum_{j=1}^m (x_i - \mu_X)(y_j - \mu_Y) \mathrm{Pr}(X=x_i) \mathrm{Pr}(Y=y_j) \\
                   & = \sum_{i=1}^n (x_i - \mu_X) \mathrm{Pr}(X=x_i) \sum_{j=1}^m (y_j - \mu_y) \mathrm{Pr}(Y=y_j) \\
                   & = 0 \times 0 = 0
\end{align*}

</div></li>

<li>That is, if \(X\) and \(Y\) are independent, they must be
uncorrelated.</li>

<li>However, the converse is not true. If \(X\) and \(Y\) are
uncorrelated, there is a possibility that they are actually
dependent.</li>

</ul>

</section>
<section id="slide-org767fb52">
<h3 id="org767fb52">Conditional mean and correlation</h3>
<ul>
<li>If \(X\) and \(Y\) are independent, then we must have 
\(\mathrm{E}(Y \mid X) = \mathrm{E}(Y) = \mu_Y\)</li>

<li><p>
Then, we can prove that
\(\mathrm{Cov}(X, Y) = 0\) and \(\mathrm{corr}(X, Y)=0\).
</p>

<div>
\begin{align*}
\mathrm{E}(XY) & = \mathrm{E}(\mathrm{E}(XY \mid X)) = \mathrm{E}(X \mathrm{E}(Y \mid X)) \\
               & = \mathrm{E}(X) \mathrm{E}(Y \mid X) = \mathrm{E}(X) \mathrm{E}(Y)
\end{align*}

</div>

<p>
It follows that \(\mathrm{Cov}(X,Y) = \mathrm{E}(XY) - \mathrm{E}(X)
   \mathrm{E}(Y) = 0\) and \(\mathrm{corr}(X, Y)=0\). 
</p></li>

</ul>

</section>
<section id="slide-orgb1a24ff">
<h3 id="orgb1a24ff">Some useful operations</h3>
<p>
The following properties
of \(\mathrm{E}(\cdot)\), \(\mathrm{Var}(\cdot)\) and
\(\mathrm{Cov}(\cdot)\) are useful in calculation,
</p>

<div>
\begin{align*}
\mathrm{E}(a + bX + cY)      & = a + b \mu_{X} + c \mu_{Y} \\
\mathrm{Var}(aX + bY)        & = a^{2} \sigma^{2}_{X} + b^{2} \sigma^{2}_{Y} + 2ab\sigma_{XY} \\
\mathrm{Cov}(a + bX + cV, Y) & = b\sigma_{XY} + c\sigma_{VY} \\
\end{align*}

</div>




</section>
</section>
<section>
<section id="slide-orgd56373d">
<h2 id="orgd56373d"><span class="section-number-2">4</span> Four Specific Distributions</h2>
<div class="outline-text-2" id="text-4">
</div></section>
<section id="slide-org0b38efd">
<h3 id="org0b38efd">The normal distribution</h3>
<div class="outline-text-3" id="text-org0b38efd">
</div><ul class="org-ul"><li><a id="org3c9f68e"></a>The normal distribution<br  /><ul>
<li>The p.d.f. of a normally distributed random variable \(X\) is
\[ f(x) =
  \frac{1}{\sigma\sqrt{2\pi}}\exp\left[-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right]
  \]</li>
<li>\(\mathrm{E}(X) = \mu\) and \(\mathrm{Var}(X) = \sigma^{2}\), and we write \(X \sim N(\mu, \sigma^{2})\)</li>

</ul></li>

<li><a id="org2edff5c"></a>The standard normal distribution<br  /><ul>
<li>The standard normal distribution has \(\mu = 0\) and \(\sigma = 0\). The p.d.f of the
standard normal distribution is
\[
  \phi(x) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^2}{2}\right)
  \]</li>
<li>The c.d.f of the standard normal distribution is often denoted as
\(\Phi(x)\).</li>

</ul></li></ul>

</section>
<section id="slide-orga5c74dc">
<h3 id="orga5c74dc">Symmetric and skinny tails</h3>
<ul>
<li>The normal distribution is symmetric around its mean, \(\mu\), with the
skewness equal 0</li>
<li>It has 95% of its probability between
\(\mu-1.96\sigma\) and \(\mu+1.96\sigma\), with the kurtosis
equal 3.</li>

</ul>

</section>
<section id="slide-org45347d1">
<h3 id="org45347d1">The p.d.f. of the normal distribution</h3>

<div id="orga23acd2" class="figure">
<p><img src="figure/Normal-distribution-curve.png" alt="Normal-distribution-curve.png" />
</p>
<p><span class="figure-number">Figure 6: </span>The normal probability density</p>
</div>

</section>
<section id="slide-org7b749f4">
<h3 id="org7b749f4">Transforming a normally distributed random variable to the standard normal distribution</h3>
<ul>
<li>Let \(X\) be a random variable with a normal distribution, i.e., \(X \sim
  N(\mu, \sigma^2)\).</li>
<li>We compute \(Z = (X-\mu)/\sigma\), which follows
the standard normal distribution, \(N(0, 1)\).</li>
<li>For example, if \(X \sim N(1, 4)\), then \(Z = (X-1)/2 \sim N(0,
  1)\). When we want to find \(\mathrm{Pr}(X \leq 4)\), we only need to
compute \(\Phi(3/2)\)</li>

</ul>

</section>
<section id="slide-org4d0a008">
<h3 id="org4d0a008">Transforming a normally distributed random variable to the standard normal distribution</h3>
<ul>
<li><p>
Generally, for any two number \(c_1 < c_2\) and let \(d_1 = (c_1 - \mu)/\sigma\) and
\(d_2 = (c_2 - \mu)/\sigma\), we have
</p>
<div>
\begin{align*}
\mathrm{Pr}(X \leq c_2) & = \mathrm{Pr}(Z \leq d_2) = \Phi(d_2) \\
\mathrm{Pr}(X \geq c_1) & = \mathrm{Pr}(Z \geq d_1) = 1 - \Phi(d_1) \\
\mathrm{Pr}(c_1 \leq X \leq c_2) & = \mathrm{Pr}(d_1 \leq Z \leq d_2) = \Phi(d_2) - \Phi(d_1)
\end{align*}

</div></li>

</ul>

</section>
<section id="slide-orgfc30865">
<h3 id="orgfc30865">The multivariate normal distribution</h3>
<ul>
<li>The multivariate normal distribution is the joint
distribution of a set of random variables.</li>

<li>The p.d.f. of the multivariate normal distribution is beyond the
scope of this course, but the following properties make this
distribution handy in analysis.</li>

</ul>

</section>
<section id="slide-org48ce30e">
<h3 id="org48ce30e">Important properties of the multivariate normal distribution</h3>
<ul>
<li>If n random variables, \(x_1, \ldots, x_n\), have a multivariate
normal distribution, then any linear combination of these variables
is normally distributed. For any real numbers, \(\alpha_1, \ldots,
  \alpha_n\), a linear combination of \({x_i}\) is \(\sum_i \alpha_i x_i\).</li>

<li>If a set of random variables has a multivariate normal
distribution, then the marginal distribution of each of the
variables is normal.</li>

<li>If random variables with a multivariate normal distribution have
covariances that equal zero, then these random variables are
independent.</li>

<li>If \(X\) and \(Y\) have a bivariate normal distribution, then
\(\mathrm{E}(Y|X = x) = a + bx\), where \(a\) and \(b\) are constants.</li>

</ul>

</section>
<section id="slide-org3aba773">
<h3 id="org3aba773">The chi-squared distribution</h3>
<ul>
<li>Let \(Z_1, \ldots, Z_n\) be n indepenent standard normal distribution,
i.e. \(Z_i \sim N(0, 1)\) for all \(i = 1, \ldots, n\). Then, the random
variable
\[W = \sum_{i=1}^n Z^2_i \]
has a chi-squared distribution with \(n\) degrees of freedom, denoted as
\(W \sim \chi^2(n)\), with \(\mathrm{E}(W) = n\) and \(\mathrm{Var}(W) = 2n\)</li>

<li>If \(Z \sim N(0, 1)\), then \(W = Z^2 \sim \chi^2(1)\) with \(\mathrm{E}(W) =
  1\) and \(\mathrm{Var}(W) = 2\).</li>

</ul>

</section>
<section id="slide-org683e79a">
<h3 id="org683e79a">The p.d.f. of chi-squared distributions</h3>

<div id="org80c6ba5" class="figure">
<p><img src="figure/chi_squared_pdf.png" alt="chi_squared_pdf.png" width="700" />
</p>
<p><span class="figure-number">Figure 7: </span>The probability density function of chi-squared distributions</p>
</div>

</section>
<section id="slide-org68e48b9">
<h3 id="org68e48b9">The student t distribution</h3>
<ul>
<li>Let \(Z \sim N(0, 1)\), \(W \sim \chi^2(m)\), and \(Z\) and \(W\) be
independently distributed. Then, the random variable
\[t = \frac{Z}{\sqrt{W/m}} \]
has a student t distribution with \(m\) degrees of freedom, denoted as
\(t \sim t(m)\).</li>

<li>As \(n\) increases, \(t\) gets close to a standard normal distribution.</li>

</ul>

</section>
<section id="slide-orgfdd8403">
<h3 id="orgfdd8403">The p.d.f. of student t distributions</h3>

<div id="orgdebfc1d" class="figure">
<p><img src="figure/students_t_pdf.png" alt="students_t_pdf.png" width="700" />
</p>
<p><span class="figure-number">Figure 8: </span>The probability density function of student t distributions</p>
</div>

</section>
<section id="slide-org8234d84">
<h3 id="org8234d84">The F distribution</h3>
<ul>
<li>Let \(W_1 \sim \chi^2(n_1)\), \(W_2 \sim \chi^2(n_2)\), and \(W_1\) and
\(W_2\) are independent. Then, the random variable
\[ F = \frac{W_1/n_1}{W_2/n_2}\]
has an F distribution with \((n_1, n_2)\) degrees of freedom, denoted as
\(F \sim F(n_1, n_2)\)</li>

<li>If \(t \sim t(n)\), then \(t^2 \sim F(1, n)\)</li>

<li>As \(n_2 \rightarrow \infty\), the \(F(n_1, \infty)\) distribution is the
same as the \(\chi^2(n_1)\) distribution divided by \(n_1\).</li>

</ul>

</section>
<section id="slide-org81cddde">
<h3 id="org81cddde">The p.d.f. of F distributions</h3>

<div id="org35e3b6f" class="figure">
<p><img src="figure/fisher_f_pdf.png" alt="fisher_f_pdf.png" width="700" />
</p>
<p><span class="figure-number">Figure 9: </span>The probability density function of F distributions</p>
</div>


</section>
</section>
<section>
<section id="slide-orga139986">
<h2 id="orga139986"><span class="section-number-2">5</span> Random Sampling and the Distribution of the Sample Average</h2>
<div class="outline-text-2" id="text-5">
</div></section>
<section id="slide-orga28f66d">
<h3 id="orga28f66d">Random sampling</h3>
<div class="outline-text-3" id="text-orga28f66d">
</div><ul class="org-ul"><li><a id="org8df8007"></a>Simple random sampling&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>A <b>population</b> is a set of similar items or events which
is of interest for some question or experiment.</li>

<li><b>Simple random sampling</b> is a procedure in which \(n\) objects are
selected at random from a population, and each member of the
population is equally likely to be included in the sample.</li>

<li>Let \(Y_1, Y_2, \ldots Y_n\) be the first \(n\) observations in a random
sample. Since they are randomly drawn from a population, \(Y_1, \ldots,
  Y_n\) are random variables.</li>

</ul></li>

<li><a id="orgfb7c0c6"></a>i.i.d draws&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>Since \(Y_1, Y_2, \ldots, Y_n\) are drawn from the same population,
the marginal distribution of \(Y_i\) is the same for each \(i=1,
  \ldots, n\), which are said to be <b>identically distributed</b>.</li>

<li>With simple random sampling, the value of \(Y_i\) does not depend on
that of \(Y_j\) for \(i \neq j\), which are said to <b>independent
distributed</b>.</li>

<li>Therefore, when \(Y_1, \ldots, Y_n\) are drawn with simple random
sampling from the same distribution of \(Y\), we say that they are
<b>independently and identically distributed</b> or <b>i.i.d</b>, which is
denoted as 
\[ Y_i \sim IID(\mu_Y, \sigma^2_Y) \text{ for } i = 1, 2, \ldots, n\]
given that the population expectation is \(\mu_Y\) and the variance
is \(\sigma^2_Y\).</li>

</ul></li></ul>


</section>
<section id="slide-orgf56a55b">
<h3 id="orgf56a55b">The sampling distribution of the sample average</h3>
<div class="outline-text-3" id="text-orgf56a55b">
</div><ul class="org-ul"><li><a id="orgd759302"></a>The sample average&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>The <b>sample average</b> or <b>sample mean</b>, \(\overline{Y}\), of the \(n\)
observations \(Y_1, Y_2, \ldots, Y_n\) is
\[ \overline{Y} = \frac{1}{n}\sum^n_{i=1} Y_i \]</li>

<li>When \(Y_1, \ldots, Y_n\) are randomly drawn, \(\overline{Y}\) is also a
random variable that should have its own distribution, called the
<b>sampling distribution</b>.</li>

</ul></li>

<li><a id="orgcae4532"></a>The mean and variance of \(\overline{Y}\)&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>Suppose that \(Y_i \sim IID(\mu_Y, \sigma^2_{Y})\) for all \(i = 1,
  \ldots, n\). Then
\[
  \mathrm{E}(\overline{Y}) = \mu_{\overline{Y}} =
  \frac{1}{n}\sum^n_{i=1}\mathrm{E}(Y_i) = \frac{1}{n} n \mu_Y = \mu_Y
  \]
and
\[
  \mathrm{Var}(\overline{Y}) = \sigma^2_{\overline{Y}} =  \frac{1}{n^2}\sum^n_{i=1}\mathrm{Var}(Y_i) +
  \frac{1}{n^2}\sum^n_{i=1}\sum^n_{j=1}\mathrm{Cov}(Y_i, Y_j) =
  \frac{\sigma^2_Y}{n}
  \]</li>
<li>The standard deviation of the sample mean is
\(\sigma_{\overline{Y}} = \sigma_Y / \sqrt{n}\).</li>

</ul></li>

<li><a id="org7ea4d03"></a>Sampling distribution of \(\overline{Y}\) when \(Y\) is normally distributed&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>When \(Y_1, \ldots, Y_n\) are i.i.d. draws from \(N(\mu_Y,
  \sigma^2_Y)\), from the properties of the multivariate normal
distribution, \(\overline{Y}\) is normally distributed. That is 
\[ \overline{Y} \sim N(\mu_Y, \sigma^2_Y/n) \]</li>

</ul></li></ul>


</section>
</section>
<section>
<section id="slide-org80496cb">
<h2 id="org80496cb"><span class="section-number-2">6</span> Large Sample Approximations to Sampling Distributions</h2>
<div class="outline-text-2" id="text-6">
</div></section>
<section id="slide-org49a5d16">
<h3 id="org49a5d16">The exact distribution and the asymptotic distribution</h3>
<ul>
<li>The sampling distribution that exactly describes the distribution of
\(\overline{Y}\) for any \(n\) is called the <b>exact distribution</b> or
<b>finite-sample distribution</b>.</li>

<li>However, in most cases, we cannot obtain an exact distribution of
\(\overline{Y}\), for which we can only get an approximation.</li>

<li>The large-sample approximation to the sampling distribution is called the
<b>asymptotic distribution</b>.</li>

</ul>

</section>
<section id="slide-org4b5753b">
<h3 id="org4b5753b">The law of large numbers</h3>
<div class="outline-text-3" id="text-org4b5753b">
</div><ul class="org-ul"><li><a id="orged5cdcc"></a>Convergence in probability&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>Let \(S_1, \ldots, S_n\) be a sequence of random variables,
denoted as \(\{S_n\}\). \(\{S_n\}\) is said to converge in probability to a
limit &mu; (denoted as \(S_n \xrightarrow{\text{p}} \mu\)), if and only if
\[ \mathrm{Pr} \left(|S_n-\mu| < \delta \right) \rightarrow 1 \]
as \(n \rightarrow \infty\) for every \(\delta > 0\).</li>

<li>For example, \(S_n = \overline{Y}\). That is, \(S_1=Y_1\), \(S_2=1/2(Y_1+Y_2)\),
\(S_n=1/n\sum_i Y_i\), and so forth.</li>

</ul></li>

<li><a id="orgf0f1c8a"></a>The law of large numbers&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li>The law of large numbers (LLN) states that if \(Y_1, \ldots, Y_n\) are i.i.d. with
\(\mathrm{E}(Y_i)=\mu_Y\) and \(\mathrm{Var}(Y_i) < \infty\), then
\(\overline{Y} \xrightarrow{\text{p}} \mu_Y\).</li>

<li>The conditions for the LLN to be held is \(Y_i\) for \(i=1, \ldots, n\)
are i.i.d., and the variance of \(Y_i\) is finite. The latter says that
there is no extremely large outliers in the random samples.</li>

</ul></li>

<li><a id="orgc57a994"></a>The LLN illustrated&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><div id="orga23abda" class="figure">
<p><img src="figure/fig-2-8.png" alt="fig-2-8.png" width="600" />
</p>
<p><span class="figure-number">Figure 10: </span>An illustration of the law of large numbers</p>
</div></li></ul>

</section>
<section id="slide-org3829e10">
<h3 id="org3829e10">The central limit theorem</h3>
<div class="outline-text-3" id="text-org3829e10">
</div><ul class="org-ul"><li><a id="org6c3a9d8"></a>Convergence in distribution&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li><p>
Let \(F_1, F_2, \ldots, F_n\) be a sequence of cumulative distribution
functions corresponding to a sequence of random variables, \(S_1, S_2,
  \ldots, S_n\). Then the sequence of random variables \({S_n}\) is said to
<b>converge in distribution</b> to a random variable \(S\) (denoted as \(S_n
  \xrightarrow{\text{d}} S\)), if the distribution functions \(\{F_n\}\)
converge to \(F\) that is the distribution function of \(S\). We can write
it as
</p>

<p>
\[ S_n \xrightarrow{\text{d}} S \text{ if and only if } \lim_{n
  \rightarrow \infty}F_n(x)=F(x) \]
</p></li>

<li>The distribution \(F\) is called the <b>asymptotic distribution</b> of \(S_n\).</li>

</ul></li>

<li><a id="org68dbefd"></a>The central limit theorem (Lindeberg-Levy CLT)&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><ul>
<li><p>
The CLT states that if \(Y_1, Y_2, \ldots, Y_n\) are i.i.d. random samples from a
probability distribution with finite mean \(\mu_Y\) and finite variance
\(\sigma^2_Y\), i.e., \(0 < \sigma^2_Y < \infty\) and \(\overline{Y} =
  (1/n)\sum_i^nY_i\). Then
</p>

<p>
\[ \sqrt{n}(\overline{Y}-\mu_Y) \xrightarrow{\text{d}} N(0,
  \sigma^2_Y) \]
</p></li>

<li><p>
It follows that since \(\sigma_{\overline{Y}} =
  \sqrt{\mathrm{Var}(\overline{Y})} = \sigma_Y/\sqrt{n}\),
</p>

<p>
\[ \frac{\overline{Y} - \mu_Y}{\sigma_{\overline{Y}}}
  \xrightarrow{\text{ d}} N(0, 1) \]
</p></li>

</ul></li>

<li><a id="orgdb9d3fb"></a>The CLT illustrated&#xa0;&#xa0;&#xa0;<span class="tag"><span class="B_frame">B_frame</span></span><br  /><div id="orge540da0" class="figure">
<p><img src="figure/fig-2-9.png" alt="fig-2-9.png" width="600" />
</p>
<p><span class="figure-number">Figure 11: </span>An illustration of the central limit theorem</p>
</div></li></ul>

</section>
<section id="slide-org7e6e9df">
<h3 id="org7e6e9df">Illustrations with Wolfram CDF player</h3>
<ul>
<li>To view the following demonstrations,
first you need to download them by saving into your disk, then open
them with Wolfram CDF Player that can be downloaded from
<a href="http://www.wolfram.com/cdf-player/">http://www.wolfram.com/cdf-player/</a>.</li>

<li>Here is another demonstration of the law of large number,
<a href="IllustratingTheLawOfLargeNumbers.cdf">IllustratingTheLawOfLargeNumbers.cdf</a>.</li>

<li>Here is the demonstration of the CLT with Wolfram CDF Player,
<a href="IllustratingTheCentralLimitTheoremWithSumsOfBernoulliRandomV.cdf">IllustratingTheCentralLimitTheoremWithSumsOfBernoulliRandomV.cdf</a>.</li>

</ul>
</section>
</section>
</div>
</div>
<script src="file:////Users/ztian/reveal.js/lib/js/head.min.js"></script>
<script src="file:////Users/ztian/reveal.js/js/reveal.js"></script>

<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: 'c',
rollingLinks: false,
keyboard: true,
overview: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none
transitionSpeed: 'default',
multiplex: {
    secret: '', // null if client
    id: '', // id, obtained from socket.io server
    url: '' // Location of socket.io server
},

// Optional libraries used to extend on reveal.js
dependencies: [
 { src: 'file:////Users/ztian/reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
 { src: 'file:////Users/ztian/reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'file:////Users/ztian/reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
 { src: 'file:////Users/ztian/reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
 { src: 'file:////Users/ztian/reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }]
});
</script>
</body>
</html>
