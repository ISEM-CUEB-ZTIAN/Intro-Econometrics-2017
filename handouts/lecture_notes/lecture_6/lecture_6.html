<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-03-22 Wed 09:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Lecture 6: Linear Regression with One Regressor</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Zheng Tian" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../../css/readtheorg.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Lecture 6: Linear Regression with One Regressor</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgd3591f7">1. Introduction</a></li>
<li><a href="#org0a61ea6">2. The Linear Regression Model</a></li>
<li><a href="#orgee3167b">3. The OLS Estimation Method for a Linear Regression Model</a></li>
<li><a href="#org1a56e08">4. Algebraic Properties of the OLS Estimator</a></li>
<li><a href="#org5faa4e7">5. Measures of Fit</a></li>
<li><a href="#org62cbb36">6. The Least Squares Assumptions</a></li>
<li><a href="#org34ceeb6">7. Sampling Distribution of the OLS Estimators</a></li>
</ul>
</div>
</div>
\(
\DeclareMathOperator*{\plim}{plim}
\newcommand{\plimn}{\plim_{n \rightarrow \infty}}
\)


<div id="outline-container-orgd3591f7" class="outline-2">
<h2 id="orgd3591f7"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
This lecture introduces a linear regression model with one regressor
called a simple linear regression model. We will learn the ordinary
least squares (OLS) method to estimate a simple linear regression
model, discuss the algebraic and statistical properties of the OLS
estimator, introduce two measures of goodness of fit, and bring up three least
squares assumptions for a linear regression model. As an example, we
apply the OLS estimation method to a linear model of test scores and
class sizes in California school districts.
</p>

<p>
This lecture lays out foundations for all lectures to come. Although
in practice we seldom use a linear regression model with only one
regressor, the essential principles of the OLS estimation method and
hypothesis testing are the same for a linear regression model with
multiple regressors.
</p>
</div>
</div>


<div id="outline-container-org0a61ea6" class="outline-2">
<h2 id="org0a61ea6"><span class="section-number-2">2</span> The Linear Regression Model</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-org8fcd967" class="outline-3">
<h3 id="org8fcd967"><span class="section-number-3">2.1</span> What is regression?</h3>
<div class="outline-text-3" id="text-2-1">
</div><div id="outline-container-org4a9a23a" class="outline-4">
<h4 id="org4a9a23a">Definition of <b>regress</b> in Merriam-Webster's dictionary</h4>
<div class="outline-text-4" id="text-org4a9a23a">
<p>
Merriam-Webster gives the following definition of the word "regress":
</p>
<ol class="org-ol">
<li>An act or the privilege of going or coming back</li>
<li>Movement backward to a previous and especially worse or more
primitive state or condition</li>
<li>The act of reasoning backward</li>
</ol>
</div>
</div>

<div id="outline-container-org78038ad" class="outline-4">
<h4 id="org78038ad">The meaning of regression in statistics?</h4>
<div class="outline-text-4" id="text-org78038ad">
<p>
In statistical modeling, regression analysis is a statistical process
for estimating the relationships among variables.<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup> Specifically,
most regression analysis focus on the conditional mean of the
dependent variable given the independent variables, which is a
function of the values of independent variables.
</p>

<p>
A very simple functional form of a conditional expectation is a linear
function. That is, we can model the conditional mean as follows,
</p>
\begin{equation}
\label{eq:genpopreg}
\mathrm{E}(Y \mid X = x) = f(x) = \beta_{0} + \beta_1 x
\end{equation}
<p>
Equation \ref{eq:genpopreg} is called a <b>simple linear regression
function</b>.
</p>
</div>
</div>
</div>


<div id="outline-container-orga3cba26" class="outline-3">
<h3 id="orga3cba26"><span class="section-number-3">2.2</span> An example: Test scores versus class size</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Let's go back to the example of California school districts,
introduced in Lecture 1.
</p>
</div>

<div id="outline-container-org897b636" class="outline-4">
<h4 id="org897b636">Research question:</h4>
<div class="outline-text-4" id="text-org897b636">
<p>
The research question of this application is: Can reducing class size
increase students' test scores?
</p>
</div>
</div>

<div id="outline-container-org7918d19" class="outline-4">
<h4 id="org7918d19">How can we answer the question?</h4>
<div class="outline-text-4" id="text-org7918d19">
<ul class="org-ul">
<li>We randomly choose 42 students and divide them into two classes,
with one having 20 students and another having 22. And they are
taught with the same subject and by the same teachers.</li>

<li>Randomization ensures that it is the difference in class sizes of
the two classes that is the only factor affecting test scores.</li>

<li><p>
After a test for both classes, we then compute the average test
scores that can be expressed as,
</p>
\begin{gather*}
\mathrm{E}(TestScore | ClassSize = 20) \\
\mathrm{E}(TestScore | ClassSize = 22)
\end{gather*}</li>

<li><p>
Then the effect of class size on test scores is the difference in
the conditional means, i.e.,
</p>
\begin{equation*}
\mathrm{E}(TestScore | ClassSize = 20) - \mathrm{E}(TestScore | ClassSize = 22)
\end{equation*}</li>

<li>If the difference is large enough, we can say that reducing class
can improve students' test performance.</li>
</ul>
</div>
</div>

<div id="outline-container-org395c637" class="outline-4">
<h4 id="org395c637">A simple linear regression model of test scores v.s. class size</h4>
<div class="outline-text-4" id="text-org395c637">
<p>
As mentioned above, a simple linear regression function can be used to
describe the relationship between test scores and class sizes. Since
it regards the association between these two variable for the whole
population, we call this regression function as the <b>population
regression function</b> or the <b>population regression line</b>, taking the
following form,
</p>
\begin{equation}
\label{eq:popreg-testscore}
\mathrm{E}(TestScore | ClassSzie) = \beta_0 + \beta_1 ClassSize
\end{equation}

<p>
By calculating the conditional expectation, some other factors, apart
from class sizes, are left out of the population regression
function. Although these factors may also influence test scores, they are
either unimportant in your reasoning or unable to be measured. We can
lump all these factors into a single term, and set up a <b>simple linear
regression model</b> as follows,
</p>
\begin{equation}
\label{eq:regmodel-testscore}
TestScore = \beta_0 + \beta_1 ClassSize + OtherFactors
\end{equation}

<p>
If we assume \(\mathrm{E}(OtherFactors | ClassSize) = 0\), then the
simple linear regression model (Eq. \ref{eq:regmodel-testscore})
becomes the population regression line
(Eq. \ref{eq:popreg-testscore}).
</p>
</div>
</div>

<div id="outline-container-org8241d9d" class="outline-4">
<h4 id="org8241d9d">A distinction between the population regression function and the population regression model</h4>
<div class="outline-text-4" id="text-org8241d9d">
<p>
Note that here we have two concepts: the population regression
function and the population regression model. What's their difference?
Simply put,
</p>
<ul class="org-ul">
<li>A population regression function gives us a deterministic
relation between class size and the expectation of test scores. That
is, when we have a value of class size and know the values of
\(\beta_0\) and \(\beta_1\), there is one and only one
expected value of test scores associated with this class size.
However, we cannot compute the exact value of the test score of a
particular observation.</li>
<li>A population regression model, by including other factors, gives us
a complete description of a data generating process (DGP). That is,
when we have all the values of class sizes and other factors and
know \(\beta_0\) and \(\beta_1\), we can generate all the values of test
scores. Also, when we consider other factors as a random variable,
the association between test scores and class size is not
deterministic, depending on the value of other factors.</li>
</ul>
</div>
</div>

<div id="outline-container-org23f66d9" class="outline-4">
<h4 id="org23f66d9">An interpretation of the population regression model</h4>
<div class="outline-text-4" id="text-org23f66d9">
<p>
Now we have set up the simple linear regression model,
</p>
\begin{equation*}
TestScore = \beta_0 + \beta_1 ClassSize + OtherFactors
\end{equation*}
<p>
What is \(\beta_1\) and \(\beta_0\) represent in the model?
</p>
</div>

<ul class="org-ul"><li><a id="org794091f"></a>Interpret \(\beta_1\)<br  /><div class="outline-text-5" id="text-org794091f">
<p>
Let's first look at \(\beta_1\). When we hold other factors constant,
the only reason for a change in test scores is a change in class
size. Denote \(\Delta TestScore\) and \(\Delta ClassSize\) to
be their respective change. According to the above regression model,
holding other factors constant, we have
\[ \Delta TestScore = \beta_1 \Delta ClassSize  \]
where \(\beta_0\) is removed because it is also a constant. Then, we get
\[ \beta_1 = \frac{\Delta TestScore}{\Delta ClassSize} \]
That is, \(\beta_1\) measures the change in the test score resulting
from a <b>one-unit change</b> in the class size. When \(TestScore\) and
\(ClassSize\) are two continuous variable, we can write \(\beta_1\) as
\[\beta_1 = \frac{\mathrm{d} TestScore}{\mathrm{d} ClassSize}  \]
Hence, we often call \(\beta_1\) as the <b>marginal effect</b> of the class
size on the test score.
</p>

<p>
The phrase of "holding other factors constant" is important. Without
it, we cannot disentangle the effect of class sizes on test scores
from other factors. "Holding other things constant" is often expressed
as the notion of <b>ceteris paribus</b>.
</p>
</div></li>

<li><a id="org79217cd"></a>Interpret \(\beta_0\)<br  /><div class="outline-text-5" id="text-org79217cd">
<p>
\(\beta_0\) is the intercept in the model. Sometimes it bears real
meanings, but sometimes it merely presents as an intercept. In this
regression model, \(\beta_0\) is the test score when the class size and
other factors are all zero, which is obviously nonsensical. Thus,
\(\beta_0\) does not have a real meaning in this model, and it just
determines where the population regression line intersects the Y
axis.
</p>
</div></li></ul>
</div>
</div>


<div id="outline-container-orgf6dae71" class="outline-3">
<h3 id="orgf6dae71"><span class="section-number-3">2.3</span> The general linear regression model</h3>
<div class="outline-text-3" id="text-2-3">
<p>
Let's generalize test scores and class sizes to be two random
variables \(Y\) and \(X\). For both, there are \(n\) observations so that
each observation \(i = 1, 2, 3, \ldots\) is associated with a pair of
values of \((X_i, Y_i)\).
</p>

<p>
Then a <b>simple linear regression model</b> that associates \(Y\) with \(X\) is
</p>
\begin{equation}
\label{eq:single-regress}
Y_i = \beta_0 + \beta_1 X_i + u_i, \text{ for } i = 1, \ldots, n
\end{equation}

<ul class="org-ul">
<li>\(Y_i\) is called the dependent variable, the regressand, or the LHS
(left-hand side) variable.</li>
<li>\(X_i\) is called the independent variable, the regressor, or the RHS
(right-hand side) variable.</li>
<li>\(\beta_{0}\) is the intercept, or the constant term. It can either have
economic meaning or have merely mathematical sense, which determines
the level of the regression line, i.e., the point of intersection
with the Y axis.</li>
<li>\(\beta_{1}\) is the slope of the population regression line. Since
\(\beta_1 = \mathrm{d}Y_i/ \mathrm{d}X_i\), it is the marginal effect
of \(X\) on \(Y\). That is, holding other things constant, one unit
change in \(X\) will make \(Y\) change by \(\beta_1\) units.</li>
<li>\(u_i\) is the error term. \(u_i = Y_i - (\beta_0 + \beta_1 X_i)\)
incorporates all the other factors besides \(X\) that determine the
value of \(Y\).</li>
<li>\(\beta_{0} + \beta_{1}X_{i}\) represents the population regression
function(or the population regression line).</li>
</ul>
</div>
</div>


<div id="outline-container-org8140239" class="outline-3">
<h3 id="org8140239"><span class="section-number-3">2.4</span> An graphical illustration of a linear regression model</h3>
<div class="outline-text-3" id="text-2-4">
<p>
The relationship between the data points, the population regression
line, and the errors (other factors) are illustrated in Figure <a href="#org6577323">1</a>.
</p>


<div id="org6577323" class="figure">
<p><img src="figure/fig-4-1.png" alt="fig-4-1.png" width="600" />
</p>
<p><span class="figure-number">Figure 1: </span>The Population Regression Line</p>
</div>
</div>
</div>
</div>


<div id="outline-container-orgee3167b" class="outline-2">
<h2 id="orgee3167b"><span class="section-number-2">3</span> The OLS Estimation Method for a Linear Regression Model</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-org8869148" class="outline-3">
<h3 id="org8869148"><span class="section-number-3">3.1</span> The intuition for the OLS and minimization</h3>
<div class="outline-text-3" id="text-3-1">
<p>
The most commonly used method to estimate a linear regression model, like
Equation \ref{eq:single-regress}, is the ordinary least squares (OLS)
estimation.
</p>

<p>
Let's explain the basic idea of the OLS by dissecting its name.
</p>

<dl class="org-dl">
<dt>Ordinary</dt><dd>It means that the OLS estimator is a very basic method,
from which we may derive some variations of the OLS
estimator, such as the weighted least squares (WLS), and the
generalized least squares (GLS).</dd>

<dt>Least</dt><dd>It means that the OLS estimator tries to minimize
something. The "something" is the mistakes we
make when we try to guess (estimate) the values of the
parameters in the model. From Equation
\ref{eq:single-regress}, if our guess for \(\beta_0\) and
\(\beta_1\) is \(b_0\) and \(b_1\), then the mistake of our guess
is \(\hat{u}_{i} = Y_{i} - b_0 - b_1 X_i\).</dd>

<dt>Squares</dt><dd>It represent the actual thing (a quantity) that we
minimize. The OLS does not attempt to minimize each
\(\hat{u}_{i}\) but to minimize the sum of the squared
mistakes, \(\sum_{i=1}^n \hat{u}_i^2\). Taking square is to
avoid possible offsetting between positive and negative values of
\(\hat{u}_i\) in \(\sum_i \hat{u}_i\).</dd>
</dl>
</div>
</div>


<div id="outline-container-org2c554a3" class="outline-3">
<h3 id="org2c554a3"><span class="section-number-3">3.2</span> The OLS estimators for \(\beta_0\) and \(\beta_1\)</h3>
<div class="outline-text-3" id="text-3-2">
<p>
Let \(b_0\) and \(b_1\) be some estimators of \(\beta_0\) and \(\beta_1\),
respectively. <sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup> Then, the OLS estimator is the
solution to the following minimization problem.
</p>

\begin{equation}
\operatorname*{min}_{b_0, b_1}\: S(b_0, b_1) = \sum_{i=1}^n \hat{u}_i^2 = \sum_{i=1}^n (Y_i - b_0 - b_1 X_i)^2 \label{eq:min-ols}
\end{equation}

<p>
where \(S(b_0, b_1)\) is a function of \(b_0\) and \(b_1\), measuring the
sum of the squared prediction mistakes over all \(n\) observation.
</p>
</div>

<div id="outline-container-org86e9436" class="outline-4">
<h4 id="org86e9436">The mathematical derivation of the OLS estimators for \(\beta_0\) and \(\beta_1\)</h4>
<div class="outline-text-4" id="text-org86e9436">
<p>
We solve the problem by taking the derivative of \(S(b_0, b_1)\) with respect to \(b_0\) and \(b_1\),
respectively. Suppose \(b_0^*=\hat{\beta}_0\) and \(b^*_1=\hat{\beta}_1\) are the
solution to the minimization problem. Then the first order conditions
evaluated at \((\hat{\beta}_0, \hat{\beta}_1)\) are
</p>
\begin{align}
& \frac{\partial S}{\partial b_0}(\hat{\beta}_0, \hat{\beta}_1) = \sum_{i=1}^n (-2)(Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0  \label{eq:b-0} \\
& \frac{\partial S}{\partial b_1}(\hat{\beta}_0, \hat{\beta}_1) = \sum_{i=1}^n (-2)(Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) X_i = 0 \label{eq:b-1}
\end{align}

<p>
Rearranging Equation \ref{eq:b-0}, we get
</p>
\begin{gather}
\sum_{i=1}^n Y_i - n \hat{\beta}_0 - \hat{\beta}_1 \sum_{i=1}^n X_i = 0 \notag  \\
\hat{\beta}_0 = \frac{1}{n} \sum_{i=1}^n Y_i - \frac{\hat{\beta}_1}{n}\sum_{i=1}^n X_i = \overline{Y} - \hat{\beta}_1 \overline{X} \label{eq:bhat-0}
\end{gather}

<p>
Rearranging Equation \ref{eq:b-1} and plugging Equation \ref{eq:bhat-0}, we get
</p>
\begin{gather}
\sum_{i=1}^n X_i Y_i - \hat{\beta}_0 \sum_{i=1}^n X_i - \hat{\beta}_1 \sum_{i=1}^n X^2_i = 0  \notag \\
\sum_{i=1}^n X_i Y_i - \frac{1}{n}\sum_{i=1}^n X_i \sum_{i=1}^n Y_i + \hat{\beta}_1 \frac{1}{n} \left(\sum_{i=1}^n X_i\right)^2 - \hat{\beta}_1 \sum_{i=1}^n X_i^2 = 0 \notag \\
\hat{\beta}_1 = \frac{n\sum_{i=1}^n X_i Y_i - \sum_{i=1}^n X_i \sum_{i=1}^n Y_i}{n\sum_{i=1}^n X_i^2 - (\sum_{i=1}^n X_i)^2} \label{eq:bhat-1}
\end{gather}

<p>
For the numerator in Equation \ref{eq:bhat-1}, we can show the following
</p>
\begin{align*}
\sum_i(X_i - \overline{X})(Y_i - \overline{Y})
&= \sum_i X_iY_i - \overline{X}\sum_iY_i - \overline{Y}\sum_iX_i + \sum_i \overline{X}\overline{Y} \\
&= \sum_i X_iY_i - 2n\overline{X}\overline{Y} + n\overline{X}\overline{Y} \\
&= \sum_i X_iY_i - n\overline{X}\overline{Y} \\
&= \frac{1}{n} \left(n\sum_i X_iY_i - \sum_i X_i \sum_i Y_i\right)
\end{align*}

<p>
Similarly, we can show that \(\sum_i (X_i - \overline{X})^2 =
\frac{1}{n} \left[n\sum_i X_i^2 - (\sum_i X_i)^2\right]\).
</p>

\begin{equation*}
\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}
\end{equation*}

<p>
Since we know that the sample covariance of \(X\) and \(Y\) is \(s_{XY} =
\frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})\)
and the sample variance of \(X\) is \(s_X^2 = \frac{1}{n-1} \sum_{i=1}^n
(X_i - \overline{X})^2\), the equation above can also be written as
\[ \hat{\beta}_1 = \frac{s_{XY}}{s^2_X}  \]
</p>

<p>
In sum, solving the minimization problem (Equation
\ref{eq:min-ols}), we obtain the OLS estimators for \(\beta_0\) and
\(\beta_1\) as
</p>
\begin{align}
\hat{\beta}_1 & = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2} = \frac{s_{XY}}{s^2_X}  \label{eq:betahat-1} \\
\hat{\beta}_0 & = \overline{Y} - \hat{\beta}_1 \overline{X}  \label{eq:betahat-0}
\end{align}
</div>
</div>
</div>


<div id="outline-container-org17ade48" class="outline-3">
<h3 id="org17ade48"><span class="section-number-3">3.3</span> The predicted values, residuals, and the sample regression line</h3>
<div class="outline-text-3" id="text-3-3">
</div><div id="outline-container-org7f7d420" class="outline-4">
<h4 id="org7f7d420">The predicted values</h4>
<div class="outline-text-4" id="text-org7f7d420">
<ul class="org-ul">
<li>After obtaining the estimators, we can compute the <b>predicted values</b>
\(\hat{Y}_i\) for \(i=1, \ldots, n\)
\[\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i\]</li>

<li>The line represented by the above equation is called <b>the sample
regression line</b>.</li>

<li>The sample average point \((\overline{X}, \overline{Y})\) is
always on the sample regression line because, from Equation
\ref{eq:betahat-0}, we have
\[ \overline{Y} = \hat{\beta}_0 + \hat{\beta}_1 \overline{X} \]</li>
</ul>
</div>
</div>

<div id="outline-container-org0ea0e72" class="outline-4">
<h4 id="org0ea0e72">The residuals</h4>
<div class="outline-text-4" id="text-org0ea0e72">
<ul class="org-ul">
<li>The <b>residuals</b> \(\hat{u}_i\) for \(i = 1, \ldots, n\) are
\[\hat{u}_i = Y_i - \hat{Y}_i\]</li>

<li>The residuals are the difference between the observed values of
\(Y_i\) and its predicted value. That is, they are the actual
prediction errors we make when using the OLS estimators.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org75ef813" class="outline-3">
<h3 id="org75ef813"><span class="section-number-3">3.4</span> A comparison between the population regression model and the sample counterparts</h3>
<div class="outline-text-3" id="text-3-4">
<p>
We should pause here to make a clear distinction between the
population regression function and model and their counterparts.
</p>
</div>

<div id="outline-container-orgc410a45" class="outline-4">
<h4 id="orgc410a45">The population regression function versus the sample regression function</h4>
<div class="outline-text-4" id="text-orgc410a45">
<ul class="org-ul">
<li>The population regression function is a function between the
conditional mean of \(Y\) given \(X\) and \(X\), that is,
\[ E(Y \mid X) = \beta_0 + \beta_1 X_i \]
where \(\beta_0\) and \(\beta_1\) are the population parameters.</li>
<li>The sample regression function is a function between the predicted
value and \(X\), that is,
\[ \hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i \]</li>
</ul>
</div>
</div>

<div id="outline-container-org49fda76" class="outline-4">
<h4 id="org49fda76">The regression errors versus residuals</h4>
<div class="outline-text-4" id="text-org49fda76">
<ul class="org-ul">
<li>The error term, \(u_i\), in the population regression model represents the
other factors that the population regression function does not take
into account. It is the difference between \(Y_i\) and \(E(Y_i \mid
  X_i)\). Thus, we have
\[Y_i = \beta_0 + \beta_1 X_i + u_i \]</li>
<li>The residuals, \(\hat{u}_i\), represent the actual mistakes we make with a set of
estimators. It is the difference between \(Y_i\) and its predicted
value \(\hat{Y}_i\). Thus, we have
\[Y_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{u}_{i}\]</li>
</ul>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> A comparison between the population regression and its sample counterparts</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">Population</th>
<th scope="col" class="org-left">Sample</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Regression functions</td>
<td class="org-left">\(\beta_{0} + \beta_{1}X_{i}\)</td>
<td class="org-left">\(\hat{\beta}_0 + \hat{\beta}_1 X_i\)</td>
</tr>

<tr>
<td class="org-left">Parameters</td>
<td class="org-left">\(\beta_{0}\), \(\beta_{1}\)</td>
<td class="org-left">\(\hat{\beta}_{0}\), \(\hat{\beta}_{1}\)</td>
</tr>

<tr>
<td class="org-left">Errors vs residuals</td>
<td class="org-left">\(u_{i}\)</td>
<td class="org-left">\(\hat{u}_{i}\)</td>
</tr>

<tr>
<td class="org-left">The regression model</td>
<td class="org-left">\(Y_i = \beta_0 + \beta_1 X_i + u_i\)</td>
<td class="org-left">\(Y_i = \hat{\beta}_0 + \hat{\beta}_1 X_i + \hat{u}_{i}\)</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>



<div id="outline-container-org3caefcf" class="outline-3">
<h3 id="org3caefcf"><span class="section-number-3">3.5</span> The OLS estimates of the relationship between test scores and the student-teacher ratio</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Let's come back to the application of test scores versus the
student-teacher ratios in California school districts. The goal is to
estimate the effect of class sizes, measured by the student-teacher
ratios, on test scores. Before setting up a formal regression model,
it is always a good practice to glance over the data using some
exploratory data analysis techniques.
</p>
</div>

<div id="outline-container-orgd2aaf10" class="outline-4">
<h4 id="orgd2aaf10">Exploratory analysis</h4>
<div class="outline-text-4" id="text-orgd2aaf10">
</div><ul class="org-ul"><li><a id="org32919ce"></a>Basic summary statistics<br  /><div class="outline-text-5" id="text-org32919ce">
<p>
We first need to compute basic summary statistics to see the sample
distribution of the data. Some commonly used summary statistics
include mean, standard deviation, median, minimum, maximum, and
quantile (percentile), etc. Table <a href="#orgc7bb907">2</a> summarizes the
distribution of test scores and class sizes for the sample.
</p>

<table id="orgc7bb907" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> Summary Of distributions of student-teacher ratios and test scores</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">Average</th>
<th scope="col" class="org-right">S.t.d.</th>
<th scope="col" class="org-right">10%</th>
<th scope="col" class="org-right">25%</th>
<th scope="col" class="org-right">40%</th>
<th scope="col" class="org-right">50%</th>
<th scope="col" class="org-right">60%</th>
<th scope="col" class="org-right">75%</th>
<th scope="col" class="org-right">90%</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left"><i>TestScore</i></td>
<td class="org-right">654.16</td>
<td class="org-right">19.05</td>
<td class="org-right">630.4</td>
<td class="org-right">640.05</td>
<td class="org-right">649.07</td>
<td class="org-right">654.45</td>
<td class="org-right">659.4</td>
<td class="org-right">666.66</td>
<td class="org-right">678.86</td>
</tr>

<tr>
<td class="org-left"><i>STR</i></td>
<td class="org-right">19.64</td>
<td class="org-right">1.89</td>
<td class="org-right">17.35</td>
<td class="org-right">18.58</td>
<td class="org-right">19.27</td>
<td class="org-right">19.72</td>
<td class="org-right">20.08</td>
<td class="org-right">20.87</td>
<td class="org-right">21.87</td>
</tr>
</tbody>
</table>
</div></li>

<li><a id="org756e799"></a>Scatterplot<br  /><div class="outline-text-5" id="text-org756e799">
<p>
A scatterplot visualizes the relationship between two variables
straightforwardly, which is helpful for us to decide what a functional
form a regression model should properly take. Figure <a href="#orge7aa67d">2</a>
shows that test scores and student-teacher ratios may be negatively
related. The correlation coefficient between the two variables is
-0.23, verifying the existence of a weak negative relationship.
</p>


<div id="orge7aa67d" class="figure">
<p><img src="figure/fig-4-2.png" alt="fig-4-2.png" width="600" />
</p>
<p><span class="figure-number">Figure 2: </span>The scatterplot between student-teacher ratios and test scores</p>
</div>
</div></li></ul>
</div>

<div id="outline-container-orgd76b72c" class="outline-4">
<h4 id="orgd76b72c">Regression analysis</h4>
<div class="outline-text-4" id="text-orgd76b72c">
<p>
After exploratory analysis, we can estimate the linear model. Although
the formula of computing \(\beta_1\) and \(\beta_0\) (Equations
\ref{eq:betahat-1} and \ref{eq:betahat-0}) seems complicated, the
practical estimation procedure is simplified by using computer
software, like R. For now, let's simply present the estimation
results in the following equation,
</p>

\begin{equation}
\label{eq:testscr-str-1e}
\widehat{TestScore} = 698.93 - 2.28 \times STR
\end{equation}

<p>
We can draw the sample regression line represented by Equation
\ref{eq:testscr-str-1e} in the scatterplot to eyeball how well the
regression model fits the data.
</p>


<div class="figure">
<p><img src="figure/fig-4-3.png" alt="fig-4-3.png" width="600" />
</p>
<p><span class="figure-number">Figure 3: </span>The estimated regression line for the California data</p>
</div>
</div>
</div>

<div id="outline-container-org0dfaa9b" class="outline-4">
<h4 id="org0dfaa9b">Interpretation of the estimated coefficients</h4>
<div class="outline-text-4" id="text-org0dfaa9b">
<p>
Upon obtaining the coefficient estimates, what we need to do next
includes hypothesis tests, model specification tests, robustness (or
sensitivity) test, and interpretation. Let's first see how to
correctly interpret the estimation results.
</p>

<ul class="org-ul">
<li>Our main interest is in the slope that tell us how much a unit
change in student-teacher ratios will cause test scores to
change. The slope of -2.28 means that an increase in the
student-teacher ratio by one student per class is, on average,
associated with a decline in district-wide test scores by 2.28
points on the test.</li>
<li>The intercept literally means that if the student-teacher ratio is
zero, the average district-wide test scores will be 698.9. However,
it is nonsense for having some positive test scores when the
student-teacher ratio is zero. Therefore, the intercept term in this
case merely serves as determining the level of the sample regression
line.</li>
<li>The mere number of -2.28 really does not make much sense for the
readers of your research. We have to put it into the context of
California school district to avoid ridiculous results even though
the estimation itself is correct. (Read the discussion in the
paragraphs in Page 117.)</li>
</ul>
</div>
</div>
</div>
</div>


<div id="outline-container-org1a56e08" class="outline-2">
<h2 id="org1a56e08"><span class="section-number-2">4</span> Algebraic Properties of the OLS Estimator</h2>
<div class="outline-text-2" id="text-4">
<p>
The OLS estimator has many good properties. Let's first look at some
of its algebraic properties. That is, these properties are the results
of the minimization problem in Equation \eqref{eq:min-ols}, regardless of any
statistical assumptions we will introduce in the next sections.
</p>
</div>

<div id="outline-container-org9003607" class="outline-3">
<h3 id="org9003607"><span class="section-number-3">4.1</span> TSS, ESS, and SSR</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>From \(Y_i = \hat{Y}_i + \hat{u}_i\), we can define
<ul class="org-ul">
<li><b>The total sum of squares</b>: \(TSS = \sum_{i=1}^n (Y_i - \overline{Y})^2\)</li>
<li><b>The explained sum of squares</b>: \(ESS = \sum_{i=1}^n (\hat{Y}_i - \overline{Y})^2\)</li>
<li><b>The sum of squared residuals</b>: \(SSR = \sum_{i=1}^n (Y_i -
    \hat{Y}_i)^2 = \sum_{i=1}^n \hat{u}_i^2\)</li>
</ul></li>
</ul>

<p>
Note that TSS, ESS, and SSR all take the form of "deviation from
the mean". This form is only valid when an intercept is included in the
regression model.<sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>
</p>
</div>
</div>


<div id="outline-container-org55728b2" class="outline-3">
<h3 id="org55728b2"><span class="section-number-3">4.2</span> Some algebraic properties among \(\hat{u}_i\), \(\hat{Y}_i\), \(Y_i\), and \(X_i\)</h3>
<div class="outline-text-3" id="text-4-2">
<p>
The OLS residuals and the predicted values satisfy the following
equations:<sup><a id="fnr.4" class="footref" href="#fn.4">4</a></sup>
</p>

\begin{gather}
\sum_{i=1}^n \hat{u}_i = 0 \label{eq:algebra-ols-1} \\
\frac{1}{n} \sum_{i=1}^n \hat{Y}_i = \overline{Y} \label{eq:algebra-ols-2} \\
\sum_{i=1}^n \hat{u}_i X_i = 0 \label{eq:algebra-ols-3} \\
TSS = ESS + SSR \label{eq:tss-ess}
\end{gather}
</div>
</div>


<div id="outline-container-org6ce0ac8" class="outline-3">
<h3 id="org6ce0ac8"><span class="section-number-3">4.3</span> The proof of these properties</h3>
<div class="outline-text-3" id="text-4-3">
<p>
Here, I just prove Equation \ref{eq:tss-ess}. The proofs for the other
equations above are in Appendix 4.3 in the textbook.
</p>
</div>

<div id="outline-container-org73d2752" class="outline-4">
<h4 id="org73d2752">Proof of Equation \ref{eq:algebra-ols-1}</h4>
<div class="outline-text-4" id="text-org73d2752">
<p>
From Equation \ref{eq:betahat-0} we can write the OLS residuals as
\[\hat{u}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i = (Y_i -
\overline{Y}) - \hat{\beta}_1 (X_i - \overline{X})\]
Thus
</p>
\begin{equation*}
\sum_{i=1}^n \hat{u}_i = \sum_{i=1}^n (Y_i - \overline{Y}) - \hat{\beta}_1 \sum_{i=1}^n (X_i - \overline{X})
\end{equation*}

<p>
By definition of the sample average, we have
\[\sum_{i=1}^n (Y_i - \overline{Y})=0 \text{ and } \sum_{i=1}^n (X_i - \overline{X})=0\]
It follows that \(\sum_{i=1}^n \hat{u}_i = 0\).
</p>
</div>
</div>

<div id="outline-container-orgd398cda" class="outline-4">
<h4 id="orgd398cda">Proof of Equation \ref{eq:algebra-ols-2}</h4>
<div class="outline-text-4" id="text-orgd398cda">
<p>
Note that \(Y_i = \hat{Y}_i + \hat{u}_i\). So
\[\sum_{i=1}^n Y_i =
\sum_{i=1}^n \hat{Y}_i + \sum_{i=1}^n \hat{u}_i = \sum_{i=1}^n
\hat{Y}_i\]
It follows that \(\overline{\hat{Y}} = (1/n)\sum_{i=1}^n \hat{Y}_i = \overline{Y}\).
</p>
</div>
</div>

<div id="outline-container-orgf86f1d3" class="outline-4">
<h4 id="orgf86f1d3">Proof of Equation \ref{eq:algebra-ols-3}</h4>
<div class="outline-text-4" id="text-orgf86f1d3">
<p>
\(\sum_{i=1}^n \hat{u}_i = 0\) implies that
</p>

\begin{align*}
& \sum_{i=1}^n \hat{u}_i X_i \\
=& \sum_{i=1}^n \hat{u}_i (X_i - \overline{X}) \\
=& \sum_{i=1}^n \left[ (Y_i - \overline{Y}) - \hat{\beta}_1 (X_i - \overline{X}) \right] (X_i - \overline{X}) \\
=& \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) - \hat{\beta}_1 \sum_{i=1}^n (X_i -\overline{X})^2 = 0
\end{align*}
</div>
</div>

<div id="outline-container-orgf7dc437" class="outline-4">
<h4 id="orgf7dc437">Proof of \(TSS = ESS + SSR\)</h4>
<div class="outline-text-4" id="text-orgf7dc437">
\begin{equation*}
\begin{split}
TSS &= \sum_{i=1}^n (Y_i - \overline{Y})^2 = \sum_{i=1}^n (Y_i - \hat{Y}_i + \hat{Y}_i - \overline{Y})^2 \\
&= \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n (\hat{Y}_i - \overline{Y})^2 + 2\sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \overline{Y}) \\
&= SSR + ESS + 2\sum_{i=1}^n \hat{u}_i \hat{Y}_i \\
&= SSR + ESS + 2\sum_{i=1}^n \hat{u}_i(\hat{\beta}_0 + \hat{\beta}_1 X_i) \\
&= SSR + ESS + 2(\hat{\beta}_0 \sum_{i=1}^n \hat{u}_i + \hat{\beta}_1\sum_{i=1}^n \hat{u}_i X_i) \\
&= SSR + ESS
\end{split}
\end{equation*}
<p>
where the final equality follows from Equations \ref{eq:algebra-ols-1} and \ref{eq:algebra-ols-3}.
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org5faa4e7" class="outline-2">
<h2 id="org5faa4e7"><span class="section-number-2">5</span> Measures of Fit</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-orgf21840e" class="outline-3">
<h3 id="orgf21840e"><span class="section-number-3">5.1</span> Goodness of Fit: R<sup>2</sup></h3>
<div class="outline-text-3" id="text-5-1">
<p>
\(R^{2}\) is one of the commonly used measures for how well the OLS
regression line fits the data. \(R^{2}\) is the fraction of the sample
variance of \(Y_i\) explained by \(X_i\). The sample variance can be
represented with \(TSS\) and the part of sample variance explained by \(X\)
can be represented by \(ESS\). Therefore, mathematically, we can define
\(R^{2}\) as
</p>

\begin{equation}
\label{eq:rsquared}
R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}
\end{equation}

<p>
\(R^2\) is often called the coefficient of determination. It indicates
the proportion of the variance in the dependent variable that is
predictable from the independent variable(s).
</p>
</div>

<div id="outline-container-org1a83fdf" class="outline-4">
<h4 id="org1a83fdf">Properties of R<sup>2</sup></h4>
<div class="outline-text-4" id="text-org1a83fdf">
</div><ul class="org-ul"><li><a id="orgc34884e"></a>\(R^2 \in [0, 1]\)<br  /><div class="outline-text-5" id="text-orgc34884e">
<p>
\(R^2 = 0\) when \(\hat{\beta}_1 = 0\), that is, \(X\) cannot explain the
variance in \(Y\).
</p>
\begin{equation*}
\hat{\beta}_1 = 0 \Rightarrow Y_i = \hat{\beta}_0 + \hat{u}_i
\Rightarrow \hat{Y}_i = \overline{Y} = \hat{\beta}_0 \Rightarrow ESS
= \sum_i^n (\hat{Y}_i - \overline{Y})^2 = 0 \Rightarrow R^2 = 0
\end{equation*}
<p>
\(R^2 = 1\) when \(\hat{u}_i = 0\) for all \(i = 1, \ldots, n\), that is,
the regression line fits all the sample data perfectly.
\[ \hat{u}_i = 0 \Rightarrow SSR = \sum_i^n \hat{u}_i^2 = 0
  \Rightarrow R^2 = 1 \]
</p>
</div></li>

<li><a id="org0ccdcfd"></a>\(R^2 = r^2_{XY}\)<br  /><div class="outline-text-5" id="text-org0ccdcfd">
<p>
\(r_{XY}\) is the sample correlation coefficient, that is,
\[ r_{XY} = \frac{S_{XY}}{S_X S_Y} = \frac{\sum_i^n(X_i -
  \overline{X})(Y_i - \overline{Y})}{\left[\sum_i^n (X_i - \overline{X})^2 \sum_i^n (Y_i -
  \overline{Y})^2 \right]^{1/2}} \]
</p>

<p>
To prove \(R^2 = r^2_{XY}\), let's look at \(SSR\).
</p>

\begin{align*}
SSR &= \sum_{i=1}^n (\hat{Y}_i - \overline{Y})^2 = \sum_{i=1}^n (\hat{\beta}_0 + \hat{\beta}_1 X_i - \overline{Y})^2 \\
&= \sum_{i=1}^n (\overline{Y} - \hat{\beta}_1 \overline{X} + \hat{\beta}_1 X_i - \overline{Y})^2 \\
&= \sum_{i=1}^n \left[ \hat{\beta}_1 (X_i - \overline{X}) \right]^2 = \hat{\beta}_1^2 \sum_{i=1}^n (X_i - \overline{X})^2 \\
&= \left[\frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}\right]^2 \sum_{i=1}^n (X_i - \overline{X})^2 \\
&= \frac{\left[ \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) \right]^2}{\sum_{i=1}^n (X_i - \overline{X})^2}
\end{align*}

<p>
It follows that
\[
  R^2 = \frac{SSR}{TSS} = \frac{\left[ \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) \right]^2}{\sum_{i=1}^n (X_i - \overline{X})^2 \sum_{i=1}^n (Y_i - \overline{Y})^2} = r^2_{XY}
  \]
</p>

<p>
<i>Note</i>: This property holds only for the linear regression model
with <b>one regressor and an intercept</b>.
</p>
</div></li></ul>
</div>

<div id="outline-container-orgf8cf545" class="outline-4">
<h4 id="orgf8cf545">The use of \(R^2\)</h4>
<div class="outline-text-4" id="text-orgf8cf545">
<ul class="org-ul">
<li>\(R^2\) is usually the first statistics that we look at for judging
how well the regression model fits the data.</li>
<li>Most computer programs for econometrics and statistics report \(R^2\)
in their estimation results.</li>
<li>However, we cannot merely rely on \(R^2\) for judge whether the
regression model is "good" or "bad". For that, we have to use some
statistics that will be taught soon.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org1f23dd5" class="outline-3">
<h3 id="org1f23dd5"><span class="section-number-3">5.2</span> The standard error of regression (SER) as a measure of fit</h3>
<div class="outline-text-3" id="text-5-2">
<p>
Like \(R^2\), the standard error of regression (SER) is another measure
of fit for the OLS regression.
</p>

\begin{equation}
\label{eq:ser}
\mathrm{SER} = \sqrt{\frac{1}{n-2}\sum^n_{i=1} \hat{u}_i^2} = s
\end{equation}

<ul class="org-ul">
<li>SER has the same unit of \(u_i\), which are the unit of \(Y_i\).</li>
<li>SER measures the average “size” of the OLS residual (the average “mistake” made by the OLS regression line).</li>
<li>The root mean squared error (RMSE) is closely related to the SER:
\[ \mathrm{RMSE} = \sqrt{\frac{1}{n}\sum^n_{i=2} \hat{u}_i^2} \]
As \(n \rightarrow \infty\), \(SER = RMSE\).</li>
</ul>
</div>
</div>


<div id="outline-container-org1dc9e02" class="outline-3">
<h3 id="org1dc9e02"><span class="section-number-3">5.3</span> \(R^2\) and SER for the application of test scores v.s. class sizes</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li>In the application of test scores v.s. class sizes, \(R^2\) is 0.051
or 5.1%, which implies that the regressor <i>STR</i> explains only 5.1%
of the variance of the dependent variable <i>TestScore</i>.</li>
<li>SER is 18.6, which means that standard deviation of the regression
residuals is 18.6 points on the test. The magnitude of SER is so
large that, in another way, shows that the simple linear regression
model does not fit the data well.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org62cbb36" class="outline-2">
<h2 id="org62cbb36"><span class="section-number-2">6</span> The Least Squares Assumptions</h2>
<div class="outline-text-2" id="text-6">
<p>
The last two sections regard the algebraic properties of the OLS
estimators. Now let's turn to their statistical properties, which are
built on the following assumptions.
</p>
</div>

<div id="outline-container-org6cf9fcb" class="outline-3">
<h3 id="org6cf9fcb"><span class="section-number-3">6.1</span> Assumption 1: The conditional mean of \(u_i\) given \(X_i\) is zero</h3>
<div class="outline-text-3" id="text-6-1">
\begin{equation}
\label{eq:Eu}
E(u_i | X_i) = 0
\end{equation}

<p>
If Equation \ref{eq:Eu} is satisfied, then \(X_i\) is called
<b>exogenous</b>. This assumption can be stated a little stronger as \(E(u|X=x) = 0\)
for any value \(x\), that is \(E(u_i | X_1, \ldots, X_n) = 0\).
</p>

<p>
Since \(E(u|X=x)=0\), it follows that \(E(u)=E(E(u|X))=E(0)=0\). The
unconditional mean of \(u\) is also zero.
</p>

<ul class="org-ul">
<li><p>
A benchmark for thinking about this assumption is to consider an
ideal randomized controlled experiment.
</p>

<p>
Because \(X\) is assigned randomly, all other individual characteristics –
the things that make up \(u\) – are distributed independently of \(X\), so \(u\)
and \(X\) are independent. Thus, in an ideal randomized controlled
experiment, \(E(u|X = x) = 0\).
</p></li>

<li>In actual experiments, or with observational data, we will need to
think hard about whether \(E(u|X = x) = 0\) holds.</li>
</ul>

<p>
Assumption 1 can be illustrated by Figure <a href="#orgfc965ef">4</a>. The conditional
mean, \(E(Y \mid X)\), of the conditional density distribution, \(f(y
\mid x)\), is vertically projected right on the population regression
line \(\beta_0 + \beta_1 X\) because \(E(Y\mid X) = \beta_0 + \beta_1 X +
E(u \mid X) = \beta_0 + \beta_1 X\).
</p>


<div id="orgfc965ef" class="figure">
<p><img src="figure/fig-4-4.png" alt="fig-4-4.png" width="600" />
</p>
<p><span class="figure-number">Figure 4: </span>An illustration of \(E(u|X=x)=0\)</p>
</div>
</div>

<ul class="org-ul"><li><a id="org522380b"></a>Correlation and conditional mean<br  /><div class="outline-text-5" id="text-org522380b">
<p>
\[ E(u_i | X_i) = 0 \Rightarrow \mathrm{Cov}(u_i, X_i) = 0 \]
</p>

<p>
That is, the zero conditional mean of \(u_i\) given \(X_i\) means that
they are uncorrelated.
</p>

\begin{equation*}
\begin{split}
\mathrm{Cov}(u_i, X_i) &= E(u_i X_i) - E(u_i) E(X_i) \\
&= E(X_i E(u_i|X_i)) - 0 \cdot E(X_i) \\
&= 0
\end{split}
\end{equation*}
<p>
where the law of iterated expectation is used twice at the second equality.
</p>

<p>
It follows that \(\mathrm{Cov}(u_i, X_i) \neq 0 \Rightarrow E(u_i|X_i) \neq 0\).
</p>
</div></li></ul>
</div>


<div id="outline-container-org8cef8cc" class="outline-3">
<h3 id="org8cef8cc"><span class="section-number-3">6.2</span> Assumption 2: \((X_i, Y_i)\) for \(i = 1, \ldots, n\) are i.i.d.</h3>
<div class="outline-text-3" id="text-6-2">
<ul class="org-ul">
<li>Each pair of \(X\) and \(Y\), i.e., \((X_i, Y_i)\) for \(i=1, \ldots, n\), is
selected randomly from the same joint distribution of \(X\) and \(Y\).</li>
<li>Since \(u_i = Y_i - \beta_0 - \beta_1 X_i\), \(u_{i}\) is i.i.d., too.</li>
<li>The cases that may violate of the i.i.d. assumption:
<ul class="org-ul">
<li>Time series data, \(\mathrm{Cov}(Y_t, Y_{t-1}) \neq 0\). That is, when
we try to regress \(Y_t\) on \(Y_{t-1}\), and if the current value
\(Y_t\) depends on \(Y_{t-1}\), which is very likely, the independence
is violated. We call this violation as serial correlation.</li>
<li>Spatial data, \(\mathrm{Cov}(Y_r, Y_s) \neq 0\), where \(s\) and \(r\)
refer to two neighboring regions. That is, when we try to regress
\(Y_r\) on \(Y_s\), they may well be correlated because they are
adjacent. We call this violation as spatial correlation.</li>
</ul></li>
</ul>
</div>
</div>


<div id="outline-container-org4f780f7" class="outline-3">
<h3 id="org4f780f7"><span class="section-number-3">6.3</span> Assumption 3: large outliers are unlikely</h3>
<div class="outline-text-3" id="text-6-3">
</div><div id="outline-container-org701f64a" class="outline-4">
<h4 id="org701f64a">\(0 < E(X^4_i) < \infty \text{ and } 0 < E(Y_i^4) < \infty\)</h4>
<div class="outline-text-4" id="text-org701f64a">
<ul class="org-ul">
<li>A large outlier is an extreme value of \(X\) or \(Y\).</li>
<li>On a technical level, if \(X\) and \(Y\) are bounded, then they have finite
fourth moments, i.e., finite kurtosis.</li>
<li>The essence of this assumption is to say that a large outlier can
strongly influence the results. So we need to rule out large
outliers in estimation.</li>
</ul>
</div>
</div>

<div id="outline-container-org161af18" class="outline-4">
<h4 id="org161af18">The influential observations and the leverage effects</h4>
<div class="outline-text-4" id="text-org161af18">
<ul class="org-ul">
<li><p>
An outlier can be detected by a scatterplot. See Figure <a href="#org19c3136">5</a>.
</p>


<div id="org19c3136" class="figure">
<p><img src="figure/fig-4-5.png" alt="fig-4-5.png" width="600" />
</p>
<p><span class="figure-number">Figure 5: </span>How an outlier can influence the OLS estimates</p>
</div></li>

<li>There are also formal tests for the existence of the influential
observations, some of which are coded in econometric software, like
R and Stata.</li>
</ul>
</div>
</div>
</div>
</div>


<div id="outline-container-org34ceeb6" class="outline-2">
<h2 id="org34ceeb6"><span class="section-number-2">7</span> Sampling Distribution of the OLS Estimators</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-org0f13a2a" class="outline-3">
<h3 id="org0f13a2a"><span class="section-number-3">7.1</span> Unbiasedness and consistency</h3>
<div class="outline-text-3" id="text-7-1">
</div><div id="outline-container-orgdc83c6f" class="outline-4">
<h4 id="orgdc83c6f">The unbiasedness of \(\hat{\beta}_0\) and \(\hat{\beta}_1\)</h4>
<div class="outline-text-4" id="text-orgdc83c6f">
</div><ul class="org-ul"><li><a id="org2ae4c1d"></a>The randomness of \(\hat{\beta}_0\) and \(\hat{\beta}_1\)<br  /><div class="outline-text-5" id="text-org2ae4c1d">
<p>
Since \((X_i, Y_i)\) for \(i = 1, \ldots, n\) are randomly drawn from a
population, different draws can render different estimates, giving
rise to the randomness of \(\hat{\beta}_0\) and \(\hat{\beta}_1\).
</p>
</div></li>

<li><a id="org04d4325"></a>The unbiasedness of \(\hat{\beta}_0\) and \(\hat{\beta}_1\)<br  /><div class="outline-text-5" id="text-org04d4325">
<p>
Let the true values of the intercept and the slope be \(\beta_0\) and \(\beta_1\). Based on the least squares assumption #1: \(E(u_i|X_i) = 0\)
\[ E(\hat{\beta}_0) = \beta_0 \text{ and } E(\hat{\beta}_1) =
  \beta_1 \]
</p>
</div></li>

<li><a id="org847e3b9"></a>Show that \(\hat{\beta}_1\) is unbiased<br  /><div class="outline-text-5" id="text-org847e3b9">
<p>
Let's rewrite the formula of \(\hat{\beta}_1\) here
</p>
\begin{equation}
\label{eq:betahat-1a}
\hat{\beta}_1  = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}
\end{equation}

<p>
Given the random samples \((X_i, Y_i)\) for \(i=1, \ldots, n\), from
\(Y_i = \beta_0 + \beta_1 X_i + u_i\), We know that \(\overline{Y} =
  \beta_0 + \beta_1 \overline{X} + \bar{u}\). It follows that \(Y_i -
  \overline{Y} = \beta_1 (X_i - \overline{X}) + u_i - \overline{u}\),
Plugging it in the numerator in Equation (\ref{eq:betahat-1a}). Then,
</p>
\begin{equation*}
\begin{split}
\sum_i (X_i - \overline{X})(Y_i - \overline{Y}) &= \sum_i (X_i - \overline{X})\left[\beta_1(X_i - \overline{X}) + (u_i - \overline{u}) \right] \\
&= \beta_1 \sum_i(X_i - \overline{X})^2 + \sum_i (X_i - \overline{X})u_i - \overline{u}\sum_i (X_i - \overline{X}) \\
&= \beta_1 \sum_i(X_i - \overline{X})^2 + \sum_i (X_i - \overline{X})u_i
\end{split}
\end{equation*}

<p>
In the second equality, we use the fact that \(\sum_i (X_i -
  \overline{X}) = 0\). Note that although we know from the first OLS
assumption, \(E(u_i) = 0\), we cannot guarantee that \(\bar{u} = 0\)
since \(u_1, \ldots, u_n\) are simply random draws of \(u_i\). 
</p>

<p>
Substituting this expression in Equation (\ref{eq:betahat-1a}) yields
</p>

\begin{equation}
\label{eq:betahat-1b}
\hat{\beta}_1 = \beta_1 + \frac{\frac{1}{n}\sum_i (X_i - \overline{X})u_i}{\frac{1}{n}\sum_i (X_i - \overline{X})^2}
\end{equation}

<p>
We prove that \(\hat{\beta}_1\) is conditionally unbiased, from which
the unconditional unbiasedness follows naturally.
</p>
\begin{equation*}
\begin{split}
E(\hat{\beta}_1 | X_1, \ldots, X_n) &= \beta_1 + E\left\lbrace \left[\frac{\frac{1}{n}\sum_i (X_i - \overline{X})u_i}{\frac{1}{n}\sum_i (X_i - \overline{X})^2} \right] \mid X_1, \ldots, X_n \right\rbrace \\
&= \beta_1 + \frac{\frac{1}{n}\sum_i (X_i - \overline{X})E(u_i|X_1, \ldots, X_n)}{\frac{1}{n}\sum_i (X_i - \overline{X})^2} \\
&= \beta_1\: \text{ (by assumption 1)}
\end{split}
\end{equation*}

<p>
It follows that \[E(\hat{\beta}_1) = E(E(\hat{\beta}_1 | X_1, \ldots, X_n)) = \beta_1\]
</p>

<p>
Therefore, \(\hat{\beta}_1\) is an unbiased estimator of \(\beta_1\).
</p>

<p>
The proof of unbiasedness of \(\hat{\beta}_0\) is left for exercise.
</p>
</div></li></ul>
</div>


<div id="outline-container-org4cc2329" class="outline-4">
<h4 id="org4cc2329">The consistency of \(\hat{\beta}_0\) and \(\hat{\beta}_1\)</h4>
<div class="outline-text-4" id="text-org4cc2329">
<p>
\(\hat{\beta}\) is said to be a consistent estimator
of \(\beta\) if as \(n\) goes to infinity, \(\hat{\beta}\) is in probability
close to \(\beta\), which can be denoted as \(n \rightarrow \infty,
\hat{\beta} \xrightarrow{p} \beta\), or simply as \(\plim_{n \rightarrow
\infty} \hat{\beta} = \beta\).
</p>

<p>
And the law of large number states that for random i.i.d. samples \(x_1,
\ldots, x_n\), if \(E(x_i) = \mu\) and \(\mathrm{Var}(x_i) < \infty\), then
\(\plim_{n \rightarrow \infty} \frac{1}{n}\sum_i x_i = \mu\).
</p>

<p>
Then we can show that \(\plim_{n \rightarrow \infty} \hat{\beta}_1 =
\beta_1\).
</p>
</div>

<ul class="org-ul"><li><a id="org3416e16"></a>A proof of consistency<br  /><div class="outline-text-5" id="text-org3416e16">
<p>
<b>The proof is not required to understand for this course. Therefore,
you can skip it when you first read the notes.</b>
</p>

<p>
From Equation (\ref{eq:betahat-1b}) we can have
\[
\plim_{n \rightarrow \infty} (\hat{\beta}_1 -\beta_1) = \plim_{n \rightarrow \infty} \frac{\frac{1}{n}\sum_i (X_i - \overline{X})u_i}{\frac{1}{n}\sum_i (X_i - \overline{X})^2}
= \frac{\plim_{n \rightarrow \infty} \frac{1}{n}\sum_i (X_i - \overline{X})u_i}{\plim_{n \rightarrow \infty} \frac{1}{n}\sum_i (X_i - \overline{X})^2}
\]
The denominator of the last equality is just a consistent estimator of the sample variance of \(X_i\), that is,
\(\plim_{n \rightarrow \infty} \frac{1}{n}\sum_i (X_i - \overline{X})^2 = \sigma^2_X\)
</p>

<p>
Now we need to focus on \(\plim_{n \rightarrow \infty} \frac{1}{n}\sum_i (X_i - \overline{X}) u_i\). To apply the law of large numbers,
we need to find the expectation of \((X_i - \overline{X})u_i\). Given that
\(E(X_i u_i) = E(E(X_i u_i |X_i)) = E(X_i E(u_i |X_i)) = 0\), we have
\[ E((X_i - \overline{X})u_i) = E(X_i u_i) + \frac{1}{n} \sum_i E(X_i u_i)
= 0 + 0 = 0  \]
So the variance of \((X_i - \overline{X})u_i\) can be expressed as
</p>
\begin{equation*}
\begin{split}
\mathrm{Var}((X_i - \overline{X})u_i) &= E((X-\overline{X})^2 u_i^2) \\
&= E(E((X - \overline{X})^2 u_i^2|X)) \\
&= E((X-\overline{X})^2 E(u_i^2|X)) \\
&= E((X-\overline{X})^2 \sigma_u^2)\; \text{ (by the extended assumption 4. See Chapter 17)} \\
&< \infty\; \text{ (by assumption 3)}
\end{split}
\end{equation*}
<p>
Since \(E((X_i - \overline{X})u_i) = 0\), \(\mathrm{Var}((X_i - \overline{X})u_i) < \infty\), and \(X_i, u_i\) for \(i=1, \ldots, n\) are i.i.d,
by the law of large numbers, we have
\[ \plim_{n \rightarrow \infty} \frac{1}{n} \sum_i (X_i - \overline{X}) u_i = 0 \]
Therefore, \(\plim_{n \rightarrow \infty} \hat{\beta}_1 = \beta_1\).
</p>

<p>
Similarly, we can also prove that \(\hat{\beta}_0\) is consistent, that
is \(\plim_{n \rightarrow \infty} \hat{\beta}_0 = \beta_0\).
</p>
</div></li></ul>
</div>
</div>


<div id="outline-container-org2f55a9a" class="outline-3">
<h3 id="org2f55a9a"><span class="section-number-3">7.2</span> The asymptotic normal distribution</h3>
<div class="outline-text-3" id="text-7-2">
<p>
The central limit theory states that if \(X_1, \ldots, X_n\) with the mean
\(\mu\) and the variance \(0 < \sigma^2 < \infty\). Then,
\(\frac{1}{n}\sum_i X_i \xrightarrow{\text{ d }}
N(\mu, \frac{\sigma^2}{n})\).
</p>

<p>
From the proof of consistency, we have already seen that \(E((X_i -
\overline{X})u_i) = 0\), \(\mathrm{Var}((X_i - \overline{X})u_i) <\infty\),
and \(X_i, u_i\) for \(i=1, \ldots, n\) are i.i.d. By the central limit
theory, we know that
\[\frac{1}{n}\sum_i (X_i - \overline{X})u_i \xrightarrow{d} N \left(0, \frac{1}{n}\mathrm{Var}\left((X_i - \overline{X})u_i\right) \right) \]
It follows that from Equation (\ref{eq:betahat-1b}) and the fact that
\(\plim_{n \rightarrow \infty} \frac{1}{n}\sum_i (X_i - \overline{X})^2 = \mathrm{Var}(X_i)\),
\(\hat{\beta}_1\) is asymptotically normally distributed as
\[ \hat{\beta}_1 \xrightarrow{d} N\left( \beta_1, \sigma^2_{\hat{\beta}_1}\right) \]
where
</p>
\begin{equation}
\label{eq:sigmabeta-1}
\sigma^2_{\hat{\beta}_1} = \frac{1}{n}\frac{\mathrm{Var}\left((X_i - \overline{X})u_i\right)}{\mathrm{Var}(X_i)^2}
\end{equation}

<p>
Similarly, we can show that \(\hat{\beta}_0 \xrightarrow{d} N(\beta_0,
\sigma^2_{\hat{\beta}_0})\), where
</p>
\begin{equation}
\label{eq:sigmabeta-2}
\sigma^2_{\hat{\beta}_0} = \frac{1}{n}\frac{\mathrm{Var}(H_i u_i)}{\left( E(H^2_i) \right)^2}, \text{ and }
H_i = 1 - \left( \frac{\mu_X}{E(X_i^2)} \right)X_i
\end{equation}

<ul class="org-ul">
<li>As \(\mathrm{Var}(X_i)\) increases, \(\mathrm{Var}(\hat{\beta}_1)\) decreases.</li>

<li><p>
As \(\mathrm{Var}(u_i)\) increases, \(\mathrm{Var}(\hat{\beta}_1)\)
increases.
</p>


<div id="org9765110" class="figure">
<p><img src="figure/fig-4-6.png" alt="fig-4-6.png" width="600" />
</p>
<p><span class="figure-number">Figure 6: </span>The Variance of \(\hat{\beta}_1\) and the variance of \(X_i\)</p>
</div></li>
</ul>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
Wikipedia, the free encyclopedia. Regression analysis. Retrieved from <a href="https://en.wikipedia.org/wiki/Regression_analysis">https://en.wikipedia.org/wiki/Regression_analysis</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
Recall that an <b>estimator</b> is a function of a sample of
data. An <b>estimate</b> is the numerical value of the estimator when it is
computed using data from a sample.
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara"><p class="footpara">
We are not going to prove this because it
involves higher level knowledge of linear algebra. You can estimate a
linear regression model of \(Y_i = \beta_1 X_i + u_i\), for which TSS is
simply \(\sum_i^n Y_i^2\) and ESS is \(\sum_i^n \hat{Y}_i^2\). Also, for
this model, \(\sum_i^n \hat{u}_i \neq 0\).
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4">4</a></sup> <div class="footpara"><p class="footpara">
Equation \ref{eq:algebra-ols-1} holds only for a
linear regression model with an intercept, but Equation
\ref{eq:algebra-ols-3} holds regardless of the presence of an intercept.
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Zheng Tian</p>
<p class="date">Created: 2017-03-22 Wed 09:33</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
