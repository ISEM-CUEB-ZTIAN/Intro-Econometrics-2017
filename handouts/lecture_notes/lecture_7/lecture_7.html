<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-03-27 Mon 16:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Lecture 7: Hypothesis Test and Confidence Intervals of Linear Regression with a Single Regressor</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Zheng Tian" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../../css/readtheorg.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Lecture 7: Hypothesis Test and Confidence Intervals of Linear Regression with a Single Regressor</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org55002ef">1. Introduction</a></li>
<li><a href="#org1bbc714">2. Testing Hypotheses about One of the Regression Coefficients</a></li>
<li><a href="#orgdfbce58">3. Confidence Intervals for a Regression Coefficient</a></li>
<li><a href="#orgbdccd62">4. Regression When \(X\) is a Binary Variable</a></li>
<li><a href="#orgfe9b097">5. Heteroskedasticity and Homoskedasticity</a></li>
<li><a href="#orgcfcdf0f">6. The Theoretical Foundations of Ordinary Least Squares</a></li>
<li><a href="#orgb362288">7. Using the t-Statistic in Regression When the Sample Size is Small</a></li>
</ul>
</div>
</div>
\(
 \newcommand{\dx}{\mathrm{d}}
 \newcommand{\var}{\mathrm{Var}}
 \newcommand{\cov}{\mathrm{Cov}}
 \newcommand{\corr}{\mathrm{Corr}}
 \newcommand{\pr}{\mathrm{Pr}}
 \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
 \renewcommand\chaptername{Lecture}
 \DeclareMathOperator*{\plim}{plim}
 \newcommand{\plimn}{\plim_{n \rightarrow \infty}}
\)


<div id="outline-container-org55002ef" class="outline-2">
<h2 id="org55002ef"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
This chapter consists of two parts. The first part concerns hypothesis
testing for a single coefficient in a simple linear regression
model. The basic concepts and ideas of hypothesis testing in this
chapter can be naturally adopted in multiple regression models
(Chapters 6 and 7). The second part goes back to some estimation
issues, including a binary regressor, homoskedasticity versus
heteroskedasticity, as well as the Gauss-Markov theorem, one of the
most fundamental theories regarding the OLS estimation. Finally,
this chapter ends up with the small sample properties of the
t-statistics.
</p>

<p>
One of the features of this textbook is that it introduces the
heteroskedasticity-robust standard error of the OLS estimators, which
is considered as a general case and homoskedasticity as a special
case. This is contrary to the common layouts of an Econometrics
textbook that often first gives the assumption of homoskedasticity,
which is a component of the classical OLS assumptions (equivalent to
the three least squares assumptions plus the assumption of the
homoskedastic and conditionally normally distributed errors). Then
treat heteroskedasticity as a violation to these assumptions. Also,
you should be aware that most discussions of the sample distributions
in this textbook are in the context of a large sample, while the small
sample statistical properties are not the focus.
</p>
</div>
</div>


<div id="outline-container-org1bbc714" class="outline-2">
<h2 id="org1bbc714"><span class="section-number-2">2</span> Testing Hypotheses about One of the Regression Coefficients</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-orgb0b2c0c" class="outline-3">
<h3 id="orgb0b2c0c"><span class="section-number-3">2.1</span> Two-sided hypotheses concerning \(\beta_1\)</h3>
<div class="outline-text-3" id="text-2-1">
<p>
In the last lecture, we estimate a simple linear regression model for test
scores and class sizes, which yields the following estimated sample
regression function,
</p>

\begin{equation}
\label{eq:testscr-str-1e}
\widehat{TestScore} = 698.93 - 2.28 \times STR
\end{equation}

<p>
Now the question faced by the superintendent of the California
elementary school districts is whether the estimated coefficient on
<i>STR</i> is valid. In the terminology of statistics, his question is
whether \(\beta_1\) is statistically significantly different from
zero. More often, we simply say whether \(\beta_1\) is significant. 
</p>

<p>
Generally, as we did in Lecture 3, we can do a hypothesis test
regarding whether \(\beta_1\) takes on a specific value \(\beta_{1,0}\)
through the following steps.
</p>
</div>

<div id="outline-container-orgd28c990" class="outline-4">
<h4 id="orgd28c990">Step 1: set up the two-sided hypothesis</h4>
<div class="outline-text-4" id="text-orgd28c990">
<p>
\[ H_0: \beta_1 = \beta_{1,0} \text{ vs. } H_1: \beta_1 \neq \beta_{1,0} \]
</p>

<p>
The null hypothesis is that \(\beta_1\) is equal to a specific value
\(\beta_{1,0}\), and the alternative hypothesis is the opposite. 
</p>
</div>
</div>

<div id="outline-container-orgb7d6df7" class="outline-4">
<h4 id="orgb7d6df7">Step 2: Compute the t-statistic</h4>
<div class="outline-text-4" id="text-orgb7d6df7">
<p>
The general form of the t-statistic is
</p>

\begin{equation}
\label{eq:general-t}
t = \frac{\text{estimator} - \text{hypothesized value}}{\text{standard error of the estimator}}
\end{equation}

<p>
The t-statistics for testing \(\beta_1\) is then
</p>

\begin{equation}
\label{eq:t-stat-b1}
t = \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}
\end{equation}
</div>

<ul class="org-ul"><li><a id="orgd403cf1"></a>The standard error of \(\hat{\beta}_1\) is calculated as<br  /><div class="outline-text-5" id="text-orgd403cf1">
\begin{equation}
\label{eq:se-b-1}
SE(\hat{\beta}_1) = \sqrt{\hat{\sigma}^2_{\hat{\beta}_1}}
\end{equation}
<p>
where
</p>
\begin{equation}
\label{eq:sigma-b-1}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \bar{X})^2 \hat{u}^2_i}{\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right]^2}
\end{equation}
</div></li>

<li><a id="org8b28d48"></a>How to understand Equation (\ref{eq:sigma-b-1}):<br  /><div class="outline-text-5" id="text-org8b28d48">
<ul class="org-ul">
<li>\(\hat{\sigma}^2_{\hat{\beta}_1}\) is the estimator of the variance of
\(\hat{\beta}_1\), i.e., \(\mathrm{Var}(\hat{\beta}_1)\).</li>

<li>In the last lecture, we know that the variance of \(\hat{\beta}_1\) is  
\[ \sigma^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\var\left( (X_i - \mu_X)u_i \right)}{\left( \var(X_i) \right)^2} \]</li>

<li>The denominator in Equation (\ref{eq:sigma-b-1}) is a consistent
estimator of \(\var(X_i)^2\).</li>

<li>The numerator in Equation (\ref{eq:sigma-b-1}) is a consistent
estimator of \(\var((X_i - \mu_X)u_i)\).</li>

<li>The standard error computed from Equation (\ref{eq:sigma-b-1}) is
the <b>heteroskedasticity-robust standard error</b>, which will be
explained in detail shortly in this lecture.</li>
</ul>
</div></li></ul>
</div>

<div id="outline-container-orgd8c6835" class="outline-4">
<h4 id="orgd8c6835">Step 3: Compute the p-value</h4>
<div class="outline-text-4" id="text-orgd8c6835">
<p>
The p-value is the probability of observing a value of \(\hat{\beta}_1\)
at least as different from \(\beta_{1,0}\) as the estimate actually
computed (\(\hat{\beta}^{act}_1\)), assuming that the null hypothesis is
correct. 
</p>

<p>
Accordingly, under the null hypothesis, the p-value for
testing \(\beta_1\) can be expressed with a probability function as
</p>
\begin{equation*}
\begin{split}
p\text{-value} &= \pr_{H_0} \left( | \hat{\beta}_1 - \beta_{1,0} | > | \hat{\beta}^{act}_1 - \beta_{1,0} | \right) \\
&= \pr_{H_0} \left( \left| \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \right| > \left| \frac{\hat{\beta}^{act}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \right| \right) \\
&= \pr_{H_0} \left( |t| > |t^{act}| \right)
\end{split}
\end{equation*}

<p>
With a large sample, the t statistic is approximately distributed as
a standard normal random variable. Therefore, we can compute 
\[p\text{-value} = \pr\left(|t| > |t^{act}|
\right) = 2 \Phi(-|t^{act}|)\]
where \(\Phi(\cdot)\) is the c.d.f. of the standard normal
distribution. 
</p>

<p>
The null hypothesis is rejected at the 5% significance level if the
\(p\text{-value} < 0.05\) or, equivalently, \(|t^{act}| > 1.96\). 
</p>
</div>
</div>

<div id="outline-container-orgff3479a" class="outline-4">
<h4 id="orgff3479a">Application to test scores</h4>
<div class="outline-text-4" id="text-orgff3479a">
<p>
The OLS estimation of the linear regression model of test scores
against student-teacher ratios, together with the standard errors of
all parameters in the model, can be represented using the following
equation, 
</p>

\begin{equation*}
\widehat{TestScore} = \underset{\displaystyle (10.4)}{698.9} - \underset{\displaystyle (0.52)}{2.28} \times STR,\; R^2 = 0.051,\; SER = 1.86
\end{equation*}

<p>
The <b>heteroskedasticity-robust</b> standard errors are reported in the
parentheses, that is, \(SE(\hat{\beta}_0) = 10.4\) and
\(SE(\hat{\beta}_1) = 0.52\). 
</p>

<p>
The superintendent's question is whether \(\beta_1\) is significant, for
which we can test the null hypothesis against the alternative one as
\[ H_0: \beta_1 = 0, H_1: \beta_1 \neq 0 \]
</p>

<p>
The t-statistics is
\[ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} = \frac{-2.28}{0.52}
= -4.38 < -1.96 \] 
</p>

<p>
The p-value associated with \(t^{act} = -4.38\) is approximately
0.00001, which is far less than 0.05. 
</p>

<p>
Based on the t-statistics and the p-value, we can say the null
hypothesis is rejected at the 5% significance level. In English, it
means that the student-teacher ratios do have a significant effect on
test scores. 
</p>


<div id="orga1cd356" class="figure">
<p><img src="figure/fig-5-1.png" alt="fig-5-1.png" width="600" />
</p>
<p><span class="figure-number">Figure 1: </span>Calculating the p-value of a two-sided test when \(t^{act}=-4.38\)</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org2bf8373" class="outline-3">
<h3 id="org2bf8373"><span class="section-number-3">2.2</span> The one-sided alternative hypothesis</h3>
<div class="outline-text-3" id="text-2-2">
</div><div id="outline-container-orge9bb9d7" class="outline-4">
<h4 id="orge9bb9d7">The one-sided hypotheses</h4>
<div class="outline-text-4" id="text-orge9bb9d7">
<p>
In some cases, it is appropriate to use a one-sided hypothesis
test. For example, the superintendent of the California school
districts want to know whether an increase in class sizes has a
negative effect on test scores, that is, \(\beta_1 < 0\).
</p>

<p>
For a one-sided test, the null hypothesis and the one-sided
alternative hypothesis are <sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
</p>

<p>
\[ H_0: \beta_1 = \beta_{1,0}, H_1: \beta_1 < \beta_{1,0} \]
</p>
</div>
</div>

<div id="outline-container-org2223b22" class="outline-4">
<h4 id="org2223b22">The one-sided left-tail test</h4>
<div class="outline-text-4" id="text-org2223b22">
<ul class="org-ul">
<li>The t-statistic is the same as in a two-sided test
\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \]</li>
<li>Since we test \(\beta_1 < \beta_{1,0}\), if this is true, the
t-statistics should be statistically significantly less than zero.</li>
<li>The p-value is computed as \(\pr(t < t^{act}) = \varPhi(t^{act})\).</li>
<li>The null hypothesis is rejected at the 5% significance level when
the \(p-\text{value} < 0.05\) or \(t^{act} < -1.645\).</li>
<li>In the application of test scores, the t-statistics is -4.38, which
is less than -1.645 and -2.33 (the critical value for a one-sided
test with a 1% significance level). Thus, the null hypothesis is
rejected at the 1% level.</li>
</ul>
</div>
</div>
</div>
</div>


<div id="outline-container-orgdfbce58" class="outline-2">
<h2 id="orgdfbce58"><span class="section-number-2">3</span> Confidence Intervals for a Regression Coefficient</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-orgdf3a0a8" class="outline-3">
<h3 id="orgdf3a0a8"><span class="section-number-3">3.1</span> Two equivalent definitions of confidence intervals</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Recall that a 95% <b>confidence interval</b> for \(\beta_1\) has two equivalent
definitions:
</p>
<ol class="org-ol">
<li>It is the set of values that cannot be rejected using a two-sided
hypothesis test with a 5% significance level.</li>
<li>It is an interval that has a 95% probability of containing the true
value of \(\beta_1\).</li>
</ol>
</div>
</div>

<div id="outline-container-org2724703" class="outline-3">
<h3 id="org2724703"><span class="section-number-3">3.2</span> Construct the 95% confidence interval for \(\beta_1\)</h3>
<div class="outline-text-3" id="text-3-2">
<p>
The 95% confidence interval for \(\beta_1\) can be constructed using the
t-statistic, assuming that with large samples, the t-statistic is
approximately normally distributed. The 95% critical value of a
standard normal distribution is 1.96. Therefore, we can obtain the 95%
confidence interval for \(\beta_1\) by the following steps
</p>

\begin{gather*}
-1.96 \leq \frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)} \leq 1.96 \\
\hat{\beta}_1 - 1.96 SE(\hat{\beta}_1) \leq \beta_1 \leq \hat{\beta}_1 + 1.96 SE(\hat{\beta}_1)
\end{gather*}

<p>
The 95% confidence interval for \(\beta_1\) is 
\[ \left[ \hat{\beta}_1 - 1.96 SE(\hat{\beta}_1),\; \hat{\beta}_1 + 1.96
SE(\hat{\beta}_1) \right] \]
</p>
</div>
</div>


<div id="outline-container-orgab570ed" class="outline-3">
<h3 id="orgab570ed"><span class="section-number-3">3.3</span> The application to test scores</h3>
<div class="outline-text-3" id="text-3-3">
<p>
In the application to test scores, given that \(\hat{\beta}_1 = -2.28\)
and \(SE(\hat{\beta}_1) = 0.52\), the 95% confidence interval for
\(\beta_1\) is \({-2.28 \pm 1.96 \times 0.52}\), or \(-3.30 \leq \beta_1
\leq -1.26\). 
</p>

<p>
Note that the confidence interval only spans over the negative
region with zero leaving outside the interval, which implies that the
null hypothesis of \(\beta_1 = 0\) can be rejected at the 5%
significance level.
</p>
</div>
</div>


<div id="outline-container-org583bce8" class="outline-3">
<h3 id="org583bce8"><span class="section-number-3">3.4</span> Confidence intervals for predicted effects of changing \(X\)</h3>
<div class="outline-text-3" id="text-3-4">
<p>
\(\beta_1\) is the marginal effect of \(X\) on \(Y\), that is, 
\[ \beta_1 = \frac{\dx Y}{ \dx X} \Rightarrow \dx Y = \beta_1 \dx X \]
When \(X\) changes by \(\Delta X\), \(Y\) changes by \(\beta_1 \Delta X\). 
</p>

<p>
So the 95% confidence interval for \(\beta_1 \Delta X\) is
\[ \left[ \hat{\beta}_1 \Delta X - 1.96 SE(\hat{\beta}_1) \Delta X,\;
\hat{\beta}_1 \Delta X + 1.96SE(\hat{\beta}_1) \Delta X \right] \]
</p>
</div>
</div>
</div>


<div id="outline-container-orgbdccd62" class="outline-2">
<h2 id="orgbdccd62"><span class="section-number-2">4</span> Regression When \(X\) is a Binary Variable</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-org2e3cd23" class="outline-3">
<h3 id="org2e3cd23"><span class="section-number-3">4.1</span> A binary variable</h3>
<div class="outline-text-3" id="text-4-1">
<p>
A <b>binary variable</b> takes on values of one if some condition is true
and zero otherwise, which is also called a <b>dummy variable</b>, a
<b>categorical variable</b>, or an <b>indicator variable</b>.
</p>

<p>
For example, 
</p>
\begin{equation*}
D_i = 
\begin{cases}
1,\; &\text{if the } i^{th} \text{ subject is female} \\
0,\; &\text{if the } i^{th} \text{ subject is male} 
\end{cases}
\end{equation*}

<p>
The linear regression model with a dummy variable as a regressor is
</p>
\begin{equation}
\label{eq:dummy-1}
Y_i = \beta_0 + \beta_1 D_i + u_i,\; i = 1, \ldots, n
\end{equation}

<p>
The coefficient on \(D_i\) is estimated by the OLS estimation method
in the same way as a continuous regressor. The difference lies in how
we interpret \(\beta_1\). 
</p>
</div>
</div>


<div id="outline-container-orgd4b695e" class="outline-3">
<h3 id="orgd4b695e"><span class="section-number-3">4.2</span> Interpretation of the regression coefficients</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Given that the assumption \(E(u_i | D_i) = 0\) holds in Equation
(\ref{eq:dummy-1}), we have two population regression functions for
the two cases, that is,
</p>
<ul class="org-ul">
<li>When \(D_i = 1\), \(E(Y_i|D_i = 1) = \beta_0 + \beta_1\)</li>
<li>When \(D_i = 0\), \(E(Y_i|D_i = 0) = \beta_0\)</li>
</ul>

<p>
Therefore, \(\beta_1 = E(Y_i | D_i = 1) - E(Y_i |D_i = 0)\), that is,
<b>the difference in the population means</b> between two groups represented by
\(D_i = 1\) and \(D_i = 0\), respectively.
</p>
</div>
</div>


<div id="outline-container-org92fe9e1" class="outline-3">
<h3 id="org92fe9e1"><span class="section-number-3">4.3</span> Hypothesis tests and confidence intervals</h3>
<div class="outline-text-3" id="text-4-3">
<p>
The hypothesis tests and confidence intervals for the coefficient on a
binary variable follows the same procedure of those for a continuous
variable \(X\). 
</p>

<p>
Usually, the null and alternative hypotheses concerning a dummy variable are
\[ H_0:\, \beta_1 = 0 \text{ vs. } H_1:\, \beta_1 \neq 0 \]
Therefore, the t-statistic is 
\[ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} \]
And the 95% confidence interval is
\[ \hat{\beta}_1 \pm 1.96 SE(\hat{\beta}_1) \]
</p>
</div>
</div>
</div>


<div id="outline-container-orgfe9b097" class="outline-2">
<h2 id="orgfe9b097"><span class="section-number-2">5</span> Heteroskedasticity and Homoskedasticity</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-org66921f9" class="outline-3">
<h3 id="org66921f9"><span class="section-number-3">5.1</span> What are heteroskedasticity and homoskedasticity?</h3>
<div class="outline-text-3" id="text-5-1">
</div><div id="outline-container-orgac4ef92" class="outline-4">
<h4 id="orgac4ef92">Homoskedasticity</h4>
<div class="outline-text-4" id="text-orgac4ef92">
<p>
The error term \(u_i\) is <b>homoskedastic</b> if the conditional variance of
\(u_i\) given \(X_i\) is constant for all \(i = 1, \ldots, n\). Mathematically,
it says \(\var(u_i | X_i) = \sigma^2,\, \text{ for } i = 1, \ldots, n\),
i.e., the variance of \(u_i\) for all <i>i</i> is a constant and does not
depend on \(X_i\).
</p>
</div>
</div>

<div id="outline-container-org9cd6804" class="outline-4">
<h4 id="org9cd6804">Heteroskedasticity</h4>
<div class="outline-text-4" id="text-org9cd6804">
<p>
In contrast, the error term \(u_i\) is <b>heteroskedastic</b> if the conditional variance of
\(u_i\) given \(X_i\) changes on \(X_i\) for \(i = 1, \ldots, n\). That is,
\(\var(u_i | X_i) = \sigma^2_i,\, \text{ for } i = 1, \ldots, n\). 
</p>

<p>
e.g.. A multiplicative form of heteroskedasticity is \(\var(u_i|X_i)
= \sigma^2 f(X_i)\) where \(f(X_i)\) is a function of \(X_i\), for
example, \(f(X_i) = X_i\) as a simplest case. 
</p>
</div>
</div>
</div>


<div id="outline-container-org9266c48" class="outline-3">
<h3 id="org9266c48"><span class="section-number-3">5.2</span> Mathematical implications of homoskedasticity</h3>
<div class="outline-text-3" id="text-5-2">
</div><div id="outline-container-orgec87942" class="outline-4">
<h4 id="orgec87942">Unbiasedness, consistency, and the asymptotic distribution</h4>
<div class="outline-text-4" id="text-orgec87942">
<p>
As long as the least squares assumptions holds, whether the error
term, \(u_i\), is homoskedastic or heteroskedastic does not affect
unbiasedness, consistency, and the asymptotic normal distribution
of the OLS estimators.
</p>
<ul class="org-ul">
<li>The unbiasedness requires that \(E(u_i|X_i) = 0\)</li>
<li>The consistency requires that \(E(X_i u_i) = 0\), which is true if
\(E(u_i|X_i)=0\).</li>
<li>The asymptotic normal distribution requires additionally that
\(\var((X_i-\mu_X)u_i) < \infty\), which still holds as long as
Assumption 3 holds, that is, no extreme outliers of \(X_i\).</li>
</ul>
</div>
</div>

<div id="outline-container-orgdffec59" class="outline-4">
<h4 id="orgdffec59">Efficiency</h4>
<div class="outline-text-4" id="text-orgdffec59">
<p>
The existence of heteroskedasticity affects the efficiency of the
OLS estimator
</p>
<ul class="org-ul">
<li>Suppose \(\hat{\beta}_1\) and \(\tilde{\beta}_1\) are both unbiased
estimators of \(\beta_1\). Then, \(\hat{\beta}_1\) is said to be more
<b>efficient</b> than \(\tilde{\beta}_1\) if \(\var(\hat{\beta}_1) <
  \var(\tilde{\beta}_1)\).</li>
<li>When the errors are homoskedastic, the OLS estimators
\(\hat{\beta}_0\) and \(\hat{\beta}_1\) are efficient among all
estimators that are linear in \(Y_1, \ldots, Y_n\) and are unbiased,
conditional on \(X_1, \ldots, X_n\).</li>

<li>See the Gauss-Markov Theorem below.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-orgfbe2c16" class="outline-3">
<h3 id="orgfbe2c16"><span class="section-number-3">5.3</span> The homoskedasticity-only variance formula</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Recall that we can write \(\hat{\beta}_1\) as
</p>
\begin{equation*}
\hat{\beta}_1 = \beta_1 + \frac{\sum_i (X_i - \bar{X})u_i}{\sum_i
(X_i - \bar{X})^2} 
\end{equation*} 

<p>
Therefore, if \(u_i\) for \(i=1, \ldots, n\) is
homoskedastic and \(\sigma^2\) is known, then
</p>
\begin{equation}
\label{eq:vbeta-1a} \var(\hat{\beta}_1 | X_i) = \frac{\sum_i (X_i -
\bar{X})^2 \var(u_i|X_i)}{\left[\sum_i (X_i - \bar{X})^2\right]^2} =
\frac{\sigma^2}{\sum_i (X_i - \bar{X})^2} 
\end{equation} 

<p>
When \(\sigma^2\) is unknown, then we use \(s^2_u = 1/(n-2) \sum_i
\hat{u}_i^2\) as an estimator of \(\sigma^2\). Thus, the
homoskedasticity-only estimator of the variance of \(\hat{\beta}_1\) is
</p>
\begin{equation}
\label{eq:vbeta-1b} \tilde{\sigma}^2_{\hat{\beta}_1} =
\frac{s^2_u}{\sum_i (X_i - \bar{X})^2} 
\end{equation} 

<p>
And the homoskedasticity-only standard error is \(SE(\hat{\beta}_1) =
\sqrt{\tilde{\sigma}^2_{\hat{\beta}_1}}\).
</p>

<p>
Recall that the heteroskedasticity-robust standard error is
</p>
\begin{equation*}
SE(\hat{\beta}_1) = \sqrt{\hat{\sigma}^2_{\hat{\beta}_1}}
\end{equation*} 
<p>
where
</p>
\begin{equation*}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\frac{1}{n-2}
\sum_{i=1}^n (X_i - \bar{X})^2 \hat{u}^2_i}{\left[ \frac{1}{n}
\sum_{i=1}^n (X_i - \bar{X})^2 \right]^2} 
\end{equation*} 
<p>
which is also referred to as Eicker-Huber-White standard errors.
</p>
</div>
</div>


<div id="outline-container-orgce367df" class="outline-3">
<h3 id="orgce367df"><span class="section-number-3">5.4</span> What does this mean in practice?</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li>Heteroskedasticity is common in cross-sectional data. If you do not
have strong beliefs in homoskedasticity, then it is always safer to
report the heteroskedasticity-robust standard errors and use these
to compute the robust t-statistic.</li>
<li>In most software, the default setting is to report the
homoskedasticity-only standard errors. Therefore, you need to
manually add the option for the robust estimation. 

<ul class="org-ul">
<li><p>
In R, you can use the following codes
</p>
<pre class="example">
library(lmtest)
model1 &lt;- lm(testscr ~ str, data = classdata)
coeftest(model1, vcov = vcovHC(model1, type="HC1"))
</pre></li>

<li><p>
In STATA, you can use
</p>
<pre class="example">
regress testscr str, robust
</pre></li>
</ul></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-orgcfcdf0f" class="outline-2">
<h2 id="orgcfcdf0f"><span class="section-number-2">6</span> The Theoretical Foundations of Ordinary Least Squares</h2>
<div class="outline-text-2" id="text-6">
<p>
In this section, we are going to show that under some conditions, the
OLS estimators are the Best Linear Unbiased Estimators (BLUE). 
</p>
</div>

<div id="outline-container-org714265d" class="outline-3">
<h3 id="org714265d"><span class="section-number-3">6.1</span> The Gauss-Markov conditions</h3>
<div class="outline-text-3" id="text-6-1">
<p>
We have already known the least squares assumptions: for \(i = 1,
\ldots, n\), (1) \(E(u_i|X_i) = 0\), (2) \((X_i, Y_i)\) are i.i.d., and (3)
large outliers are unlikely. The Gauss-Markov conditions are similar
to these least squares assumptions and add the assumption of
homoskedastic errors.
</p>
</div>

<div id="outline-container-org14af0fd" class="outline-4">
<h4 id="org14af0fd">The Gauss-Markov conditions</h4>
<div class="outline-text-4" id="text-org14af0fd">
<p>
For \(\mathbf{X} = [X_1, \ldots, X_n]\) <sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>
</p>

<p>
Here I use the vector
notation to represent all observations of \(X_i\) for \(i=1, \ldots,
n\). We will formally introduce the matrix notation for a linear
regression model and the OLS estimation in the next lecture.
</p>

<ol class="org-ol">
<li>\(E(u_i| \mathbf{X}) = 0\)</li>
<li>\(\var(u_i | \mathbf{X}) = \sigma^2_u,\, 0 < \sigma^2_u < \infty\)</li>
<li>\(E(u_i u_j | \mathbf{X}) = 0,\, i \neq j\)</li>
</ol>
</div>
</div>

<div id="outline-container-orga51bf9e" class="outline-4">
<h4 id="orga51bf9e">From the three Least Squares Assumptions and the homoskedasticity assumption to the Gauss-Markov conditions</h4>
<div class="outline-text-4" id="text-orga51bf9e">
<p>
Note that the conditional expectations in the Gauss-Markov conditions
regard all observations \(\mathbf{X}\), not just one observation,
\(X_i\). However, all the Gauss-Markov conditions can be derived from
the least squares assumptions plus the homoskedasticity
assumption. Specifically,
</p>

<ul class="org-ul">
<li>Assumptions (1) and (2) imply \(E(u_i | \mathbf{X}) = E(u_i | X_i) =
  0\).</li>
<li>Assumptions (1) and (2) imply \(\var(u_i| \mathbf{X}) =
  \var(u_i | X_i)\). With the homoskedasticity assumption, \(\var(u_i |
  X_i) = \sigma^2_u\), Assumption (3) then implies \(0 < \sigma^2_u < \infty\).</li>
<li>Assumptions (1) and (2) imply that \(E(u_i u_j | \mathbf{X}) = E(u_i
  u_j | X_i, X_j) = E(u_i|X_i) E(u_j|X_j) = 0\).</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org42b0824" class="outline-3">
<h3 id="org42b0824"><span class="section-number-3">6.2</span> Linear conditionally unbiased estimator</h3>
<div class="outline-text-3" id="text-6-2">
</div><div id="outline-container-orgd7cc93d" class="outline-4">
<h4 id="orgd7cc93d">The general form of a linear conditionally unbiased estimator of \(\beta_1\)</h4>
<div class="outline-text-4" id="text-orgd7cc93d">
<p>
The class of linear conditionally unbiased estimators consists of all
estimators of \(\beta_1\) that are linear function of \(Y_i, \ldots, Y_n\)
and that are unbiased, conditioned on \(X_1, \ldots, X_n\). 
</p>

<p>
For any linear estimator \(\tilde{\beta}_1\), it can be written as
</p>
\begin{equation}
\label{eq:beta1-tilde}
\tilde{\beta}_1 = \sum_{i=1}^n a_i Y_i\
\end{equation}
<p>
where the weights \(a_i\) for \(i = 1, \ldots, n\) depend on \(X_1, \ldots,
X_n\) but not on \(Y_1, \ldots, Y_n\). 
</p>

<p>
\(\tilde{\beta}_1\) is conditionally unbiased means that
</p>
\begin{equation}
\label{eq:e-beta1-tilde}
E(\tilde{\beta}_1 | \mathbf{X}) = \beta_1\
\end{equation}

<p>
By the Gauss-Markov conditions, from Equation (\ref{eq:beta1-tilde}),  we can have
</p>
\begin{equation*}
\begin{split}
E(\tilde{\beta}_1 | \mathbf{X}) &= \sum_i a_i E(\beta_0 + \beta_1 X_i + u_i | \mathbf{X}) \\
&= \beta_0 \sum_i a_i + \beta_1 \sum_i a_i X_i
\end{split}
\end{equation*}

<p>
For Equation (\ref{eq:e-beta1-tilde}) being satisfied with any
\(\beta_0\) and \(\beta_1\), we must have
\[ \sum_i a_i = 0 \text{ and } \sum_i a_iX_i = 1 \]
</p>
</div>
</div>

<div id="outline-container-org69e9634" class="outline-4">
<h4 id="org69e9634">The OLS esimator \(\hat{\beta}_1\) is a linear conditionally unbiased estimator</h4>
<div class="outline-text-4" id="text-org69e9634">
<p>
We have known that \(\hat{\beta}_1\) is unbiased both conditionally and
unconditionally. Next, we show that it is linear. 
\[ \hat{\beta}_1 = \frac{\sum_i (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_i
(X_i - \bar{X})^2} = \frac{\sum_i (X_i - \bar{X})Y_i}{\sum_i
(X_i - \bar{X})^2} = \sum_i \hat{a}_i Y_i \]
where the weights are
\[ \hat{a}_i = \frac{X_i - \bar{X}}{\sum_i (X_i - \bar{X})^2}, \text{
for } i = 1, \ldots, n \] 
Since \(\hat{\beta}_1\) is a linear conditionally unbiased estimator, we
must have
\[ \sum_i \hat{a}_i = 0 \text{ and } \sum_i \hat{a}_i X_i = 1  \]
which can be simply verified.
</p>
</div>
</div>
</div>


<div id="outline-container-org9fe29f6" class="outline-3">
<h3 id="org9fe29f6"><span class="section-number-3">6.3</span> The Gauss-Markov Theorem</h3>
<div class="outline-text-3" id="text-6-3">
<p>
The Gauss-Markov Theorem for \(\hat{\beta}_1\) states
</p>
<blockquote id="The Gauss-Markov Theorem for $\tilde{\beta}_{1}$">
<p>
If the Gauss-Markov conditions hold, then the OLS estimator
\(\hat{\beta}_1\) is the Best (most efficient) Linear conditionally
Unbiased Estimator (BLUE).
</p>
</blockquote>

<p>
The theorem can also be applied to \(\hat{\beta}_0\).
</p>

<p>
The proof of the Gauss-Markov theorem is in Appendix 5.2. A key in
this proof is that we can rewrite the expression of any linear
conditionally unbiased estimator \(\tilde{\beta}_1\) as
\[ \tilde{\beta}_1 = \sum_i a_i Y_i = \sum_i (\hat{a}_i + d_i)Y_i =
\hat{\beta}_1 + \sum_i d_i Y_i \]
And the goal of
the proof is to show that
\[ \var(\hat{\beta}_1 | \mathbf{X}) \leq \var(\tilde{\beta}_1 |
\mathbf{X}) \]
The equality holds only when \(\tilde{\beta}_1 = \hat{\beta}_1\). 
</p>
</div>
</div>


<div id="outline-container-org15ee0ab" class="outline-3">
<h3 id="org15ee0ab"><span class="section-number-3">6.4</span> The limitations of the Gauss-Markov theorem</h3>
<div class="outline-text-3" id="text-6-4">
<ol class="org-ol">
<li><p>
The Gauss-Markov conditions may not hold in practice. Any violation
of the Gauss-Markov conditions will result in the OLS estimators
that are not BLUE. The table below summarizes the cases in which a
kind of violation occurs, the consequences of such violation to the
OLS estimators, and possible remedies.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Summary of Violations of the Gauss-Markov Theorem</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Violation</th>
<th scope="col" class="org-left">Cases</th>
<th scope="col" class="org-left">Consequences</th>
<th scope="col" class="org-left">Remedies</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(E(u \mid X) \neq 0\)</td>
<td class="org-left">omitted variables, endogeneity</td>
<td class="org-left">biased</td>
<td class="org-left">more \(X\), IV method</td>
</tr>

<tr>
<td class="org-left">\(\var(u_i\mid X)\) not constant</td>
<td class="org-left">heteroskedasticity</td>
<td class="org-left">inefficient</td>
<td class="org-left">WLS, GLS, HCCME</td>
</tr>

<tr>
<td class="org-left">\(E(u_{i}u_{j}\mid X) \neq 0\)</td>
<td class="org-left">autocorrelation</td>
<td class="org-left">inefficient</td>
<td class="org-left">GLS, HAC</td>
</tr>
</tbody>
</table></li>

<li>There are other candidate estimators that are not linear and
conditionally unbiased; under some conditions, these estimators are
more efficient than the OLS estimators.</li>
</ol>
</div>
</div>
</div>


<div id="outline-container-orgb362288" class="outline-2">
<h2 id="orgb362288"><span class="section-number-2">7</span> Using the t-Statistic in Regression When the Sample Size is Small</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-org2f29158" class="outline-3">
<h3 id="org2f29158"><span class="section-number-3">7.1</span> The classical assumptions of the least squares estimation</h3>
<div class="outline-text-3" id="text-7-1">
<p>
We first expand the LS assumptions by two additional assumptions. One
is the assumption of the homoskedastic errors, and another one is the
assumption that the conditional distribution of \(u_i\) given \(X_i\) is
the normal distribution, i.e., \(u_i \mid X_i \sim N(0, \sigma^2_u) \text{ for }
i = 1, \ldots, n\).
</p>

<p>
All these assumptions together are often referred to as the classical
assumptions of the least squares estimation:
For \(i = 1, 2, \ldots, n\)
</p>
<ul class="org-ul">
<li>Assumption 1: \(E(u_i | X_i) = 0\) (exogeneity of \(X\))</li>
<li>Assumption 2: \((X_i, Y_i)\) are i.i.d. (IID of \(X, Y\))</li>
<li>Assumption 3: \(0 < E(X_i^4) < \infty\) and \(0 < E(Y_i^4) < \infty\)
(No large outliers)</li>
<li>Extended Assumption 4: \(\var(u_i | X_i) = \sigma^2_u, \text{ and } 0 <
                   \sigma^2_u < \infty\) (homoskedasticity)</li>
<li>Extended Assumption 5: \(u_i | X_i \sim N(0, \sigma^2_u)\) (normality)</li>
</ul>
</div>
</div>


<div id="outline-container-orgebf2899" class="outline-3">
<h3 id="orgebf2899"><span class="section-number-3">7.2</span> The t-Statistic and the Student-t Distribution</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Under all the classical assumptions, we can construct the
t-statistic for hypothesis testing of a single coefficient. Even with
a small samples, the t-statistic has an exact Student-t distribution. 
</p>
</div>

<div id="outline-container-org9a33a77" class="outline-4">
<h4 id="org9a33a77">The t-statistic is for \(\beta_1\)</h4>
<div class="outline-text-4" id="text-org9a33a77">
<p>
\[H_0: \beta_1 = \beta_{1,0} \text{ vs } H_1: \beta_1 \neq \beta_{1,0}\]
</p>
\begin{equation}
t = \frac{\hat{\beta}_1 - \beta_{1,0}}{\hat{\sigma}_{\hat{\beta}_1}}
\end{equation}
<p>
where
</p>
\begin{equation*}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{s^2_u}{\sum_i (X_i - \bar{X})^2} \text{ and } s^2_u = \frac{1}{n-2}\sum_i \hat{u}_i^2 = SER^2
\end{equation*}
<p>
the former of which is the homoskedasticity-only standard error of
\(\hat{\beta}_1\) and the latter is the standard error of the
regression. 
</p>

<p>
When the classical least squares assumptions hold, we have the
t-statistic has the exact distribution of \(t(n-2)\), i.e., the
Student's t distribution with \((n-2)\) degrees of freedom. 
</p>

<p>
\[ t = \frac{\hat{\beta}_1 -
\beta_{1,0}}{\hat{\sigma}_{\hat{\beta}_1}} \sim t(n-2) \]
</p>

<p>
What follows is to show the above equation is true when all classical
assumptions are true. 
</p>
</div>
</div>

<div id="outline-container-org15e06b6" class="outline-4">
<h4 id="org15e06b6">The Student-t distribution of \(t\)</h4>
<div class="outline-text-4" id="text-org15e06b6">
<p>
The t statistic can be rewritten as
</p>
\begin{equation}
\label{eq:t-stat-b1a}
t = \frac{(\hat{\beta}_1 - \beta_{1,0})/\sigma_{\hat{\beta}_1}}{\sqrt{\frac{\hat{\sigma}^2_{\hat{\beta}_1}}{\sigma^2_{\hat{\beta}_1}}}} 
= \frac{z_{\hat{\beta}_1}}{\sqrt{\frac{s^2_u}{\sigma^2_u}}} = \frac{z_{\hat{\beta}_1}}{\sqrt{\frac{W}{n-2}}}
\end{equation}
<p>
where 
</p>

<p>
\[\sigma^2_{\hat{\beta}_1} = \frac{\sigma^2_u}{\sum_i (X_i -
\bar{X})^2} \] 
</p>

<p>
is the homoskedasticity-only variance of
\(\hat{\beta}_1\) when the variance of errors \(\sigma^2_u\) is known.  
</p>

<p>
\[
z_{\hat{\beta}_1} =\frac{\hat{\beta}_1 -
\beta_{1,0}}{\sigma_{\hat{\beta}_1}} 
\] 
</p>

<p>
is the z-statistic which has a standard normal distribution, that is,
\(z_{\hat{\beta}_1} \sim N(0, 1)\)
</p>

<p>
\[ 
W = (n-2)\frac{s^2_u}{\sigma^2_u} =
\frac{\sum_i\hat{u}_i^2}{\sigma^2_u} = \sum_i
\left(\frac{\hat{u}_i}{\sigma_u}\right)^2
 \] 
</p>

<p>
It can be shown that W is the sum of squares of \((n-2)\) independent
standard normally distributed variables, which results in a
chi-squared distribution with \((n-2)\) degrees of freedom. That is, \(W
\sim \chi^2(n-2)\), which is also independent of
\(z_{\hat{\beta}_1}\). Therefore, the t-statistic in Equation
(\ref{eq:t-stat-b1a}), as the ratio of \(z_{\hat{\beta}_1}\) and
\(\sqrt{W/(n-2)}\), is distributed as \(t(n-2)\).
</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
Note that the trick here is we put the
desired hypothesis to the alternative place. 
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
Here I use the vector
notation to represent all observations of \(X_i\) for \(i=1, \ldots,
n\). We will formally introduce the matrix notation for a linear
regression model and the OLS estimation in the next lecture.
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Zheng Tian</p>
<p class="date">Created: 2017-03-27 Mon 16:52</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
