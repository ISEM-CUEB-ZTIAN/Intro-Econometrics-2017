<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-03-21 Tue 20:16 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Lecture 7: Hypothesis Test and Confidence Intervals of Linear Regression with a Single Regressor</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Zheng Tian" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../../css/readtheorg.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Lecture 7: Hypothesis Test and Confidence Intervals of Linear Regression with a Single Regressor</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgef35215">1. Introduction</a></li>
<li><a href="#orgee5ef20">2. Testing Hypotheses about One of the Regression Coefficients</a></li>
<li><a href="#org4e8995a">3. Confidence Intervals for a Regression Coefficient</a></li>
<li><a href="#org02e0f9d">4. Regression When \(X\) is a Binary Variable</a></li>
<li><a href="#org645f3ba">5. Heteroskedasticity and Homoskedasticity</a></li>
<li><a href="#org570d704">6. The Theoretical Foundations of Ordinary Least Squares</a></li>
<li><a href="#orgac99434">7. Using the t-Statistic in Regression When the Sample Size is Small</a></li>
</ul>
</div>
</div>
\(
 \newcommand{\dx}{\mathrm{d}}
 \newcommand{\var}{\mathrm{Var}}
 \newcommand{\cov}{\mathrm{Cov}}
 \newcommand{\corr}{\mathrm{Corr}}
 \newcommand{\pr}{\mathrm{Pr}}
 \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
 \renewcommand\chaptername{Lecture}
 \DeclareMathOperator*{\plim}{plim}
 \newcommand{\plimn}{\plim_{n \rightarrow \infty}}
\)


<div id="outline-container-orgef35215" class="outline-2">
<h2 id="orgef35215"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-org8d3a5a0" class="outline-3">
<h3 id="org8d3a5a0"><span class="section-number-3">1.1</span> Overview</h3>
<div class="outline-text-3" id="text-1-1">
<p>
This chapter consists of two parts. The first part concerns hypothesis
testing for a single coefficient in a simple linear regression
model. The basic concepts and ideas of hypothesis testing in this
chapter can be naturally adopted in multiple regression models
(Chapters 6 and 7). The second part goes back to some estimation
issues, including a binary regressor, homoskedasticity versus
heteroskedasticity, as well as the Gauss-Markov theorem, one of the
most fundamental theories regarding the OLS estimation. Finally,
this chapter ends up with the small sample properties of the
t-statistics.
</p>

<p>
One of the features of this textbook is that it introduces the
heteroskedasticity-robust standard error of the OLS estimators, which
is considered as a general case and homoskedasticity as a special
case. This is contrary to the common layouts of an Econometrics
textbook that often first gives the assumption of homoskedasticity,
which is a component of the classical OLS assumptions (equivalent to
the three least squares assumptions plus the assumption of the
homoskedastic and conditionally normally distributed errors), 
and considers heteroskedasticity as a violation of these
assumptions. Also, please be aware that most discussions of the sample
distributions in this textbook are in the context of a large sample,
while the small sample properties are not the focus.
</p>
</div>
</div>

<div id="outline-container-orgc12ce73" class="outline-3">
<h3 id="orgc12ce73"><span class="section-number-3">1.2</span> Learning goals</h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>Grasp the basic concepts and techniques of hypothesis testing: the
null vs alternative hypotheses, the p-value, the significance level,
the critical value, the confidence intervals, the t-statistics.</li>
<li>Be able to create a binary variable to represent two categories, and
know how to correctly interpret the coefficient.</li>
<li>Understand homoskedasticity and heteroskedasticity and their
implications.</li>
<li>Understand the Gauss-Markov theorem.</li>
</ul>
</div>
</div>
<div id="outline-container-orgf87d637" class="outline-3">
<h3 id="orgf87d637"><span class="section-number-3">1.3</span> Readings</h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li><i>Introduction to Econometrics</i> by Stock and Watson. Read thoroughly
Chapter 5.</li>
<li><i>Introductory Econometrics: a Modern Approach</i> by
J. Wooldridge. Read Section 2.5 for the OLS assumptions, and Section
4.2 and 4.3 for hypothesis testing and confidence intervals for a
simple regression model.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgee5ef20" class="outline-2">
<h2 id="orgee5ef20"><span class="section-number-2">2</span> Testing Hypotheses about One of the Regression Coefficients</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-org21c2d4a" class="outline-3">
<h3 id="org21c2d4a"><span class="section-number-3">2.1</span> A brief review of basic concepts in hypothesis tests</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Let's quickly review what we have learned in Lecture 3 concerning
hypothesis testing with an example of testing the true value of the
population mean from a random sample. 
</p>
</div>
<div id="outline-container-org95b0cf2" class="outline-4">
<h4 id="org95b0cf2">The null versus alternative hypotheses</h4>
<div class="outline-text-4" id="text-org95b0cf2">
<p>
We want to test two contrasting hypotheses, the null hypothesis versus
the alternative hypothesis. 
</p>
<ul class="org-ul">
<li><p>
Two-sided tests
</p>

<p>
\(H_0:\; E(Y) = \mu_{Y,0}\) v.s. \(H_1:\; E(Y) \neq \mu_{Y,0}\)
</p></li>

<li><p>
One-sided test
</p>

<p>
\(H_0:\; E(Y) = \mu_{Y,0}\) v.s. \(H_1:\; E(Y) > \mu_{Y,0}\)
</p></li>
</ul>

<p>
What follows only concerns the two-sided test. 
</p>
</div>
</div>

<div id="outline-container-orgc51254e" class="outline-4">
<h4 id="orgc51254e">Test statistics</h4>
<div class="outline-text-4" id="text-orgc51254e">
<p>
We need some tool to be used in the test, which is referred to as test
statistics. 
</p>

<ul class="org-ul">
<li>When \(\sigma_Y\) is known, we use the z-statistics
\[ z = \frac{\overline{Y} -
  \mu_{Y,0}}{\sigma_{\overline{Y}}} = \frac{\overline{Y} -
  \mu_{Y,0}}{\sigma_Y/\sqrt{n}} \xrightarrow{\text{ \textit d }} N(0, 1)\]</li>

<li>When \(\sigma_Y\) is unknown, we use the t-statistics<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
\[ t = \frac{\overline{Y} - \mu_{Y,0}}{SE(\overline{Y})} =
  \frac{\overline{Y} - \mu_{Y,0}}{s_Y/\sqrt{n}} \xrightarrow{ \text{
  \textit d } } N(0, 1) \]</li>
</ul>
</div>
</div>

<div id="outline-container-orgf4d7a1f" class="outline-4">
<h4 id="orgf4d7a1f">The rules for hypothesis testing</h4>
<div class="outline-text-4" id="text-orgf4d7a1f">
<p>
We need to set up some rules for judging that under what
circumstances, the null hypothesis can be rejected or fail
to be rejected. 
</p>
</div>
<ul class="org-ul"><li><a id="org61fc814"></a>Type I and type II errors<br  /><div class="outline-text-5" id="text-org61fc814">
<ul class="org-ul">
<li><b>Type I error</b>. The null hypothesis is rejected when in fact it is
true.</li>
<li><b>Type II error</b>. The null hypothesis is not rejected when in fact it
is false.</li>
</ul>
</div></li>

<li><a id="orgf5eaf7f"></a>The significance level, the critical value, and the p-value<br  /><div class="outline-text-5" id="text-orgf5eaf7f">
<ul class="org-ul">
<li><b>The significance level</b>. The pre-specified probability of type I
error.  \(\alpha = 0.05, 0.10, \text{ or } 0.01\)</li>

<li><p>
<b>The critical value</b>. The value of the test statistic for which the
test rejects the null hypothesis at the given significance level.
</p>

<p>
For example. In a two-sided test, with the z statistic. The critical
value at the 5\% significance level is \(c_{\alpha}\) such that
\(\varPhi(c_{\alpha}) = 0.975\). Accordingly, we know \(c_{\alpha}
  \approx 1.96\).
</p></li>

<li><p>
<b>The p-value</b>. The p-value is the probability of drawing a statistic
at least as adverse to the null hypothesis as the one you actually
computed in your sample, assuming the null hypothesis is
correct. 
</p>

<p>
Equivalently, the p-value is the smallest significance
level at which the null hypothesis could be rejected, based on the
test statistic actually observed.
</p>

<p>
Mathematically, the p-value is 
\[  \pr_{H_0}\left( \left| \overline{Y} - \mu_{Y,0}
  \right| > \left| \overline{Y}^{act} - \mu_{Y,0} \right| \right) =
  2\varPhi(-|t^{act}|) \text{ .} \]
</p></li>
</ul>
</div></li>

<li><a id="org18a6851"></a>Rejection rules<br  /><div class="outline-text-5" id="text-org18a6851">
<p>
The following two statements are equivalent in terms of rejecting the
null hypothesis at the 5% significance level. 
</p>

<ul class="org-ul">
<li>We can reject the null if the test statistics falls into the
rejection region set by the critical values at the 5% significance
level, that is, when \(|t^{act}| > c_{\alpha} = 1.96\),</li>
<li>We can reject the null if the p-value is less than the significance
level that is 5% in this case.</li>
</ul>

<p>
The rejection rule can be illustrated using Figure <a href="#org440a34c">1</a>.
</p>


<div id="org440a34c" class="figure">
<p><img src="./figure/fig9_1.png" alt="fig9_1.png" />
</p>
<p><span class="figure-number">Figure 1: </span>An illustration of a two-sided test</p>
</div>
</div></li></ul>
</div>
</div>

<div id="outline-container-org5192873" class="outline-3">
<h3 id="org5192873"><span class="section-number-3">2.2</span> Two-sided hypotheses concerning \(\beta_1\)</h3>
<div class="outline-text-3" id="text-2-2">
</div><div id="outline-container-org0c34a83" class="outline-4">
<h4 id="org0c34a83">Application to test scores</h4>
<div class="outline-text-4" id="text-org0c34a83">
<p>
In the last lecture, we estimate a simple linear regression model for test
scores and class sizes, which yields the following estimated sample
regression function,
</p>

\begin{equation}
\label{eq:testscr-str-1e}
\widehat{TestScore} = 698.93 - 2.28 \times STR
\end{equation}

<p>
Now the question faced by the superintendent of the California
elementary school districts is whether the estimated coefficient on
<i>STR</i> is valid. In the terminology of statistics, his question is
whether \(\beta_1\) is statistically significantly different from zero. 
</p>
</div>
</div>

<div id="outline-container-orgef60a99" class="outline-4">
<h4 id="orgef60a99">Testing hypotheses about the slope \(\beta_1\)</h4>
<div class="outline-text-4" id="text-orgef60a99">
<div class="LaTeX">
<p>
\textcolor{red}{Note: all discussions about hypothesis testing that
follows involve only the regression with a large sample size. The
last section of this lecture touches upon the small sample properties
of the test statistics.}
</p>

</div>
</div>
<ul class="org-ul"><li><a id="orgbb8470f"></a>The two-sided hypothesis<br  /><div class="outline-text-5" id="text-orgbb8470f">
<p>
\[ H_0: \beta_1 = \beta_{1,0} \text{ vs. } H_1: \beta_1 \neq \beta_{1,0} \]
</p>
</div></li>

<li><a id="org0550aa9"></a>The t-statistic<br  /><ul class="org-ul"><li><a id="org82d7986"></a>The general form of the t-statistic<br  /><div class="outline-text-6" id="text-org82d7986">
<p>
In general, the t-statisitc has the form
</p>
\begin{equation}
\label{eq:general-t}
t = \frac{\text{estimator} - \text{hypothesized value}}{\text{standard error of the estimator}}
\end{equation}
</div></li>

<li><a id="orgebca4ac"></a>The t-statistics for testing \(\beta_1\)<br  /><div class="outline-text-6" id="text-orgebca4ac">
\begin{equation}
\label{eq:t-stat-b1}
t = \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}
\end{equation}
</div></li></ul></li>

<li><a id="org72c30a5"></a>The standard error of \(\hat{\beta}_1\) is calculated as<br  /><div class="outline-text-5" id="text-org72c30a5">
\begin{equation}
\label{eq:se-b-1}
SE(\hat{\beta}_1) = \sqrt{\hat{\sigma}^2_{\hat{\beta}_1}}
\end{equation}
<p>
where
</p>
\begin{equation}
\label{eq:sigma-b-1}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \bar{X})^2 \hat{u}^2_i}{\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right]^2}
\end{equation}
</div></li>

<li><a id="orgd441223"></a>How to understand Equation \ref{eq:sigma-b-1}<br  /><div class="outline-text-5" id="text-orgd441223">
<ul class="org-ul">
<li>The population variance of \(\beta_1\) is 
\[ \sigma^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\var\left( (X_i - \mu_X)u_i \right)}{\left( \var(X_i) \right)^2} \]</li>

<li>The denominator in Equation (\ref{eq:sigma-b-1}) is a consistent
estimator of \(\var(X_i)^2\).</li>

<li>The numerator in Equation (\ref{eq:sigma-b-1}) is a consistent
estimator of \(\var((X_i - \mu_X)u_i)\), adjusted by \(n-2\) degrees
of freedom.</li>

<li>The standard error computed from Equation (\ref{eq:sigma-b-1}) is
the <b>heteroskedasticity-robust standard error</b>, which will be
explained in detail shortly in this lecture.</li>
</ul>
</div></li>

<li><a id="orgd431b53"></a>Compute the p-value<br  /><div class="outline-text-5" id="text-orgd431b53">
<p>
Let's reiterate the two equivalent definitions of the p-value:
</p>

<ol class="org-ol">
<li>The p-value is the probability of observing a value of
\(\hat{\beta}_1\) at least as different from \(\beta_{1,0}\) as the
estimate actually computed (\(\hat{\beta}^{act}_1\)), assuming that
the null hypothesis is correct.</li>

<li>The p-value is the smallest significance level at which the null
hypothesis could be rejected, based on the test statistic
actually observed.</li>
</ol>

<p>
According to the first definition, the p-value for testing \(\beta_1\)
can be expressed with a probability function under the null
hypothesis as
</p>
<div class="LaTeX">
\begin{equation*}
\begin{split}
p\text{-value} &= \pr_{H_0} \left( | \hat{\beta}_1 - \beta_{1,0} | > | \hat{\beta}^{act}_1 - \beta_{1,0} | \right) \\
&= \pr_{H_0} \left( \left| \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \right| > \left| \frac{\hat{\beta}^{act}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \right| \right) \\
&= \pr_{H_0} \left( |t| > |t^{act}| \right)
\end{split}
\end{equation*}

</div>

<p>
In large samples, the \(p\text{-value} = \pr\left(|t| > |t^{act}|
\right) = 2 \varPhi(-|t^{act}|)\).
</p>

<p>
The null hypothesis is rejected at the 5% significance level if the
\(p\text{-value} < 0.05\) or \(|t^{act}| > 1.96\), which is the critical
value at the 5% significant level. 
</p>
</div></li>

<li><a id="org18f3518"></a>Application to test scores<br  /><div class="outline-text-5" id="text-org18f3518">
<p>
The OLS estimation of the linear regression model of test scores
against student-teacher ratios, together with the standard errors of
all parameters in the model, can be represented using the following
equation, 
</p>
<div class="LaTeX">
\begin{equation*}
\widehat{TestScore} = \underset{\displaystyle (10.4)}{698.9} - \underset{\displaystyle (0.52)}{2.28} \times STR,\; R^2 = 0.051,\; SER = 1.86
\end{equation*}

</div>

<p>
The <b>heteroskedasticity-robust</b> standard errors are reported in the
parentheses, that is, \(SE(\hat{\beta}_0) = 10.4\) and
\(SE(\hat{\beta}_1) = 0.52\). 
</p>

<p>
The superintendent's question is whether \(\beta_1\) is significant for
which we can test the null hypothesis against the alternative one as
\[ H_0: \beta_1 = 0, H_1: \beta_1 \neq 0 \]
</p>

<p>
The t-statistics is
\[ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} = \frac{-2.28}{0.52}
= -4.38 < -1.96 \] 
</p>

<p>
The p-value associated with \(t^{act} = -4.38\) is approximately
0.00001, which is far less than 0.05. 
</p>

<p>
Based on the t-statistics and the p-value, we can say the null
hypothesis is rejected at the 5% significance level. In English, it
means that the student-teacher ratios do have a significant effect on
test scores. 
</p>


<div id="orge12e653" class="figure">
<p><img src="figure/fig-5-1.png" alt="fig-5-1.png" />
</p>
<p><span class="figure-number">Figure 2: </span>Calculating the p-value of a two-sided test when \(t^{act}=-4.38\)</p>
</div>
</div></li></ul>
</div>
</div>

<div id="outline-container-org4876f78" class="outline-3">
<h3 id="org4876f78"><span class="section-number-3">2.3</span> The one-sided alternative hypothesis</h3>
<div class="outline-text-3" id="text-2-3">
</div><div id="outline-container-org975c763" class="outline-4">
<h4 id="org975c763">The one-sided hypotheses</h4>
<div class="outline-text-4" id="text-org975c763">
<p>
In some cases, it is appropriate to use a one-sided hypothesis
test. For example, the superintendent of the California school
districts want to know whether class sizes have a negative effect on
test scores, that is, \(\beta_1 < 0\). 
</p>

<p>
For a one-sided test, the null hypothesis and the one-sided
alternative hypothesis are<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>  
</p>

<p>
\[ H_0: \beta_1 = \beta_{1,0} \text{ vs. } H_1: \beta_1 < \beta_{1,0} \]
</p>
</div>
</div>

<div id="outline-container-orge473dae" class="outline-4">
<h4 id="orge473dae">The one-sided left-tail test</h4>
<div class="outline-text-4" id="text-orge473dae">
<ul class="org-ul">
<li>The t-statistic is the same as in a two-sided test
\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \]</li>
<li>Since we test \(\beta_1 < \beta_{1,0}\), if this is true, the
t-statistics should be statistically significantly less than zero.</li>
<li>The p-value is computed as \(\pr(t < t^{act}) = \varPhi(t^{act})\).</li>
<li>The null hypothesis is rejected at the 5% significance level when
\(\text{p-value} < 0.05\) or \(t^{act} < -1.645\).</li>
<li>In the application of test scores, the t-statistics is -4.38, which
is less than -1.645 and -2.33 (the critical value for a one-sided
test with a 1% significance level). Thus, the null hypothesis is
rejected at the 1% level.</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org4e8995a" class="outline-2">
<h2 id="org4e8995a"><span class="section-number-2">3</span> Confidence Intervals for a Regression Coefficient</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-org440dcad" class="outline-3">
<h3 id="org440dcad"><span class="section-number-3">3.1</span> Two equivalent definitions of confidence intervals</h3>
<div class="outline-text-3" id="text-3-1">
<p>
A 95% <b>confidence interval</b> for \(\beta_1\) has two equivalent
definitions. 
</p>
<ol class="org-ol">
<li>It is the set of values that cannot be rejected using a <span class="underline">two-sided</span>
hypothesis test with a 5% significance level.</li>
<li>It is an interval that has a 95% probability of containing the true
value of \(\beta_1\); that is, in 95% of possible samples that might be
drawn, the confidence interval will contain the true value of
\(\beta_1\).</li>
</ol>

<p>
Let's go back to Figure <a href="#org440a34c">1</a>. According to the first
definition, the acceptance region contains the values of the
test statistics that fail to reject the null hypothesis,
which corresponds to the values of \(\beta_1\) that cannot be rejected. 
</p>
</div>
</div>

<div id="outline-container-org0108e8d" class="outline-3">
<h3 id="org0108e8d"><span class="section-number-3">3.2</span> Construct the 95% confidence interval for \(\beta_1\)</h3>
<div class="outline-text-3" id="text-3-2">
<p>
The 95% confidence interval for \(\beta_1\) can be constructed using the
t-statistic, assuming that with large samples, the t-statistic is
approximately normally distributed. The 95% critical value of a
standard normal distribution is 1.96. Therefore, we can obtain the 95%
confidence interval for \(\beta_1\) by the following steps
</p>
<div class="LaTeX">
\begin{gather*}
-1.96 \leq \frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)} \leq 1.96 \\
\hat{\beta}_1 - 1.96 SE(\hat{\beta}_1) \leq \beta_1 \leq \hat{\beta}_1 + 1.96 SE(\hat{\beta}_1)
\end{gather*}

</div>

<p>
The 95% confidence interval for \(\beta_1\) is 
\[ \left[ \hat{\beta}_1 - 1.96 SE(\hat{\beta}_1),\; \hat{\beta}_1 + 1.96
SE(\hat{\beta}_1) \right] \]
</p>
</div>
</div>

<div id="outline-container-orgf9f49b6" class="outline-3">
<h3 id="orgf9f49b6"><span class="section-number-3">3.3</span> The application to test scores</h3>
<div class="outline-text-3" id="text-3-3">
<p>
In the application to test scores, given that \(\hat{\beta}_1 = -2.28\)
and \(SE(\hat{\beta}_1) = 0.52\), the 95% confidence interval for
\(\beta_1\) is \({-2.28 \pm 1.96 \times 0.52}\), or \(-3.30 \leq \beta_1
\leq -1.26\). Notice that the confidence interval only spans over the
negative region without going beyond zero, which implies that the null
hypothesis of \(\beta_1 = 0\) can be rejected at the 5% significance
level. 
</p>
</div>
</div>

<div id="outline-container-orgf03f2e9" class="outline-3">
<h3 id="orgf03f2e9"><span class="section-number-3">3.4</span> Confidence intervals for predicted effects of changing \(X\)</h3>
<div class="outline-text-3" id="text-3-4">
<p>
\(\beta_1\) is the marginal effect of \(X\) on \(Y\), that is, 
\[ \beta_1 = \frac{\dx Y}{ \dx X} \Rightarrow \dx Y = \beta_1 \dx X \]
When \(X\) changes by \(\Delta X\), \(Y\) changes by \(\beta_1 \Delta X\). 
</p>

<p>
So the 95% confidence interval for \(\beta_1 \Delta X\) is
\[ \left[ \hat{\beta}_1 \Delta X - 1.96 SE(\hat{\beta}_1) \Delta X,\;
\hat{\beta}_1 \Delta X + 1.96SE(\hat{\beta}_1) \Delta X \right] \]
</p>
</div>
</div>
</div>

<div id="outline-container-org02e0f9d" class="outline-2">
<h2 id="org02e0f9d"><span class="section-number-2">4</span> Regression When \(X\) is a Binary Variable</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-orge51d6a2" class="outline-3">
<h3 id="orge51d6a2"><span class="section-number-3">4.1</span> A binary variable</h3>
<div class="outline-text-3" id="text-4-1">
<p>
A <b>binary variable</b>  takes on values of one if some condition is true or zero
otherwise, which is also called a <b>dummy variable</b>, a <b>categorical
variable</b>, or an <b>indicator variable</b>. 
</p>

<p>
For example, 
</p>
\begin{equation*}
D_i = 
\begin{cases}
1,\; &\text{if the i\textsuperscript{th} subject is female} \\
0,\; &\text{if the i\textsuperscript{th} subject is male} 
\end{cases}
\end{equation*}

<p>
The linear regression model with a dummy variable as the regressor is
</p>
\begin{equation}
\label{eq:dummy-1}
Y_i = \beta_0 + \beta_1 D_i + u_i,\; i = 1, \ldots, n
\end{equation}

<p>
The coefficient on \(\beta_1\) is estimated by the OLS estimation method
in the same way as a continuous regressor. The difference lies in how
we interpret \(\beta_1\). 
</p>
</div>
</div>

<div id="outline-container-org29d557b" class="outline-3">
<h3 id="org29d557b"><span class="section-number-3">4.2</span> Interpretation of the regression coefficients</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Given that the assumption \(E(u_i | D_i) = 0\) holds in Equation
(\ref{eq:dummy-1}), we have two population regression functions for
the two cases, that is,
</p>
<ul class="org-ul">
<li>When \(D_i = 1\), \(E(Y_i|D_i = 1) = \beta_0 + \beta_1\)</li>
<li>When \(D_i = 0\), \(E(Y_i|D_i = 0) = \beta_0\)</li>
</ul>

<p>
Therefore, \(\beta_1 = E(Y_i | D_i = 1) - E(Y_i |D_i = 0)\), <b>the
difference in the population means</b> between two groups represented by
\(D_i = 1\) and \(D_i = 0\), respectively.
</p>
</div>
</div>

<div id="outline-container-org74e44c1" class="outline-3">
<h3 id="org74e44c1"><span class="section-number-3">4.3</span> Hypothesis tests and confidence intervals</h3>
<div class="outline-text-3" id="text-4-3">
<p>
The hypothesis tests and confidence intervals for the coefficient on a
binary variable follows the same procedure of those for a continuous
variable \(X\). 
</p>

<p>
Usually, the null and alternative hypotheses concerning a dummy variable are
\[ H_0:\, \beta_1 = 0 \text{ vs. } H_1:\, \beta_1 \neq 0 \]
Therefore, the t-statistic is 
\[ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} \]
And the 95% confidence interval is
\[ \hat{\beta}_1 \pm 1.96 SE(\hat{\beta}_1) \]
</p>
</div>
</div>
</div>

<div id="outline-container-org645f3ba" class="outline-2">
<h2 id="org645f3ba"><span class="section-number-2">5</span> Heteroskedasticity and Homoskedasticity</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-orgaaecacd" class="outline-3">
<h3 id="orgaaecacd"><span class="section-number-3">5.1</span> What are heteroskedasticity and homoskedasticity?</h3>
<div class="outline-text-3" id="text-5-1">
</div><div id="outline-container-org6561c65" class="outline-4">
<h4 id="org6561c65">Homoskedasticity</h4>
<div class="outline-text-4" id="text-org6561c65">
<p>
The error term \(u_i\) is <b>homoskedastic</b> if the conditional variance of
\(u_i\) given \(X_i\) is constant for \(i = 1, \ldots, n\). Mathematically,
it says \(\var(u_i | X_i) = \sigma^2,\, \text{ for } i = 1, \ldots, n\),
i.e., the variance of \(u_i\) for all $i$'s is a constant and does not
depend on \(X_i\).
</p>
</div>
</div>

<div id="outline-container-org92048f0" class="outline-4">
<h4 id="org92048f0">Heteroskedasticity</h4>
<div class="outline-text-4" id="text-org92048f0">
<p>
In contrast, the error term \(u_i\) is <b>heteroskedastic</b> if the conditional variance of
\(u_i\) given \(X_i\) changes on \(X_i\) for \(i = 1, \ldots, n\). That is,
\(\var(u_i | X_i) = \sigma^2_i,\, \text{ for } i = 1, \ldots, n\). 
</p>

<p>
e.g.. A multiplicative form of heteroskedasticity is \(\var(u_i|X_i)
= \sigma^2 f(X_i)\) where \(f(X_i)\) is a function of \(X_i\), for
example, \(f(X_i) = X_i\) as a simplest case. 
</p>

<p>
See Figure \ref{fig:homovshetero} for a visual comparison between
homoskedasticity and heteroskedasticity. 
</p>

<div class="LaTeX">
\begin{figure}
    \centering
    \begin{subfigure}[!ht]{0.85\textwidth}
        \includegraphics[width=\textwidth]{./figure/fig-4-4}
        \caption{Homoskedasticity}
        \label{fig:homo1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[!ht]{0.85\textwidth}
        \includegraphics[width=\textwidth]{./figure/fig-5-2}
        \caption{Heteroskedasticity}
        \label{fig:hetero1}
    \end{subfigure}
    \caption{Homoskedasticity Versus Heteroskedasticity}\label{fig:homovshetero}
\end{figure}

</div>
</div>
</div>
</div>

<div id="outline-container-org6f35f7a" class="outline-3">
<h3 id="org6f35f7a"><span class="section-number-3">5.2</span> Mathematical implications of homoskedasticity</h3>
<div class="outline-text-3" id="text-5-2">
</div><div id="outline-container-org424f6ce" class="outline-4">
<h4 id="org424f6ce">Unbiasedness, consistency, and the asymptotic distribution</h4>
<div class="outline-text-4" id="text-org424f6ce">
<p>
As long as the least squares assumptions holds, whether the error
term, \(u_i\), is homoskedastic or heteroskedastic does not affect
unbiasedness, consistency, and the asymptotic normal distribution
of the OLS estimators.
</p>
<ul class="org-ul">
<li>The unbiasedness requires that \(E(u_i|X_i) = 0\)</li>
<li>The consistency requires that \(E(X_i u_i) = 0\), which is true if
\(E(u_i|X_i)=0\).</li>
<li>The asymptotic normal distribution requires additionally that
\(\var((X_i-\mu_X)u_i) < \infty\), which still holds as long as
Assumption 3 holds, that is, no extreme outliers of \(X_i\).</li>
</ul>
</div>
</div>
<div id="outline-container-orgd5d7d7e" class="outline-4">
<h4 id="orgd5d7d7e">Efficiency</h4>
<div class="outline-text-4" id="text-orgd5d7d7e">
</div><ul class="org-ul"><li><a id="orgf476e66"></a>What means "being efficient"<br  /><div class="outline-text-5" id="text-orgf476e66">
<p>
The existence of heteroskedasticity affects the efficiency of the
OLS estimator
</p>
<ul class="org-ul">
<li>Suppose \(\hat{\beta}_1\) and \(\tilde{\beta}_1\) are both unbiased
estimators of \(\beta_1\). Then, \(\hat{\beta}_1\) is said to be more
<b>efficient</b> than \(\tilde{\beta}_1\) if \(\var(\hat{\beta}_1) <
  \var(\tilde{\beta}_1)\).</li>
<li>When the errors are homoskedastic, the OLS estimators
\(\hat{\beta}_0\) and \(\hat{\beta}_1\) are efficient among all
estimators that are linear in \(Y_1, \ldots, Y_n\) and are unbiased,
conditional on \(X_1, \ldots, X_n\).</li>

<li>See the Gauss-Markov Theorem below.</li>
</ul>
</div></li></ul>
</div>
</div>

<div id="outline-container-org895a525" class="outline-3">
<h3 id="org895a525"><span class="section-number-3">5.3</span> The homoskedasticity-only variance formula</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Recall that we can write \(\hat{\beta}_1\) as
</p>
<div class="LaTeX">
\begin{equation*}
\hat{\beta}_1 = \beta_1 + \frac{\sum_i (X_i - \bar{X})u_i}{\sum_i
(X_i - \bar{X})^2} 
\end{equation*} 

</div>

<p>
Therefore, if $u_i$'s are
homoskedastic and \(\sigma^2\) is known, then
</p>
<div class="LaTeX">
\begin{equation}
\label{eq:vbeta-1a} \var(\hat{\beta}_1 | X_i) = \frac{\sum_i (X_i -
\bar{X})^2 \var(u_i|X_i)}{\left[\sum_i (X_i - \bar{X})^2\right]^2} =
\frac{\sigma^2}{\sum_i (X_i - \bar{X})^2} 
\end{equation} 

</div>

<p>
When \(\sigma^2\) is unknown, then we use \(s^2_u = 1/(n-2) \sum_i
\hat{u}_i^2\) as an estimator of \(\sigma^2\). Thus, the
homoskedasticity-only estimator of the variance of \(\hat{\beta}_1\) is
</p>
<div class="LaTeX">
\begin{equation}
\label{eq:vbeta-1b} \tilde{\sigma}^2_{\hat{\beta}_1} =
\frac{s^2_u}{\sum_i (X_i - \bar{X})^2} 
\end{equation} 

</div>

<p>
And the homoskedasticity-only standard error is \(SE(\hat{\beta}_1) =
\sqrt{\tilde{\sigma}^2_{\hat{\beta}_1}}\).
</p>

<p>
Recall that the heteroskedasticity-robust standard error is
</p>
<div class="LaTeX">
\begin{equation*}
SE(\hat{\beta}_1) = \sqrt{\hat{\sigma}^2_{\hat{\beta}_1}}
\end{equation*} 

</div>
<p>
where
</p>
<div class="LaTeX">
\begin{equation*}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\frac{1}{n-2}
\sum_{i=1}^n (X_i - \bar{X})^2 \hat{u}^2_i}{\left[ \frac{1}{n}
\sum_{i=1}^n (X_i - \bar{X})^2 \right]^2} 
\end{equation*} 

</div>
<p>
which is also referred to as Eicker-Huber-White standard errors.
</p>
</div>
</div>

<div id="outline-container-org2c602c8" class="outline-3">
<h3 id="org2c602c8"><span class="section-number-3">5.4</span> What does this mean in practice?</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li>Heteroskedasticity is common in cross-sectional data. If you do not
have strong beliefs in homoskedasticity, then it is always safer to
report the heteroskedasticity-robust standard errors and use these
to compute the robust t-statistic.</li>
<li>In most software, the default setting is to report the
homoskedasticity-only standard errors. Therefore, you need to
manually add the option for the robust estimation. 

<ul class="org-ul">
<li><p>
In R, you can use the following codes
</p>
<pre class="example">
library(AER)
model1 &lt;- lm(testscr ~ str, data = classdata)
coeftest(model1, vcov = vcovHC(model1, type="HC1"))
</pre></li>

<li><p>
In STATA, you can use
</p>
<pre class="example">
regress testscr str, robust
</pre></li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org570d704" class="outline-2">
<h2 id="org570d704"><span class="section-number-2">6</span> The Theoretical Foundations of Ordinary Least Squares</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-orgf06f215" class="outline-3">
<h3 id="orgf06f215"><span class="section-number-3">6.1</span> The Gauss-Markov conditions</h3>
<div class="outline-text-3" id="text-6-1">
<p>
We have already known the least squares assumptions: for \(i = 1,
\ldots, n\), (1) \(E(u_i|X_i) =
0\), (2) \((X_i, Y_i)\) are i.i.d., and (3) large outliers are unlikely. 
</p>

<p>
The Gauss-Markov conditions provide anther version of these
assumptions plus the assumption of homoskedastic errors. 
</p>
</div>

<div id="outline-container-orgc145065" class="outline-4">
<h4 id="orgc145065">The Gauss-Markov conditions</h4>
<div class="outline-text-4" id="text-orgc145065">
<p>
For \(\mathbf{X} = [X_1, \ldots, X_n]\) <sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>
</p>
<ol class="org-ol">
<li>\(E(u_i| \mathbf{X}) = 0\)</li>
<li>\(\var(u_i | \mathbf{X}) = \sigma^2_u,\, 0 < \sigma^2_u < \infty\)</li>
<li>\(E(u_i u_j | \mathbf{X}) = 0,\, i \neq j\)</li>
</ol>
</div>
</div>

<div id="outline-container-org0343bf2" class="outline-4">
<h4 id="org0343bf2">From the three Least Squares Assumptions and the homoskedasticity assumption to the Gauss-Markov conditions</h4>
<div class="outline-text-4" id="text-org0343bf2">
<p>
Note that the conditional expectations in the G-M conditions are in
terms of all observations \(\mathbf{X}\), not just one observation,
\(X_i\). However, all the G-M conditions can be derived from the least
squares assumptions plus the homoskedasticity assumption. Specifically,
</p>

<ul class="org-ul">
<li>Assumptions (1) and (2) imply \(E(u_i | \mathbf{X}) = E(u_i | X_i) =
  0\).</li>
<li>Assumptions (1) and (2) imply \(\var(u_i| \mathbf{X}) =
  \var(u_i | X_i)\). With the homoskedasticity assumption, \(\var(u_i |
  X_i) = \sigma^2_u\), Assumption (3) then implies \(0 < \sigma^2_u < \infty\).</li>
<li>Assumptions (1) and (2) imply that \(E(u_i u_j | \mathbf{X}) = E(u_i
  u_j | X_i, X_j) = E(u_i|X_i) E(u_j|X_j) = 0\).</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org96461c2" class="outline-3">
<h3 id="org96461c2"><span class="section-number-3">6.2</span> Linear conditionally unbiased estimator</h3>
<div class="outline-text-3" id="text-6-2">
</div><div id="outline-container-org3537fc0" class="outline-4">
<h4 id="org3537fc0">The general form of a linear conditionally unbiased estimator of \(\beta_1\)</h4>
<div class="outline-text-4" id="text-org3537fc0">
<p>
The class of linear conditionally unbiased estimators consists of all
estimators of \(\beta_1\) that are linear function of \(Y_i, \ldots, Y_n\)
and that are unbiased, conditioned on \(X_1, \ldots, X_n\). 
</p>

<p>
For any linear estimator \(\tilde{\beta}_1\), it can be written as
</p>
\begin{equation}
\label{eq:beta1-tilde}
\tilde{\beta}_1 = \sum_{i=1}^n a_i Y_i\
\end{equation}
<p>
where the weights \(a_i\) for \(i = 1, \ldots, n\) depend on \(X_1, \ldots,
X_n\) but not on \(Y_1, \ldots, Y_n\). 
</p>

<p>
\(\tilde{\beta}_1\) is conditionally unbiased means that
</p>
\begin{equation}
\label{eq:e-beta1-tilde}
E(\tilde{\beta}_1 | \mathbf{X}) = \beta_1\
\end{equation}

<p>
By the Gauss-Markov conditions, from Equation (\ref{eq:beta1-tilde}),  we can have
</p>
\begin{equation*}
\begin{split}
E(\tilde{\beta}_1 | \mathbf{X}) &= \sum_i a_i E(\beta_0 + \beta_1 X_i + u_i | \mathbf{X}) \\
&= \beta_0 \sum_i a_i + \beta_1 \sum_i a_i X_i
\end{split}
\end{equation*}

<p>
For Equation (\ref{eq:e-beta1-tilde}) being satisfied with any
\(\beta_0\) and \(\beta_1\), we must have
\[ \sum_i a_i = 0 \text{ and } \sum_i a_iX_i = 1 \]
</p>
</div>
</div>

<div id="outline-container-org8eccb1b" class="outline-4">
<h4 id="org8eccb1b">The OLS esimator \(\hat{\beta}_1\) is a linear conditionally unbiased estimator</h4>
<div class="outline-text-4" id="text-org8eccb1b">
<p>
We have known that \(\hat{\beta}_1\) is unbiased both conditionally and
unconditionally. Next, we show that it is linear. 
\[ \hat{\beta}_1 = \frac{\sum_i (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_i
(X_i - \bar{X})^2} = \frac{\sum_i (X_i - \bar{X})Y_i}{\sum_i
(X_i - \bar{X})^2} = \sum_i \hat{a}_i Y_i \]
where the weights are
\[ \hat{a}_i = \frac{X_i - \bar{X}}{\sum_i (X_i - \bar{X})^2}, \text{
for } i = 1, \ldots, n \] 
Since \(\hat{\beta}_1\) is a linear conditionally unbiased estimator, we
must have
\[ \sum_i \hat{a}_i = 0 \text{ and } \sum_i \hat{a}_i X_i = 1  \]
which can be simply verified.
</p>
</div>
</div>
</div>

<div id="outline-container-org78695f7" class="outline-3">
<h3 id="org78695f7"><span class="section-number-3">6.3</span> The Gauss-Markov Theorem</h3>
<div class="outline-text-3" id="text-6-3">
<p>
The Gauss-Markov Theorem for \(\hat{\beta}_1\) states
</p>
<blockquote id="The Gauss-Markov Theorem for $\tilde{\beta}_{1}$">
<p>
If the Gauss-Markov conditions hold, then the OLS estimator
\(\hat{\beta}_1\) is the *B*est (most efficient) *L*inear conditionally
*U*nbiased *E*stimator (BLUE).
</p>
</blockquote>

<p>
The theorem can also be applied to \(\hat{\beta}_0\).
</p>

<p>
The proof of the Gauss-Markov theorem is in Appendix 5.2. A key in
this proof is that we can rewrite the expression of any linear
conditionally unbiased estimator \(\tilde{\beta}_1\) as
\[ \tilde{\beta}_1 = \sum_i a_i Y_i = \sum_i (\hat{a}_i + d_i)Y_i =
\hat{\beta}_1 + \sum_i d_i Y_i \]
And the goal of
the proof is to show that
\[ \var(\hat{\beta}_1 | \mathbf{X}) \leq \var(\tilde{\beta}_1 |
\mathbf{X}) \]
The equality holds only when \(\tilde{\beta}_1 = \hat{\beta}_1\). 
</p>
</div>
</div>

<div id="outline-container-org798e613" class="outline-3">
<h3 id="org798e613"><span class="section-number-3">6.4</span> The limitations of the Gauss-Markov theorem</h3>
<div class="outline-text-3" id="text-6-4">
<ol class="org-ol">
<li><p>
The Gauss-Markov conditions may hold in practice. Any violation of
the Gauss-Markov conditions will result in the OLS estimator not
being BLUE. The table below summarizes the cases in which a kind of
violation occurs, the consequences of such violation to the OLS
estimators, and possible remedies.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Summary of Violations of the Gauss-Markov Theorem</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Violation</th>
<th scope="col" class="org-left">Cases</th>
<th scope="col" class="org-left">Consequences</th>
<th scope="col" class="org-left">Remedies</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(E(u \mid X) \neq 0\)</td>
<td class="org-left">omitted variables, endogeneity</td>
<td class="org-left">biased</td>
<td class="org-left">more $X$'s, IV method</td>
</tr>

<tr>
<td class="org-left">\(\var(u_i\mid X)\) not constant</td>
<td class="org-left">heteroskedasticity</td>
<td class="org-left">inefficient</td>
<td class="org-left">WLS, GLS, HCCME</td>
</tr>

<tr>
<td class="org-left">\(E(u_{i}u_{j}\mid X) \neq 0\)</td>
<td class="org-left">autocorrelation</td>
<td class="org-left">inefficient</td>
<td class="org-left">GLS, HAC</td>
</tr>
</tbody>
</table></li>

<li>There are other candidate estimators that are not linear and
conditionally unbiased; under some conditions, these estimators are
more efficient than the OLS estimators.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-orgac99434" class="outline-2">
<h2 id="orgac99434"><span class="section-number-2">7</span> Using the t-Statistic in Regression When the Sample Size is Small</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-orgd8f1153" class="outline-3">
<h3 id="orgd8f1153"><span class="section-number-3">7.1</span> The classical assumptions of the least squares estimation</h3>
<div class="outline-text-3" id="text-7-1">
<p>
We first expand the OLS assumptions by two additional ones. One is the
assumption of the homoskedastic errors, and another one is the
assumption that the conditional distribution of \(u_i\) given \(X_i\) is
the normal distribution, i.e., \(u_i \sim N(0,
\sigma^2_u) \text{ for } i = 1, \ldots, n\). 
</p>

<p>
All these assumptions together are often referred to as the classical
assumptions of the least squares estimation. 
For \(i = 1, 2, \ldots, n\)
</p>
<ul class="org-ul">
<li>Assumption 1: \(E(u_i | X_i) = 0\) (exogeneity of \(X\))</li>
<li>Assumption 2: \((X_i, Y_i)\) are i.i.d. (IID of \(X, Y, \text{ and }
                   u\))</li>
<li>Assumption 3: \(0 < E(X_i^4) < \infty\) and \(0 < E(Y_i^4) < \infty\)
(No large outliers)</li>
<li>Extended Assumption 4: \(\var(u_i | X_i) = \sigma^2_u, \text{ and } 0 <
                   \sigma^2_u < \infty\) (homoskedasticity)</li>
<li>Extended Assumption 5: \(u_i | X_i \sim N(0, \sigma^2_u)\) (normality)</li>
</ul>
</div>
</div>

<div id="outline-container-org253ab03" class="outline-3">
<h3 id="org253ab03"><span class="section-number-3">7.2</span> The t-Statistic and the Student-t Distribution</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Under all the classical assumptions, we can construct the
t-statistic for hypothesis testing of a single coefficient. Even with
a small samples, the t-statistic has an exact Student-t distribution. 
</p>
</div>

<div id="outline-container-orgd3664a4" class="outline-4">
<h4 id="orgd3664a4">The t-statistic is for \(\beta_1\)</h4>
<div class="outline-text-4" id="text-orgd3664a4">
<p>
\[H_0: \beta_1 = \beta_{1,0} \text{ vs } H_1: \beta_1 \neq \beta_{1,0}\]
</p>
\begin{equation}
t = \frac{\hat{\beta}_1 - \beta_{1,0}}{\hat{\sigma}_{\hat{\beta}_1}}
\end{equation}
<p>
where
</p>
\begin{equation*}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{s^2_u}{\sum_i (X_i - \bar{X})^2} \text{ and } s^2_u = \frac{1}{n-2}\sum_i \hat{u}_i^2
\end{equation*}
<p>
the former of which is the homoskedasticity-only standard error of
\(\hat{\beta}_1\) and the latter is the standard error of the
regression. 
</p>
</div>
</div>

<div id="outline-container-orgc6fa66f" class="outline-4">
<h4 id="orgc6fa66f">The Student-t distribution of \(t\)</h4>
<div class="outline-text-4" id="text-orgc6fa66f">
<p>
The t statistic can be rewritten as
</p>
\begin{equation}
\label{eq:t-stat-b1a}
t = \frac{(\hat{\beta}_1 - \beta_{1,0})/\sigma_{\hat{\beta}_1}}{\sqrt{\frac{\hat{\sigma}^2_{\hat{\beta}_1}}{\sigma^2_{\hat{\beta}_1}}}} 
= \frac{z_{\hat{\beta}_1}}{\sqrt{\frac{s^2_u}{\sigma^2_u}}} = \frac{z_{\hat{\beta}_1}}{\sqrt{\frac{W}{n-2}}}
\end{equation}
<p>
where 
</p>

<p>
\[\sigma^2_{\hat{\beta}_1} = \frac{\sigma^2_u}{\sum_i (X_i -
\bar{X})^2} \] 
</p>

<p>
is the homoskedasticity-only variance of
\(\hat{\beta}_1\) when the variance of errors \(\sigma^2_u\) is known.  
</p>

<p>
\[
z_{\hat{\beta}_1} =\frac{\hat{\beta}_1 -
\beta_{1,0}}{\sigma_{\hat{\beta}_1}} 
\] 
</p>

<p>
is the z-statistic which has a standard normal distribution, that is,
\(z_{\hat{\beta}_1} \sim N(0, 1)\)
</p>

<p>
\[ 
W = (n-2)\frac{s^2_u}{\sigma^2_u} =
\frac{\sum_i\hat{u}_i^2}{\sigma^2_u} = \sum_i
\left(\frac{\hat{u}_i}{\sigma_u}\right)^2
 \] 
</p>

<p>
It can be shown that W is the sum of squares of \((n-2)\) independent
standard normally distributed variables, which results in a
chi-squared distribution with \((n-2)\) degrees of freedom. That is, \(W
\sim \chi^2(n-2)\), which is also independent of
\(z_{\hat{\beta}_1}\). Therefore, the t-statistic in Equation
(\ref{eq:t-stat-b1a}), as the ratio of \(z_{\hat{\beta}_1}\) and
\(\sqrt{W/(n-2)}\), is distributed as \(t(n-2)\).
</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara">In a small
sample case, the exact distribution of the t-statistics is the
student-t distribution with \(n-1\) degree of freedom.</div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara">Note that the trick here is we put the
desired hypothesis to the alternative place.</div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara">Here I use the vector
notation to represent all observations of \(X_i\) for \(i=1, \ldots,
n\). We will formally introduce the matrix notation for a linear
regression model and the OLS estimation in the next lecture.</div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Zheng Tian</p>
<p class="date">Created: 2017-03-21 Tue 20:16</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
