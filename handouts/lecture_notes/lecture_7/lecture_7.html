<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-03-22 Wed 08:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Lecture 7: Hypothesis Test and Confidence Intervals of Linear Regression with a Single Regressor</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Zheng Tian" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="../../../css/readtheorg.css" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Lecture 7: Hypothesis Test and Confidence Intervals of Linear Regression with a Single Regressor</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org43decdb">1. Introduction</a></li>
<li><a href="#org240e8fc">2. Testing Hypotheses about One of the Regression Coefficients</a></li>
<li><a href="#orgca0f7a5">3. Confidence Intervals for a Regression Coefficient</a></li>
<li><a href="#orgcad772a">4. Regression When \(X\) is a Binary Variable</a></li>
<li><a href="#orgec751cf">5. Heteroskedasticity and Homoskedasticity</a></li>
<li><a href="#org9346ae3">6. The Theoretical Foundations of Ordinary Least Squares</a></li>
<li><a href="#org6a9feaf">7. Using the t-Statistic in Regression When the Sample Size is Small</a></li>
</ul>
</div>
</div>
\(
 \newcommand{\dx}{\mathrm{d}}
 \newcommand{\var}{\mathrm{Var}}
 \newcommand{\cov}{\mathrm{Cov}}
 \newcommand{\corr}{\mathrm{Corr}}
 \newcommand{\pr}{\mathrm{Pr}}
 \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
 \renewcommand\chaptername{Lecture}
 \DeclareMathOperator*{\plim}{plim}
 \newcommand{\plimn}{\plim_{n \rightarrow \infty}}
\)


<div id="outline-container-org43decdb" class="outline-2">
<h2 id="org43decdb"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
This chapter consists of two parts. The first part concerns hypothesis
testing for a single coefficient in a simple linear regression
model. The basic concepts and ideas of hypothesis testing in this
chapter can be naturally adopted in multiple regression models
(Chapters 6 and 7). The second part goes back to some estimation
issues, including a binary regressor, homoskedasticity versus
heteroskedasticity, as well as the Gauss-Markov theorem, one of the
most fundamental theories regarding the OLS estimation. Finally,
this chapter ends up with the small sample properties of the
t-statistics.
</p>

<p>
One of the features of this textbook is that it introduces the
heteroskedasticity-robust standard error of the OLS estimators, which
is considered as a general case and homoskedasticity as a special
case. This is contrary to the common layouts of an Econometrics
textbook that often first gives the assumption of homoskedasticity,
which is a component of the classical OLS assumptions (equivalent to
the three least squares assumptions plus the assumption of the
homoskedastic and conditionally normally distributed errors). Then
treat heteroskedasticity as a violation to these assumptions. Also,
you should be aware that most discussions of the sample distributions
in this textbook are in the context of a large sample, while the small
sample statistical properties are not the focus.
</p>
</div>
</div>


<div id="outline-container-org240e8fc" class="outline-2">
<h2 id="org240e8fc"><span class="section-number-2">2</span> Testing Hypotheses about One of the Regression Coefficients</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-orgd362eb0" class="outline-3">
<h3 id="orgd362eb0"><span class="section-number-3">2.1</span> A brief review of basic concepts in hypothesis tests</h3>
<div class="outline-text-3" id="text-2-1">
<p>
Let's quickly review what we have learned in Lecture 3 about
hypothesis tests, taking an example of testing the true value of the
population mean from a random sample.
</p>
</div>

<div id="outline-container-org5e6e74a" class="outline-4">
<h4 id="org5e6e74a">The null versus alternative hypotheses</h4>
<div class="outline-text-4" id="text-org5e6e74a">
<p>
We want to test two contrasting hypotheses, the null hypothesis versus
the alternative hypothesis. 
</p>

<ul class="org-ul">
<li>Two-sided tests: 
\[H_0:\; E(Y) = \mu_{Y,0}$ v.s. $H_1:\; E(Y) \neq \mu_{Y,0}\]</li>

<li>One-sided test: 
\[H_0:\; E(Y) = \mu_{Y,0}$ v.s. $H_1:\; E(Y) > \mu_{Y,0}\]</li>
</ul>
</div>
</div>

<div id="outline-container-org211a1d7" class="outline-4">
<h4 id="org211a1d7">Test statistics</h4>
<div class="outline-text-4" id="text-org211a1d7">
<p>
We need some tools for the testing, which is referred to as test
statistics.
</p>

<ul class="org-ul">
<li>When \(\sigma_Y\) is known, we use the z-statistics
\[ z = \frac{\overline{Y} -
  \mu_{Y,0}}{\sigma_{\overline{Y}}} = \frac{\overline{Y} -
  \mu_{Y,0}}{\sigma_Y/\sqrt{n}} \xrightarrow{\text{ d }} N(0, 1)\]</li>

<li><p>
When \(\sigma_Y\) is unknown, we use the standard error of
\(\overline{Y}\) and compute the t-statistic<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
</p>

<p>
\[ t = \frac{\overline{Y} - \mu_{Y,0}}{SE(\overline{Y})} =
  \frac{\overline{Y} - \mu_{Y,0}}{s_Y/\sqrt{n}} \xrightarrow{ \text{ d } } N(0, 1) \] 
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orgd50d15f" class="outline-4">
<h4 id="orgd50d15f">The rules for hypothesis testing</h4>
<div class="outline-text-4" id="text-orgd50d15f">
<p>
We need to set up some rules for judging that under what
circumstances, the null hypothesis is rejected or fails
to be rejected. 
</p>
</div>

<ul class="org-ul"><li><a id="org3b15be5"></a>Type I and type II errors<br  /><div class="outline-text-5" id="text-org3b15be5">
<ul class="org-ul">
<li><b>Type I error</b>. The null hypothesis is rejected when in fact it is
true.</li>
<li><b>Type II error</b>. The null hypothesis is not rejected when in fact it
is false.</li>
</ul>
</div></li>

<li><a id="orga1f41d0"></a>The significance level, the critical value, and the p-value<br  /><div class="outline-text-5" id="text-orga1f41d0">
<ul class="org-ul">
<li><b>The significance level</b>. The pre-specified probability of type I
error.  \(\alpha = 0.05, 0.10, \text{ or } 0.01\)</li>

<li><p>
<b>The critical value</b>. The value of the test statistic for which the
test rejects the null hypothesis at the given significance level.
</p>

<p>
For example. In a two-sided test, with the z statistic. The critical
value at the 5% significance level is \(c_{\alpha}\) such that
\(\varPhi(c_{\alpha}) = 0.975\). Accordingly, we know \(c_{\alpha}
  \approx 1.96\).
</p></li>

<li><p>
<b>The p-value</b>. The p-value is the probability of drawing a statistic
at least as adverse to the null hypothesis as the one you actually
computed in your sample, assuming the null hypothesis is
correct. 
</p>

<p>
Equivalently, the p-value is the smallest significance
level at which the null hypothesis could be rejected, based on the
test statistic actually computed. 
</p>

<p>
Mathematically, the p-value is 
\[  \pr_{H_0}\left( \left| \frac{\overline{Y} - \mu_{Y,0}}{SE(\overline{Y})}
  \right| > \left| \frac{\overline{Y}^{act} - \mu_{Y,0}}{SE(\overline{Y})} \right| \right) =
  2\varPhi(-|t^{act}|) \text{ .} \]
</p></li>
</ul>
</div></li>

<li><a id="org169d94f"></a>Rejection rules<br  /><div class="outline-text-5" id="text-org169d94f">
<p>
The following two statements are equivalent in terms of rejecting the
null hypothesis at the 5% significance level. 
</p>

<ul class="org-ul">
<li>We can reject the null if the test statistics falls into the
rejection region delimited by the critical values at the 5%
significance level, that is, when \(|t^{act}| > c_{\alpha} = 1.96\),</li>

<li>We can reject the null if the p-value is less than the significance
level that is 5% in this case.</li>
</ul>

<p>
The rejection rule can be illustrated using Figure <a href="#org73d5b0b">1</a>.
</p>


<div id="org73d5b0b" class="figure">
<p><img src="./figure/fig9_1.png" alt="fig9_1.png" />
</p>
<p><span class="figure-number">Figure 1: </span>An illustration of a two-sided test</p>
</div>
</div></li></ul>
</div>
</div>


<div id="outline-container-org08a2ea9" class="outline-3">
<h3 id="org08a2ea9"><span class="section-number-3">2.2</span> Two-sided hypotheses concerning \(\beta_1\)</h3>
<div class="outline-text-3" id="text-2-2">
</div><div id="outline-container-orgfdc848e" class="outline-4">
<h4 id="orgfdc848e">Application to test scores</h4>
<div class="outline-text-4" id="text-orgfdc848e">
<p>
In the last lecture, we estimate a simple linear regression model for test
scores and class sizes, which yields the following estimated sample
regression function,
</p>

\begin{equation}
\label{eq:testscr-str-1e}
\widehat{TestScore} = 698.93 - 2.28 \times STR
\end{equation}

<p>
Now the question faced by the superintendent of the California
elementary school districts is whether the estimated coefficient on
<i>STR</i> is valid. In the terminology of statistics, his question is
whether \(\beta_1\) is statistically significantly different from zero. 
</p>
</div>
</div>

<div id="outline-container-orgfc28592" class="outline-4">
<h4 id="orgfc28592">Testing hypotheses about the slope \(\beta_1\)</h4>
<div class="outline-text-4" id="text-orgfc28592">
<p>
Note that all discussions about hypothesis testing that
follows involve only the regression with a large sample size. The
last section of this lecture touches upon the small sample properties
of the test statistics.
</p>
</div>

<ul class="org-ul"><li><a id="org7e0e689"></a>The two-sided hypothesis<br  /><div class="outline-text-5" id="text-org7e0e689">
<p>
\[ H_0: \beta_1 = \beta_{1,0} \text{ vs. } H_1: \beta_1 \neq \beta_{1,0} \]
</p>

<p>
The null hypothesis is that \(\beta_1\) is equal to a specific value
\(\beta_{1,0}\), and the alternative hypothesis is the opposite. 
</p>
</div></li>

<li><a id="org5550c9e"></a>The t-statistic<br  /><div class="outline-text-5" id="text-org5550c9e">
<p>
The general form of the t-statistic is
</p>

\begin{equation}
\label{eq:general-t}
t = \frac{\text{estimator} - \text{hypothesized value}}{\text{standard error of the estimator}}
\end{equation}

<p>
The t-statistics for testing \(\beta_1\) is then
</p>

\begin{equation}
\label{eq:t-stat-b1}
t = \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}
\end{equation}
</div></li>

<li><a id="org7c84c6d"></a>The standard error of \(\hat{\beta}_1\) is calculated as<br  /><div class="outline-text-5" id="text-org7c84c6d">
\begin{equation}
\label{eq:se-b-1}
SE(\hat{\beta}_1) = \sqrt{\hat{\sigma}^2_{\hat{\beta}_1}}
\end{equation}
<p>
where
</p>
\begin{equation}
\label{eq:sigma-b-1}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\frac{1}{n-2} \sum_{i=1}^n (X_i - \bar{X})^2 \hat{u}^2_i}{\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right]^2}
\end{equation}
</div></li>

<li><a id="orgcf17bc7"></a>How to understand Equation \ref{eq:sigma-b-1}<br  /><div class="outline-text-5" id="text-orgcf17bc7">
<ul class="org-ul">
<li>The population variance of \(\beta_1\) is 
\[ \sigma^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\var\left( (X_i - \mu_X)u_i \right)}{\left( \var(X_i) \right)^2} \]</li>

<li>The denominator in Equation (\ref{eq:sigma-b-1}) is a consistent
estimator of \(\var(X_i)^2\).</li>

<li>The numerator in Equation (\ref{eq:sigma-b-1}) is a consistent
estimator of \(\var((X_i - \mu_X)u_i)\), adjusted by \(n-2\) degrees
of freedom.</li>

<li>The standard error computed from Equation (\ref{eq:sigma-b-1}) is
the <b>heteroskedasticity-robust standard error</b>, which will be
explained in detail shortly in this lecture.</li>
</ul>
</div></li>

<li><a id="org91efe44"></a>Compute the p-value<br  /><div class="outline-text-5" id="text-org91efe44">
<p>
The p-value is the probability of observing a value of \(\hat{\beta}_1\)
at least as different from \(\beta_{1,0}\) as the estimate actually
computed (\(\hat{\beta}^{act}_1\)), assuming that the null hypothesis is
correct. Accordingly, under the null hypothesis, the p-value for
testing \(\beta_1\) can be expressed with a probability function as
</p>

\begin{equation*}
\begin{split}
p\text{-value} &= \pr_{H_0} \left( | \hat{\beta}_1 - \beta_{1,0} | > | \hat{\beta}^{act}_1 - \beta_{1,0} | \right) \\
&= \pr_{H_0} \left( \left| \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \right| > \left| \frac{\hat{\beta}^{act}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \right| \right) \\
&= \pr_{H_0} \left( |t| > |t^{act}| \right)
\end{split}
\end{equation*}

<p>
With a large sample, \(p\text{-value} = \pr\left(|t| > |t^{act}|
\right) = 2 \varPhi(-|t^{act}|)\).
</p>

<p>
The null hypothesis is rejected at the 5% significance level if the
\(p\text{-value} < 0.05\) or, equivalently, \(|t^{act}| > 1.96\). 
</p>
</div></li>

<li><a id="orge345729"></a>Application to test scores<br  /><div class="outline-text-5" id="text-orge345729">
<p>
The OLS estimation of the linear regression model of test scores
against student-teacher ratios, together with the standard errors of
all parameters in the model, can be represented using the following
equation, 
</p>

\begin{equation*}
\widehat{TestScore} = \underset{\displaystyle (10.4)}{698.9} - \underset{\displaystyle (0.52)}{2.28} \times STR,\; R^2 = 0.051,\; SER = 1.86
\end{equation*}

<p>
The <b>heteroskedasticity-robust</b> standard errors are reported in the
parentheses, that is, \(SE(\hat{\beta}_0) = 10.4\) and
\(SE(\hat{\beta}_1) = 0.52\). 
</p>

<p>
The superintendent's question is whether \(\beta_1\) is significant for
which we can test the null hypothesis against the alternative one as
\[ H_0: \beta_1 = 0, H_1: \beta_1 \neq 0 \]
</p>

<p>
The t-statistics is
\[ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} = \frac{-2.28}{0.52}
= -4.38 < -1.96 \] 
</p>

<p>
The p-value associated with \(t^{act} = -4.38\) is approximately
0.00001, which is far less than 0.05. 
</p>

<p>
Based on the t-statistics and the p-value, we can say the null
hypothesis is rejected at the 5% significance level. In English, it
means that the student-teacher ratios do have a significant effect on
test scores. 
</p>


<div id="orgea45c28" class="figure">
<p><img src="figure/fig-5-1.png" alt="fig-5-1.png" width="600" />
</p>
<p><span class="figure-number">Figure 2: </span>Calculating the p-value of a two-sided test when \(t^{act}=-4.38\)</p>
</div>
</div></li></ul>
</div>
</div>


<div id="outline-container-org155c9ef" class="outline-3">
<h3 id="org155c9ef"><span class="section-number-3">2.3</span> The one-sided alternative hypothesis</h3>
<div class="outline-text-3" id="text-2-3">
</div><div id="outline-container-org786e100" class="outline-4">
<h4 id="org786e100">The one-sided hypotheses</h4>
<div class="outline-text-4" id="text-org786e100">
<p>
In some cases, it is appropriate to use a one-sided hypothesis
test. For example, the superintendent of the California school
districts want to know whether class sizes have a negative effect on
test scores, that is, \(\beta_1 < 0\). 
</p>

<p>
For a one-sided test, the null hypothesis and the one-sided
alternative hypothesis are <sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>
</p>

<p>
\[ H_0: \beta_1 = \beta_{1,0} \text{ vs. } H_1: \beta_1 < \beta_{1,0} \]
</p>
</div>
</div>

<div id="outline-container-org82e9dfd" class="outline-4">
<h4 id="org82e9dfd">The one-sided left-tail test</h4>
<div class="outline-text-4" id="text-org82e9dfd">
<ul class="org-ul">
<li>The t-statistic is the same as in a two-sided test
\[ t = \frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)} \]</li>
<li>Since we test \(\beta_1 < \beta_{1,0}\), if this is true, the
t-statistics should be statistically significantly less than zero.</li>
<li>The p-value is computed as \(\pr(t < t^{act}) = \varPhi(t^{act})\).</li>
<li>The null hypothesis is rejected at the 5% significance level when
\(\text{p-value} < 0.05\) or \(t^{act} < -1.645\).</li>
<li>In the application of test scores, the t-statistics is -4.38, which
is less than -1.645 and -2.33 (the critical value for a one-sided
test with a 1% significance level). Thus, the null hypothesis is
rejected at the 1% level.</li>
</ul>
</div>
</div>
</div>
</div>


<div id="outline-container-orgca0f7a5" class="outline-2">
<h2 id="orgca0f7a5"><span class="section-number-2">3</span> Confidence Intervals for a Regression Coefficient</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-org5512acb" class="outline-3">
<h3 id="org5512acb"><span class="section-number-3">3.1</span> Two equivalent definitions of confidence intervals</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Recall that a 95% <b>confidence interval</b> for \(\beta_1\) has two equivalent
definitions:
</p>
<ol class="org-ol">
<li>It is the set of values that cannot be rejected using a two-sided
hypothesis test with a 5% significance level.</li>
<li>It is an interval that has a 95% probability of containing the true
value of \(\beta_1\).</li>
</ol>

<p>
Let's go back to Figure <a href="#org73d5b0b">1</a>. According to the first
definition, the acceptance region contains the values of the
test statistics that fail to reject the null hypothesis,
which corresponds to the values of \(\beta_1\) that cannot be rejected. 
</p>
</div>
</div>


<div id="outline-container-orgaffd03e" class="outline-3">
<h3 id="orgaffd03e"><span class="section-number-3">3.2</span> Construct the 95% confidence interval for \(\beta_1\)</h3>
<div class="outline-text-3" id="text-3-2">
<p>
The 95% confidence interval for \(\beta_1\) can be constructed using the
t-statistic, assuming that with large samples, the t-statistic is
approximately normally distributed. The 95% critical value of a
standard normal distribution is 1.96. Therefore, we can obtain the 95%
confidence interval for \(\beta_1\) by the following steps
</p>

\begin{gather*}
-1.96 \leq \frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)} \leq 1.96 \\
\hat{\beta}_1 - 1.96 SE(\hat{\beta}_1) \leq \beta_1 \leq \hat{\beta}_1 + 1.96 SE(\hat{\beta}_1)
\end{gather*}

<p>
The 95% confidence interval for \(\beta_1\) is 
\[ \left[ \hat{\beta}_1 - 1.96 SE(\hat{\beta}_1),\; \hat{\beta}_1 + 1.96
SE(\hat{\beta}_1) \right] \]
</p>
</div>
</div>


<div id="outline-container-orgee831bc" class="outline-3">
<h3 id="orgee831bc"><span class="section-number-3">3.3</span> The application to test scores</h3>
<div class="outline-text-3" id="text-3-3">
<p>
In the application to test scores, given that \(\hat{\beta}_1 = -2.28\)
and \(SE(\hat{\beta}_1) = 0.52\), the 95% confidence interval for
\(\beta_1\) is \({-2.28 \pm 1.96 \times 0.52}\), or \(-3.30 \leq \beta_1
\leq -1.26\). 
</p>

<p>
Note that the confidence interval only spans over the negative
region with zero leaving outside the interval, which implies that the
null hypothesis of \(\beta_1 = 0\) can be rejected at the 5%
significance level.
</p>
</div>
</div>


<div id="outline-container-org76ad6e4" class="outline-3">
<h3 id="org76ad6e4"><span class="section-number-3">3.4</span> Confidence intervals for predicted effects of changing \(X\)</h3>
<div class="outline-text-3" id="text-3-4">
<p>
\(\beta_1\) is the marginal effect of \(X\) on \(Y\), that is, 
\[ \beta_1 = \frac{\dx Y}{ \dx X} \Rightarrow \dx Y = \beta_1 \dx X \]
When \(X\) changes by \(\Delta X\), \(Y\) changes by \(\beta_1 \Delta X\). 
</p>

<p>
So the 95% confidence interval for \(\beta_1 \Delta X\) is
\[ \left[ \hat{\beta}_1 \Delta X - 1.96 SE(\hat{\beta}_1) \Delta X,\;
\hat{\beta}_1 \Delta X + 1.96SE(\hat{\beta}_1) \Delta X \right] \]
</p>
</div>
</div>
</div>


<div id="outline-container-orgcad772a" class="outline-2">
<h2 id="orgcad772a"><span class="section-number-2">4</span> Regression When \(X\) is a Binary Variable</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-org0796d09" class="outline-3">
<h3 id="org0796d09"><span class="section-number-3">4.1</span> A binary variable</h3>
<div class="outline-text-3" id="text-4-1">
<p>
A <b>binary variable</b> takes on values of one if some condition is true
and zero otherwise, which is also called a <b>dummy variable</b>, a
<b>categorical variable</b>, or an <b>indicator variable</b>.
</p>

<p>
For example, 
</p>
\begin{equation*}
D_i = 
\begin{cases}
1,\; &\text{if the } i^{th} \text{ subject is female} \\
0,\; &\text{if the } i^{th} \text{ subject is male} 
\end{cases}
\end{equation*}

<p>
The linear regression model with a dummy variable as a regressor is
</p>
\begin{equation}
\label{eq:dummy-1}
Y_i = \beta_0 + \beta_1 D_i + u_i,\; i = 1, \ldots, n
\end{equation}

<p>
The coefficient on \(D_i\) is estimated by the OLS estimation method
in the same way as a continuous regressor. The difference lies in how
we interpret \(\beta_1\). 
</p>
</div>
</div>


<div id="outline-container-org552cc99" class="outline-3">
<h3 id="org552cc99"><span class="section-number-3">4.2</span> Interpretation of the regression coefficients</h3>
<div class="outline-text-3" id="text-4-2">
<p>
Given that the assumption \(E(u_i | D_i) = 0\) holds in Equation
(\ref{eq:dummy-1}), we have two population regression functions for
the two cases, that is,
</p>
<ul class="org-ul">
<li>When \(D_i = 1\), \(E(Y_i|D_i = 1) = \beta_0 + \beta_1\)</li>
<li>When \(D_i = 0\), \(E(Y_i|D_i = 0) = \beta_0\)</li>
</ul>

<p>
Therefore, \(\beta_1 = E(Y_i | D_i = 1) - E(Y_i |D_i = 0)\), that is,
<b>the difference in the population means</b> between two groups represented by
\(D_i = 1\) and \(D_i = 0\), respectively.
</p>
</div>
</div>


<div id="outline-container-orge8456ad" class="outline-3">
<h3 id="orge8456ad"><span class="section-number-3">4.3</span> Hypothesis tests and confidence intervals</h3>
<div class="outline-text-3" id="text-4-3">
<p>
The hypothesis tests and confidence intervals for the coefficient on a
binary variable follows the same procedure of those for a continuous
variable \(X\). 
</p>

<p>
Usually, the null and alternative hypotheses concerning a dummy variable are
\[ H_0:\, \beta_1 = 0 \text{ vs. } H_1:\, \beta_1 \neq 0 \]
Therefore, the t-statistic is 
\[ t = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} \]
And the 95% confidence interval is
\[ \hat{\beta}_1 \pm 1.96 SE(\hat{\beta}_1) \]
</p>
</div>
</div>
</div>


<div id="outline-container-orgec751cf" class="outline-2">
<h2 id="orgec751cf"><span class="section-number-2">5</span> Heteroskedasticity and Homoskedasticity</h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-org414e8e1" class="outline-3">
<h3 id="org414e8e1"><span class="section-number-3">5.1</span> What are heteroskedasticity and homoskedasticity?</h3>
<div class="outline-text-3" id="text-5-1">
</div><div id="outline-container-org2f1d6c8" class="outline-4">
<h4 id="org2f1d6c8">Homoskedasticity</h4>
<div class="outline-text-4" id="text-org2f1d6c8">
<p>
The error term \(u_i\) is <b>homoskedastic</b> if the conditional variance of
\(u_i\) given \(X_i\) is constant for \(i = 1, \ldots, n\). Mathematically,
it says \(\var(u_i | X_i) = \sigma^2,\, \text{ for } i = 1, \ldots, n\),
i.e., the variance of \(u_i\) for all <i>i</i> is a constant and does not
depend on \(X_i\).
</p>
</div>
</div>

<div id="outline-container-org3574a2a" class="outline-4">
<h4 id="org3574a2a">Heteroskedasticity</h4>
<div class="outline-text-4" id="text-org3574a2a">
<p>
In contrast, the error term \(u_i\) is <b>heteroskedastic</b> if the conditional variance of
\(u_i\) given \(X_i\) changes on \(X_i\) for \(i = 1, \ldots, n\). That is,
\(\var(u_i | X_i) = \sigma^2_i,\, \text{ for } i = 1, \ldots, n\). 
</p>

<p>
e.g.. A multiplicative form of heteroskedasticity is \(\var(u_i|X_i)
= \sigma^2 f(X_i)\) where \(f(X_i)\) is a function of \(X_i\), for
example, \(f(X_i) = X_i\) as a simplest case. 
</p>
</div>
</div>
</div>


<div id="outline-container-orgb742a67" class="outline-3">
<h3 id="orgb742a67"><span class="section-number-3">5.2</span> Mathematical implications of homoskedasticity</h3>
<div class="outline-text-3" id="text-5-2">
</div><div id="outline-container-orgd14df12" class="outline-4">
<h4 id="orgd14df12">Unbiasedness, consistency, and the asymptotic distribution</h4>
<div class="outline-text-4" id="text-orgd14df12">
<p>
As long as the least squares assumptions holds, whether the error
term, \(u_i\), is homoskedastic or heteroskedastic does not affect
unbiasedness, consistency, and the asymptotic normal distribution
of the OLS estimators.
</p>
<ul class="org-ul">
<li>The unbiasedness requires that \(E(u_i|X_i) = 0\)</li>
<li>The consistency requires that \(E(X_i u_i) = 0\), which is true if
\(E(u_i|X_i)=0\).</li>
<li>The asymptotic normal distribution requires additionally that
\(\var((X_i-\mu_X)u_i) < \infty\), which still holds as long as
Assumption 3 holds, that is, no extreme outliers of \(X_i\).</li>
</ul>
</div>
</div>

<div id="outline-container-org0335d99" class="outline-4">
<h4 id="org0335d99">Efficiency</h4>
<div class="outline-text-4" id="text-org0335d99">
<p>
The existence of heteroskedasticity affects the efficiency of the
OLS estimator
</p>
<ul class="org-ul">
<li>Suppose \(\hat{\beta}_1\) and \(\tilde{\beta}_1\) are both unbiased
estimators of \(\beta_1\). Then, \(\hat{\beta}_1\) is said to be more
<b>efficient</b> than \(\tilde{\beta}_1\) if \(\var(\hat{\beta}_1) <
  \var(\tilde{\beta}_1)\).</li>
<li>When the errors are homoskedastic, the OLS estimators
\(\hat{\beta}_0\) and \(\hat{\beta}_1\) are efficient among all
estimators that are linear in \(Y_1, \ldots, Y_n\) and are unbiased,
conditional on \(X_1, \ldots, X_n\).</li>

<li>See the Gauss-Markov Theorem below.</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org9a75b12" class="outline-3">
<h3 id="org9a75b12"><span class="section-number-3">5.3</span> The homoskedasticity-only variance formula</h3>
<div class="outline-text-3" id="text-5-3">
<p>
Recall that we can write \(\hat{\beta}_1\) as
</p>
\begin{equation*}
\hat{\beta}_1 = \beta_1 + \frac{\sum_i (X_i - \bar{X})u_i}{\sum_i
(X_i - \bar{X})^2} 
\end{equation*} 

<p>
Therefore, if \(u_i\) for \(i=1, \ldots, n\) is
homoskedastic and \(\sigma^2\) is known, then
</p>
\begin{equation}
\label{eq:vbeta-1a} \var(\hat{\beta}_1 | X_i) = \frac{\sum_i (X_i -
\bar{X})^2 \var(u_i|X_i)}{\left[\sum_i (X_i - \bar{X})^2\right]^2} =
\frac{\sigma^2}{\sum_i (X_i - \bar{X})^2} 
\end{equation} 

<p>
When \(\sigma^2\) is unknown, then we use \(s^2_u = 1/(n-2) \sum_i
\hat{u}_i^2\) as an estimator of \(\sigma^2\). Thus, the
homoskedasticity-only estimator of the variance of \(\hat{\beta}_1\) is
</p>
\begin{equation}
\label{eq:vbeta-1b} \tilde{\sigma}^2_{\hat{\beta}_1} =
\frac{s^2_u}{\sum_i (X_i - \bar{X})^2} 
\end{equation} 

<p>
And the homoskedasticity-only standard error is \(SE(\hat{\beta}_1) =
\sqrt{\tilde{\sigma}^2_{\hat{\beta}_1}}\).
</p>

<p>
Recall that the heteroskedasticity-robust standard error is
</p>
\begin{equation*}
SE(\hat{\beta}_1) = \sqrt{\hat{\sigma}^2_{\hat{\beta}_1}}
\end{equation*} 
<p>
where
</p>
\begin{equation*}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{1}{n} \frac{\frac{1}{n-2}
\sum_{i=1}^n (X_i - \bar{X})^2 \hat{u}^2_i}{\left[ \frac{1}{n}
\sum_{i=1}^n (X_i - \bar{X})^2 \right]^2} 
\end{equation*} 
<p>
which is also referred to as Eicker-Huber-White standard errors.
</p>
</div>
</div>


<div id="outline-container-org141a111" class="outline-3">
<h3 id="org141a111"><span class="section-number-3">5.4</span> What does this mean in practice?</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li>Heteroskedasticity is common in cross-sectional data. If you do not
have strong beliefs in homoskedasticity, then it is always safer to
report the heteroskedasticity-robust standard errors and use these
to compute the robust t-statistic.</li>
<li>In most software, the default setting is to report the
homoskedasticity-only standard errors. Therefore, you need to
manually add the option for the robust estimation. 

<ul class="org-ul">
<li><p>
In R, you can use the following codes
</p>
<pre class="example">
library(AER)
model1 &lt;- lm(testscr ~ str, data = classdata)
coeftest(model1, vcov = vcovHC(model1, type="HC1"))
</pre></li>

<li><p>
In STATA, you can use
</p>
<pre class="example">
regress testscr str, robust
</pre></li>
</ul></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org9346ae3" class="outline-2">
<h2 id="org9346ae3"><span class="section-number-2">6</span> The Theoretical Foundations of Ordinary Least Squares</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-orga349887" class="outline-3">
<h3 id="orga349887"><span class="section-number-3">6.1</span> The Gauss-Markov conditions</h3>
<div class="outline-text-3" id="text-6-1">
<p>
We have already known the least squares assumptions: for \(i = 1,
\ldots, n\), (1) \(E(u_i|X_i) =
0\), (2) \((X_i, Y_i)\) are i.i.d., and (3) large outliers are unlikely. 
</p>

<p>
The Gauss-Markov conditions provide anther version of these
assumptions plus the assumption of homoskedastic errors. 
</p>
</div>

<div id="outline-container-org395d830" class="outline-4">
<h4 id="org395d830">The Gauss-Markov conditions</h4>
<div class="outline-text-4" id="text-org395d830">
<p>
For \(\mathbf{X} = [X_1, \ldots, X_n]\) <sup><a id="fnr.3" class="footref" href="#fn.3">3</a></sup>
</p>
<ol class="org-ol">
<li>\(E(u_i| \mathbf{X}) = 0\)</li>
<li>\(\var(u_i | \mathbf{X}) = \sigma^2_u,\, 0 < \sigma^2_u < \infty\)</li>
<li>\(E(u_i u_j | \mathbf{X}) = 0,\, i \neq j\)</li>
</ol>
</div>
</div>

<div id="outline-container-org2312bd3" class="outline-4">
<h4 id="org2312bd3">From the three Least Squares Assumptions and the homoskedasticity assumption to the Gauss-Markov conditions</h4>
<div class="outline-text-4" id="text-org2312bd3">
<p>
Note that the conditional expectations in the G-M conditions are in
terms of all observations \(\mathbf{X}\), not just one observation,
\(X_i\). However, all the G-M conditions can be derived from the least
squares assumptions plus the homoskedasticity assumption. Specifically,
</p>

<ul class="org-ul">
<li>Assumptions (1) and (2) imply \(E(u_i | \mathbf{X}) = E(u_i | X_i) =
  0\).</li>
<li>Assumptions (1) and (2) imply \(\var(u_i| \mathbf{X}) =
  \var(u_i | X_i)\). With the homoskedasticity assumption, \(\var(u_i |
  X_i) = \sigma^2_u\), Assumption (3) then implies \(0 < \sigma^2_u < \infty\).</li>
<li>Assumptions (1) and (2) imply that \(E(u_i u_j | \mathbf{X}) = E(u_i
  u_j | X_i, X_j) = E(u_i|X_i) E(u_j|X_j) = 0\).</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgb8f8728" class="outline-3">
<h3 id="orgb8f8728"><span class="section-number-3">6.2</span> Linear conditionally unbiased estimator</h3>
<div class="outline-text-3" id="text-6-2">
</div><div id="outline-container-org9f63fe6" class="outline-4">
<h4 id="org9f63fe6">The general form of a linear conditionally unbiased estimator of \(\beta_1\)</h4>
<div class="outline-text-4" id="text-org9f63fe6">
<p>
The class of linear conditionally unbiased estimators consists of all
estimators of \(\beta_1\) that are linear function of \(Y_i, \ldots, Y_n\)
and that are unbiased, conditioned on \(X_1, \ldots, X_n\). 
</p>

<p>
For any linear estimator \(\tilde{\beta}_1\), it can be written as
</p>
\begin{equation}
\label{eq:beta1-tilde}
\tilde{\beta}_1 = \sum_{i=1}^n a_i Y_i\
\end{equation}
<p>
where the weights \(a_i\) for \(i = 1, \ldots, n\) depend on \(X_1, \ldots,
X_n\) but not on \(Y_1, \ldots, Y_n\). 
</p>

<p>
\(\tilde{\beta}_1\) is conditionally unbiased means that
</p>
\begin{equation}
\label{eq:e-beta1-tilde}
E(\tilde{\beta}_1 | \mathbf{X}) = \beta_1\
\end{equation}

<p>
By the Gauss-Markov conditions, from Equation (\ref{eq:beta1-tilde}),  we can have
</p>
\begin{equation*}
\begin{split}
E(\tilde{\beta}_1 | \mathbf{X}) &= \sum_i a_i E(\beta_0 + \beta_1 X_i + u_i | \mathbf{X}) \\
&= \beta_0 \sum_i a_i + \beta_1 \sum_i a_i X_i
\end{split}
\end{equation*}

<p>
For Equation (\ref{eq:e-beta1-tilde}) being satisfied with any
\(\beta_0\) and \(\beta_1\), we must have
\[ \sum_i a_i = 0 \text{ and } \sum_i a_iX_i = 1 \]
</p>
</div>
</div>

<div id="outline-container-orge336a07" class="outline-4">
<h4 id="orge336a07">The OLS esimator \(\hat{\beta}_1\) is a linear conditionally unbiased estimator</h4>
<div class="outline-text-4" id="text-orge336a07">
<p>
We have known that \(\hat{\beta}_1\) is unbiased both conditionally and
unconditionally. Next, we show that it is linear. 
\[ \hat{\beta}_1 = \frac{\sum_i (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_i
(X_i - \bar{X})^2} = \frac{\sum_i (X_i - \bar{X})Y_i}{\sum_i
(X_i - \bar{X})^2} = \sum_i \hat{a}_i Y_i \]
where the weights are
\[ \hat{a}_i = \frac{X_i - \bar{X}}{\sum_i (X_i - \bar{X})^2}, \text{
for } i = 1, \ldots, n \] 
Since \(\hat{\beta}_1\) is a linear conditionally unbiased estimator, we
must have
\[ \sum_i \hat{a}_i = 0 \text{ and } \sum_i \hat{a}_i X_i = 1  \]
which can be simply verified.
</p>
</div>
</div>
</div>

<div id="outline-container-org05fe335" class="outline-3">
<h3 id="org05fe335"><span class="section-number-3">6.3</span> The Gauss-Markov Theorem</h3>
<div class="outline-text-3" id="text-6-3">
<p>
The Gauss-Markov Theorem for \(\hat{\beta}_1\) states
</p>
<blockquote id="The Gauss-Markov Theorem for $\tilde{\beta}_{1}$">
<p>
If the Gauss-Markov conditions hold, then the OLS estimator
\(\hat{\beta}_1\) is the *B*est (most efficient) *L*inear conditionally
*U*nbiased *E*stimator (BLUE).
</p>
</blockquote>

<p>
The theorem can also be applied to \(\hat{\beta}_0\).
</p>

<p>
The proof of the Gauss-Markov theorem is in Appendix 5.2. A key in
this proof is that we can rewrite the expression of any linear
conditionally unbiased estimator \(\tilde{\beta}_1\) as
\[ \tilde{\beta}_1 = \sum_i a_i Y_i = \sum_i (\hat{a}_i + d_i)Y_i =
\hat{\beta}_1 + \sum_i d_i Y_i \]
And the goal of
the proof is to show that
\[ \var(\hat{\beta}_1 | \mathbf{X}) \leq \var(\tilde{\beta}_1 |
\mathbf{X}) \]
The equality holds only when \(\tilde{\beta}_1 = \hat{\beta}_1\). 
</p>
</div>
</div>

<div id="outline-container-org771fd28" class="outline-3">
<h3 id="org771fd28"><span class="section-number-3">6.4</span> The limitations of the Gauss-Markov theorem</h3>
<div class="outline-text-3" id="text-6-4">
<ol class="org-ol">
<li><p>
The Gauss-Markov conditions may hold in practice. Any violation of
the Gauss-Markov conditions will result in the OLS estimator not
being BLUE. The table below summarizes the cases in which a kind of
violation occurs, the consequences of such violation to the OLS
estimators, and possible remedies.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Summary of Violations of the Gauss-Markov Theorem</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Violation</th>
<th scope="col" class="org-left">Cases</th>
<th scope="col" class="org-left">Consequences</th>
<th scope="col" class="org-left">Remedies</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">\(E(u \mid X) \neq 0\)</td>
<td class="org-left">omitted variables, endogeneity</td>
<td class="org-left">biased</td>
<td class="org-left">more $X$'s, IV method</td>
</tr>

<tr>
<td class="org-left">\(\var(u_i\mid X)\) not constant</td>
<td class="org-left">heteroskedasticity</td>
<td class="org-left">inefficient</td>
<td class="org-left">WLS, GLS, HCCME</td>
</tr>

<tr>
<td class="org-left">\(E(u_{i}u_{j}\mid X) \neq 0\)</td>
<td class="org-left">autocorrelation</td>
<td class="org-left">inefficient</td>
<td class="org-left">GLS, HAC</td>
</tr>
</tbody>
</table></li>

<li>There are other candidate estimators that are not linear and
conditionally unbiased; under some conditions, these estimators are
more efficient than the OLS estimators.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org6a9feaf" class="outline-2">
<h2 id="org6a9feaf"><span class="section-number-2">7</span> Using the t-Statistic in Regression When the Sample Size is Small</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-org1da5463" class="outline-3">
<h3 id="org1da5463"><span class="section-number-3">7.1</span> The classical assumptions of the least squares estimation</h3>
<div class="outline-text-3" id="text-7-1">
<p>
We first expand the OLS assumptions by two additional ones. One is the
assumption of the homoskedastic errors, and another one is the
assumption that the conditional distribution of \(u_i\) given \(X_i\) is
the normal distribution, i.e., \(u_i \sim N(0,
\sigma^2_u) \text{ for } i = 1, \ldots, n\). 
</p>

<p>
All these assumptions together are often referred to as the classical
assumptions of the least squares estimation. 
For \(i = 1, 2, \ldots, n\)
</p>
<ul class="org-ul">
<li>Assumption 1: \(E(u_i | X_i) = 0\) (exogeneity of \(X\))</li>
<li>Assumption 2: \((X_i, Y_i)\) are i.i.d. (IID of \(X, Y, \text{ and }
                   u\))</li>
<li>Assumption 3: \(0 < E(X_i^4) < \infty\) and \(0 < E(Y_i^4) < \infty\)
(No large outliers)</li>
<li>Extended Assumption 4: \(\var(u_i | X_i) = \sigma^2_u, \text{ and } 0 <
                   \sigma^2_u < \infty\) (homoskedasticity)</li>
<li>Extended Assumption 5: \(u_i | X_i \sim N(0, \sigma^2_u)\) (normality)</li>
</ul>
</div>
</div>

<div id="outline-container-org531b0b2" class="outline-3">
<h3 id="org531b0b2"><span class="section-number-3">7.2</span> The t-Statistic and the Student-t Distribution</h3>
<div class="outline-text-3" id="text-7-2">
<p>
Under all the classical assumptions, we can construct the
t-statistic for hypothesis testing of a single coefficient. Even with
a small samples, the t-statistic has an exact Student-t distribution. 
</p>
</div>

<div id="outline-container-org7117f0c" class="outline-4">
<h4 id="org7117f0c">The t-statistic is for \(\beta_1\)</h4>
<div class="outline-text-4" id="text-org7117f0c">
<p>
\[H_0: \beta_1 = \beta_{1,0} \text{ vs } H_1: \beta_1 \neq \beta_{1,0}\]
</p>
\begin{equation}
t = \frac{\hat{\beta}_1 - \beta_{1,0}}{\hat{\sigma}_{\hat{\beta}_1}}
\end{equation}
<p>
where
</p>
\begin{equation*}
\hat{\sigma}^2_{\hat{\beta}_1} = \frac{s^2_u}{\sum_i (X_i - \bar{X})^2} \text{ and } s^2_u = \frac{1}{n-2}\sum_i \hat{u}_i^2
\end{equation*}
<p>
the former of which is the homoskedasticity-only standard error of
\(\hat{\beta}_1\) and the latter is the standard error of the
regression. 
</p>
</div>
</div>

<div id="outline-container-org489f82e" class="outline-4">
<h4 id="org489f82e">The Student-t distribution of \(t\)</h4>
<div class="outline-text-4" id="text-org489f82e">
<p>
The t statistic can be rewritten as
</p>
\begin{equation}
\label{eq:t-stat-b1a}
t = \frac{(\hat{\beta}_1 - \beta_{1,0})/\sigma_{\hat{\beta}_1}}{\sqrt{\frac{\hat{\sigma}^2_{\hat{\beta}_1}}{\sigma^2_{\hat{\beta}_1}}}} 
= \frac{z_{\hat{\beta}_1}}{\sqrt{\frac{s^2_u}{\sigma^2_u}}} = \frac{z_{\hat{\beta}_1}}{\sqrt{\frac{W}{n-2}}}
\end{equation}
<p>
where 
</p>

<p>
\[\sigma^2_{\hat{\beta}_1} = \frac{\sigma^2_u}{\sum_i (X_i -
\bar{X})^2} \] 
</p>

<p>
is the homoskedasticity-only variance of
\(\hat{\beta}_1\) when the variance of errors \(\sigma^2_u\) is known.  
</p>

<p>
\[
z_{\hat{\beta}_1} =\frac{\hat{\beta}_1 -
\beta_{1,0}}{\sigma_{\hat{\beta}_1}} 
\] 
</p>

<p>
is the z-statistic which has a standard normal distribution, that is,
\(z_{\hat{\beta}_1} \sim N(0, 1)\)
</p>

<p>
\[ 
W = (n-2)\frac{s^2_u}{\sigma^2_u} =
\frac{\sum_i\hat{u}_i^2}{\sigma^2_u} = \sum_i
\left(\frac{\hat{u}_i}{\sigma_u}\right)^2
 \] 
</p>

<p>
It can be shown that W is the sum of squares of \((n-2)\) independent
standard normally distributed variables, which results in a
chi-squared distribution with \((n-2)\) degrees of freedom. That is, \(W
\sim \chi^2(n-2)\), which is also independent of
\(z_{\hat{\beta}_1}\). Therefore, the t-statistic in Equation
(\ref{eq:t-stat-b1a}), as the ratio of \(z_{\hat{\beta}_1}\) and
\(\sqrt{W/(n-2)}\), is distributed as \(t(n-2)\).
</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
In a small sample case, the exact distribution of the
t-statistics is the Student-t distribution with \(n-1\) degree of
freedom.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
Note that the trick here is we put the
desired hypothesis to the alternative place. 
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3">3</a></sup> <div class="footpara">Here I use the vector
notation to represent all observations of \(X_i\) for \(i=1, \ldots,
n\). We will formally introduce the matrix notation for a linear
regression model and the OLS estimation in the next lecture.</div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Zheng Tian</p>
<p class="date">Created: 2017-03-22 Wed 08:58</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
