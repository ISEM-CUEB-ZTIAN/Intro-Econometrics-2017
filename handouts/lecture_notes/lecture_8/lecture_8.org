#+TITLE: Lecture 8: Linear Regression with Multiple Regressors
#+AUTHOR: Zheng Tian
#+DATE:
#+OPTIONS: toc:1 H:3 num:2 tex:t todo:nil <:nil ^:{}
#+PROPERTY: header-args:R  :session my-r-session
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,11pt]
#+LATEX_HEADER: \usepackage[margin=1.2in]{geometry}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \hypersetup{colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}
#+LATEX_HEADER: \newtheorem{mydef}{Definition}
#+LATEX_HEADER: \newtheorem{mythm}{Theorem}
#+LATEX_HEADER: \newcommand{\dx}{\mathrm{d}}
#+LATEX_HEADER: \newcommand{\var}{\mathrm{Var}}
#+LATEX_HEADER: \newcommand{\cov}{\mathrm{Cov}}
#+LATEX_HEADER: \newcommand{\corr}{\mathrm{Corr}}
#+LATEX_HEADER: \newcommand{\pr}{\mathrm{Pr}}
#+LATEX_HEADER: \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
#+LATEX_HEADER: \renewcommand\chaptername{Lecture}
#+LATEX_HEADER: \DeclareMathOperator*{\plim}{plim}
#+LATEX_HEADER: \newcommand{\plimn}{\plim_{n \rightarrow \infty}}
#+LATEX_HEADER: \def\mathbi#1{\textbf{\em #1}}


* Introduction

** Overview

This lecture extends the simple regression model to the multiple
regression model. Many aspects of multiple regression parallel those
of regression with a single regressor. The coefficients of the
multiple regression model can be estimated using the OLS estimation
method. The algebraic and statistical properties of the OLS estimators
of the multiple regression are also similar to those of the simple
regression. However, there are some new concepts, such as the
omitted variable bias and multicollinearity, to deepen our
understanding of the OLS estimation.


** Learning goals

- Be able to set up a multiple regression model with matrix notation.
- Understand the meaning of holding other things constant.
- Estimate the multiple regression model with the OLS estimation.
- Understand the Frisch-Waugh-Lovell theorem.
- Capable of detecting the omitted variable bias and
  multicollinearity.


** Readings

- /Introduction to Econometrics/ by Stock and Watson. Read thoroughly
  Chapter 6 and Sections 18.1 and 18.2
- /Introductory Econometrics: a Modern Approach/ by
  Wooldridge. Chapter 3.


* The Multiple Regression Model

** The problem of a simple linear regression

In the last two lectures, we use a simple linear regression model to
examine the effect of class sizes on test scores in the California
elementary school districts. The simple linear regression model with
only one regressor is
\begin{equation*}
TestScore = \beta_0 + \beta_1 \times STR + OtherFactors
\end{equation*}

*** Is this model adequate to characterize the determination of test scores?

Obviously, it ignores too many other important factors. Instead, all
these other factors are lumped into a single term, /OtherFactors/,
which is the error term, $u_i$, in the regression model.

 What are possible other important factors?
 - School district characteristics: average income level, demographic
   components
 - School characteristics: teachers' quality, school buildings,
 - Student characteristics: family economic conditions, individual
   ability

*** Percentage of English learners as an example

The percentage of English learners in a school district could be an
relevant and important determinant of test scores, which is omitted
in the simple regression model.

**** How can it affect the estimate of the effect of student-teacher ratios on test score?

- The districts with high percentages of English learners tend to have
  large student-teacher ratios. That is, these two variables are
  correlated with the correlation coefficient being 0.17.
- The higher percentage of English learners a district has, the lower
  test scores students there will make.
- In the simple regression model, the estimated negative coefficient on
  student-teacher ratios on test scores could include not only the
  negative influence from class sizes but also that from the
  percentage of English learners.
- In the terminology of statistics, the magnitude of the coefficient
  on student-teacher ratio is *overestimated*.
- Generally, we commit an *omitted variable bias* by setting up a simple
  regression model. We will explore the omitted variable bias in the
  last section in this lecture.

**** Solutions to the omitted variable bias

We can include these important but ignored variables, like the
percentage of English learners ($PctEL$), in the regression model.

\[
TestScore_i = \beta_0 + \beta_1 STR_i + \beta_2 PctEL_i +
OtherFactors_i
\]

A regression model with more than one regressors is a multiple
regression model.


** A multiple regression model

The general form of a *multiple regression model* is
\begin{equation}
\label{eq:multi-regress-1}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_k X_{ki} + u_i,\; i = 1, \ldots, n
\end{equation}
where
- $Y_i$ is the i^{th} observation on the dependent variable.
- $X_{1i}, X_{2i}, \ldots, X_{ki}$ are the i^{th} observation on each
  of the $k$ regressors.
- $u_i$ is the error term associated with the i^{th} observation,
  representing all other factors that are not included in the model.
- The population regression line (or population regression
  function) is the relationship that holds between $Y$ and $X$ on
  average in the population
  \begin{equation*}
  E(Y_i | X_{1i}, \ldots, X_{ki}) = \beta_0 + \beta_1 X_{1i} + \cdots + \beta_k X_{ki}
  \end{equation*}
- $\beta_1, \ldots, \beta_k$ are the coefficients on the corresponding
  $X_i,\, i = 1, \ldots, k$. $\beta_0$ is the intercept, which can
  also be thought of the coefficient on a regressor $X_{0i}$ that equals
  1 for all observations.
  - Including $X_{0i}$, there are $k+1$ regressors in the multiple
    regression model.
  - The linear regression model with a single regressor is in fact a
    multiple regression model with two regressors, 1 and $X$.


** The interpretation of $\beta_i$

***  Holding other things constant

We can suppress the subscript $i$ in Equation (\ref{eq:multi-regress-1})
so that we re-write it as

\begin{equation}
\label{eq:multi-regress-1a}
Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k + u
\end{equation}

In Equation (\ref{eq:multi-regress-1a}), the coefficient $\beta_i$ on
a regressor $X_i$, for $i=1, \ldots, k$, measures the effect on $Y$ of a
unit change in $X_i$, *holding other $X$ constant*.

Suppose we have two regressors $X_1$ and $X_2$ and we are interested
in the effect of $X_1$ on $Y$. We can let $X_1$ change by $\Delta X_1$
and holding $X_2$ constant. Then, the new value of $Y$ is
\[
Y + \Delta Y = \beta_0 + \beta_1 (X_1 + \Delta X_1) + \beta_2 X_2
\]
Subtracting $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2$, we have
$\Delta Y = \beta_1 \Delta X_1$. That is
\[ \beta_1 = \frac{\Delta Y}{\Delta X} \text{ holding } X_2 \text{ constant} \]

*** Partial effect

If $Y$ and $X_i$ for $i = 1, \ldots, k$ are continuous and
differentiated variables, from Equation (\ref{eq:multi-regress-1a}),
we know that $\beta_i$ is as simply as the partial derivative of $Y$ with
respect to $X_i$. That is \[\beta_i = \frac{\partial Y}{\partial
X_i}\] By the definition of a partial derivative, $\beta_i$ is just
the effect of a marginal change in $X_i$ on $Y$ holding other $X$
constant.


** The matrix notation of a multiple regression model

*** Consider the matrix notation as a way to organize data

When we save the data set of California school districts in Excel, it
is saved in a spreadsheet as shown in Figure [[fig:data-snapshot]].

#+NAME: fig:data-snapshot
#+CAPTION: The California data set in spreadsheet
#+ATTR_LATEX: :width 0.6\textwidth :height 0.6\textwidth
[[file:img/data_snapshot.png]]

Each row represents an observation of all variables pertaining to a
school district, and each column represents a variable with all
observations. This format of data display can be concisely denoted
using vectors and a matrix.

Let us first define the following vectors and matrices:
\begin{equation*}
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{pmatrix},\,
\mathbf{X} =
\begin{pmatrix}
1 & X_{11} & \cdots & X_{k1} \\
1 & X_{12} & \cdots & X_{k2} \\
\vdots & \vdots & \ddots & \vdots \\
1 & X_{1n} & \cdots & X_{kn}
\end{pmatrix}
=
\begin{pmatrix}
\mathbi{x}^{\prime}_1 \\
\mathbi{x}^{\prime}_2 \\
\vdots \\
\mathbi{x}^{\prime}_n
\end{pmatrix},\,
\mathbf{u} =
\begin{pmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{pmatrix},\,
\boldsymbol{\beta} =
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{pmatrix}
\end{equation*}

- $\mathbf{Y}$ is an $n \times 1$ vector of $n$ observations on the
  dependent variable.
- $\mathbf{X}$ is an $n \times (k+1)$ matrix of $n$ observations on
  $k + 1$ regressors which include the intercept term as a regressor of
  1's.
- $\mathbi{x}_i$ is a $(k+1) \times 1$ vector of the i^{th}
  observation on all $(k+1)$ regressors. Thus,
  $\mathbi{x}^{\prime}_i$ denotes the i^{th} row in $\mathbf{X}$.
- $\boldsymbol{\beta}$ is a $(k+1) \times 1$ vector of the $(k+1)$
  regression coefficients.
- $\mathbf{u}$ is an $n \times 1$ vector of the $n$ error terms.

*** Write a multiple regression model with matrix notation
**** The multiple regression model for one observation

The multiple regression model in Equation (\ref{eq:multi-regress-1})
for the i^{th} observation can be written as
\begin{equation}
\label{eq:multi-regress-mi}
Y_i = \mathbi{x}^{\prime}_i \boldsymbol{\beta} + u_i,\; i = 1, \ldots, n
\end{equation}

**** The multiple regression model for all observations

Stacking all $n$ observations in Equation (\ref{eq:multi-regress-mi})
yields the multiple regression model in matrix form:
\begin{equation}
\label{eq:multi-regress-m}
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}
\end{equation}

$\mathbf{X}$ can also be written in terms of column vectors as
\[
\mathbf{X} = [\mathbf{X}_0, \mathbf{X}_1, \ldots, \mathbf{X}_k ]
\]
where $\mathbf{X}_i = [X_{i1}, X_{i2}, \ldots, X_{in}]^{\prime}$ is a
$n \times 1$ vector of $n$ observations of the k^{th}
regressor. $\mathbf{X}_0$ is a vector of 1s. That is,
$\mathbf{X}_0 = [1, 1, \ldots, 1]^{\prime}$. More often, we use
$\boldsymbol{\iota}$ to denote such a vector of 1s. [fn:1]

Thus, Equation (\ref{eq:multi-regress-m}) can be re-written as
\begin{equation}
\label{eq:multi-regress-m2}
\mathbf{Y} = \beta_0\boldsymbol{\iota} + \beta_1\mathbf{X}_1 + \cdots + \beta_k\mathbf{X}_k + \mathbf{u}
\end{equation}


* The OLS Estimator in Multiple Regression

** The OLS estimator

*** The minimization problem

The idea of the ordinary least squares estimation for a multiple
regression model is exactly the same as for a simple regression
model. The OLS estimators of the multiple regression model are obtained by
minimizing the sum of the squared prediction mistakes.

Let $\mathbf{b} = [b_0, b_1, \ldots, b_k]^{\prime}$ be some estimators of
$\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots,
\beta_k]^{\prime}$. The predicted $Y_i$ can be obtained by
\[ \hat{Y}_i = b_0 + b_1 X_{1i} + \cdots + b_k X_{ki} = \mathbi{x}^{\prime}_i
\mathbf{b},\, i = 1, \ldots, n \]
or in matrix notation
\[ \hat{\mathbf{Y}} = \mathbf{Xb} \]

The prediction mistakes with $\mathbf{b}$, or called the residuals, are
\[ \hat{u}_i = Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} = Y_i -
\mathbi{x}^{\prime}_i \mathbf{b} \]
or in matrix notation, the residual vector is
\[ \hat{\mathbf{u}} = \mathbf{Y} - \mathbf{Xb} \]

Then the sum of the squared prediction mistakes (residuals) is
\begin{align*}
S(\mathbf{b}) & = S(b_0, b_1, \ldots, b_k) = \sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki})^2 \\
& = \sum_{i=1}^n (Y_i - \mathbi{x}^{\prime}_i \mathbf{b})^2 = (\mathbf{Y} -
\mathbf{Xb})^{\prime}(\mathbf{Y}-\mathbf{Xb}) \\
& = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} = \sum_{i=1}^n \hat{u}_i^2
\end{align*}

The OLS estimator is the solution to the following minimization problem:
\begin{equation}
\label{eq:ols-multi-regress}
\operatorname*{min}_{\mathbf{b}}\: S(\mathbf{b}) = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}}
\end{equation}

*** The OLS estimator of $\boldsymbol{\beta}$ as a solution to the minimization problem

The formula for the OLS estimator is obtained by taking the derivative
of the sum of squared prediction mistakes, $S(b_0, b_1, \ldots, b_k)$, with respect to each coefficient,
setting these derivatives to zero, and solving for the estimator
$\hat{\boldsymbol{\beta}}$.

The derivative of $S(b_0, \ldots, b_k)$ with respect to $b_j$ is
\begin{gather*}
\label{eq:ols-wrt-bj}
\frac{\partial }{\partial b_j} \sum_{i=1}^n \left(Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} \right)^2 = \\
-2 \sum_{i=1}^n X_{ji} \left(Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} \right) = 0
\end{gather*}
There are $k+1$ such equations for $j=0, \ldots, k$. Solving this
system of equations, we obtain the OLS estimator
$\hat{\boldsymbol{\beta}} = (\hat{\beta}_0, \ldots, \hat{\beta}_k)^{\prime}$.

Using matrix notation, the formula for the OLS estimator
$\boldsymbol{\hat{\beta}}$ is
\begin{equation}
\label{eq:betahat-mult}
\boldsymbol{\hat{\beta}} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}
\end{equation}

To prove Equation (\ref{eq:betahat-mult}), we need to use some results
of matrix calculus.
\begin{equation}
\label{eq:matrix-calc}
\frac{\partial \mathbf{a}^{\prime} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a},\; \frac{\partial \mathbf{x}^{\prime} \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a},\; \text{ and } \frac{\partial \mathbf{x}^{\prime} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = (\mathbf{A} + \mathbf{A}^{\prime}) \mathbf{x}
\end{equation}
when $\mathbf{A}$ is symmetric, then $(\partial \mathbf{x}^{\prime} \mathbf{A} \mathbf{x}) / (\partial \mathbf{x}) = 2\mathbf{A} \mathbf{x}$

\begin{proof}[Proof of Equation (\ref{eq:betahat-mult})]
The sum of squared prediction mistakes is
\begin{equation*}
S(\mathbf{b}) = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} = \mathbf{Y}^{\prime} \mathbf{Y} - \mathbf{b}^{\prime} \mathbf{X}^{\prime} \mathbf{Y} - \mathbf{Y}^{\prime} \mathbf{Xb} - \mathbf{b}^{\prime} \mathbf{X}^{\prime} \mathbf{Xb}
\end{equation*}
The first order conditions for minimizing $S(\mathbf{b})$ with respect to $\mathbf{b}$ is
\begin{equation}
\label{eq:ols-mult-eqs}
-2 \mathbf{X}^{\prime} \mathbf{Y} - 2 \mathbf{X}^{\prime} \mathbf{Xb} = \mathbf{0}
\end{equation}
Then
\begin{equation*}
\mathbf{b} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}
\end{equation*}
given that $\mathbf{X}^{\prime} \mathbf{X}$ is invertible.
\end{proof}

Note that Equation (\ref{eq:ols-mult-eqs}) represents a system of
equations with $k+1$ equations.


** Example: the OLS estimator of $\hat{\beta}_1$ in a simple regression model

Let take a simple linear regression model as an example. The simple
linear regression model written in matrix notation is
\begin{equation*}
\mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1 \mathbf{X}_1 + \mathbf{u} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}
\end{equation*}
where

\begin{equation*}
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix},\,
\mathbf{X} =
\begin{pmatrix}
\boldsymbol{\iota} & \mathbf{X}_1
\end{pmatrix}
=
\begin{pmatrix}
1 & X_{11} \\
\vdots & \vdots \\
1 & X_{1n}
\end{pmatrix},\,
\mathbf{u} =
\begin{pmatrix}
u_1 \\
\vdots \\
u_n
\end{pmatrix},\,
\boldsymbol{\beta} =
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\end{pmatrix}
\end{equation*}

Let's get the components in Equation (\ref{eq:betahat-mult}) step by
step.

First, the most important part is
$\left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}$.

\begin{align*}
\mathbf{X}^{\prime}\mathbf{X} &=
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \\
\mathbf{X}_1^{\prime}
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{\iota} & \mathbf{X}_1
\end{pmatrix} =
\begin{pmatrix}
1 & \cdots & 1 \\
X_{11} & \cdots & X_{1n}
\end{pmatrix}
\begin{pmatrix}
1 & X_{11} \\
\vdots & \vdots \\
1 & X_{1n}
\end{pmatrix} \\
&=
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \boldsymbol{\iota} & \boldsymbol{\iota}^{\prime} \mathbf{X}_1 \\
\mathbf{X}_1^{\prime} \boldsymbol{\iota} & \mathbf{X}_1^{\prime} \mathbf{X}_1
\end{pmatrix} =
\begin{pmatrix}
n & \sum_{i=1}^n X_{1i} \\
\sum_{i=1}^n X_{1i} & \sum_{i=1}^n X_{1i}^2
\end{pmatrix}
\end{align*}

Recall that the inverse of a $2 \times 2$ matrix can be calculated as follows
\begin{equation*}
\begin{pmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{pmatrix}^{-1}
=\frac{1}{a_{11}a_{22} - a_{12}a_{21}}
\begin{pmatrix}
a_{22} & -a_{12} \\
-a_{21} & a_{11}
\end{pmatrix}
\end{equation*}

Thus, the inverse of $\mathbf{X}^{\prime}\mathbf{X}$ is
\begin{equation*}
\left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1} =
\frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
\begin{pmatrix}
\sum_{i=1}^n X_{1i}^2 & - \sum_{i=1}^n X_{1i} \\
-\sum_{i=1}^n X_{1i} & n
\end{pmatrix}
\end{equation*}

Next, we compute $\mathbf{X}^{\prime} \mathbf{Y}$.
\begin{equation*}
\mathbf{X}^{\prime} \mathbf{Y} =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \\
\mathbf{X}_1^{\prime}
\end{pmatrix}
\mathbf{Y} =
\begin{pmatrix}
1 & \cdots & 1 \\
X_{11} & \cdots & X_{1n}
\end{pmatrix}
\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix} =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \mathbf{Y} \\
\mathbf{X}_1^{\prime} \mathbf{Y}
\end{pmatrix} =
\begin{pmatrix}
\sum_{i=1}^n Y_i \\
\sum_{i=1}^n X_{1i} Y_i
\end{pmatrix}
\end{equation*}

Finally, we compute $\boldsymbol{\hat{\beta}} = (\mathbf{X}^{\prime}
\mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}$, which is
\begin{align*}
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix} & =
\frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
\begin{pmatrix}
\sum_{i=1}^n X_{1i}^2 & - \sum_{i=1}^n X_{1i} \\
-\sum_{i=1}^n X_{1i} & n
\end{pmatrix}
\begin{pmatrix}
\sum_{i=1}^n Y_i \\
\sum_{i=1}^n X_{1i} Y_i
\end{pmatrix} \\
& =
\frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
\begin{pmatrix}
\sum_{i=1}^n X_{1i}^2 \sum_{i=1}^n Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n X_{1i}Y_i \\
-\sum_{i=1}^n X_{1i} \sum_{i=1}^n Y_i + n \sum_{i=1}^n X_{1i} Y_i
\end{pmatrix}
\end{align*}

Therefore, $\hat{\beta}_1$ is the second element of the vector
pare-multiplied by the fraction, that is,
\begin{equation*}
\hat{\beta}_1 = \frac{n \sum_{i=1}^n X_{1i} Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n Y_i}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2} = \frac{\sum_{i=1}^n (X_{1i} - \bar{X}_1)(Y_i - \bar{Y})}{\sum_{i=1}^n (X_{1i} - \bar{X}_1)^2}
\end{equation*}

It follows that
\begin{equation*}
\hat{\beta}_0 = \frac{\sum_{i=1}^n X_{1i}^2 \sum_{i=1}^n Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n X_{1i}Y_i}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2} = \bar{Y} - \hat{\beta}_1 \bar{X}_1
\end{equation*}


** Application to Test Scores and the Student-Teacher Ratio

Now we can apply the OLS estimation method of multiple regression to
the application of California school districts. Recall that the
estimated simple linear regression model is
\[ \widehat{TestScore} = 698.9 - 2.28 \times STR \]

Since we concern that the estimated coefficient on /STR/ may be
overestimated without considering the percentage of English
learners in the districts, we include this new variable in the
multiple regression model to control for the effect of English
learners, yielding a new estimated regression model as
\[ \widehat{TestScore} = 686.0 - 1.10 \times STR - 0.65 \times PctEL
\]
- The interpretation of the new estimated coefficient on /STR/ is,
  *holding the percentage of English learners constant*, a unit
  decrease in /STR/ is estimated to increase test scores by 1.10
  points.
- We can also interpret the estimated coefficient on /PctEL/ as,
  holding /STR/ constant, one unit decrease in /PctEL/ increases test
  scores by 0.65 point.
- The magnitude of the negative effect of /STR/ on test scores in the
  multiple regression is approximately half as large as when /STR/ is
  the only regressor, which verifies our concern that we may omit
  important variables in the simple linear regression model.


* Measures of Fit in Multiple Regression

** The Standard errors of the regression (SER)

The standard error
of regression (SER) estimates the standard deviation of the error term
$\mathbf{u}$. In multiple regression, the SER is
\begin{equation}
\label{eq:ser-m}
SER = s_{\hat{u}},\; \text{ where } s^2_{\hat{u}} = \frac{1}{n-k-1} \sum_{i=1}^n \hat{u}_i^2 =\frac{\mathbf{\hat{u}}^{\prime} \mathbf{\hat{u}}}{n-k-1} = \frac{SSR}{n-k-1}
\end{equation}
Here $SSR$ is divided by $(n-k-1)$ because there are $(k+1)$
coefficients to be estimated using $n$ samples.


** $R^2$

*** The definition of $R^2$ in multiple regression models

Like in the regression model with single regressor, we can define
$TSS$, $ESS$, and $SSR$ in the multiple regression model.

- *The total sum of squares (TSS)*: $TSS = \sum_{i=1}^n (Y_i - \bar{Y})^2$
- *The explained sum of squares (ESS)*: $ESS = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$
- *The sum of squared residuals (SSR)*: $SSR = \sum_{i=1}^n \hat{u}_i^2$

Let $y_i$ be the deviation of $Y_i$ from its sample mean, that it,
$y_i = Y_i - \bar{Y},\; i=1,\ldots,n$. In matrix notation, we have
\begin{equation*}
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{pmatrix},\;
\boldsymbol{\iota} =
\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix},\;
\mathbf{y} =
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
=
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{pmatrix}
-
\begin{pmatrix}
\bar{Y} \\
\bar{Y} \\
\vdots \\
\bar{Y}
\end{pmatrix}
=
\mathbf{Y} - \bar{Y} \boldsymbol{\iota}
\end{equation*}
Therefore, $\mathbf{y}$ is the vector of the deviation from the mean of
$Y_i,\; i=1,\ldots,n$. Similarly, we can define the deviation from the
mean of $\hat{Y}_i,\, i=1, \ldots, n$ as $\hat{\mathbf{y}} =
\hat{\mathbf{Y}} - \bar{Y} \boldsymbol{\iota}$. Then we can rewrite
$TSS, ESS,\, \text{ and } SSR$ as
\[ TSS = \mathbf{y}^{\prime} \mathbf{y},\; ESS =
\hat{\mathbf{y}}^{\prime} \hat{\mathbf{y}},\; \text{ and } SSR =
\hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} \]

In multiple regression, the relationship that
\[ TSS = ESS + SSR, \text{ or, } \mathbf{y}^{\prime} \mathbf{y} =
\hat{\mathbf{y}}^{\prime} \hat{\mathbf{y}} + \hat{\mathbf{u}}^{\prime}
\hat{\mathbf{u}}\]
still holds so that we can define $R^2$ as
\begin{equation}
\label{eq:r2-center}
R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}
\end{equation}

*** Limitations of $R^2$

1. $R^2$ is valid only if a regression model is estimated using the OLS
   since otherwise it would not be true that $TSS = ESS + SSR$.

2. $R^2$ defined in the form of the deviation from the mean is only
   valid when a constant term is included in regression. Otherwise,
   use the uncentered version of $R^2$, which is also defined as
   \begin{equation}
   \label{eq:r2-uncenter}
   R^2_u = \frac{EES}{TSS} = 1 - \frac{SSR}{TSS}
   \end{equation}
   where $TSS = \sum_{i=1}^n Y_i^2 = \mathbf{Y}^{\prime} \mathbf{Y}$,
   $ESS = \sum_{i=1}^2 \hat{Y}_i^2 = \hat{\mathbf{Y}}^{\prime}
   \hat{\mathbf{Y}}$, and $SSR = \sum_{i=1}^n \hat{u}_i^2 =
   \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}}$, using the uncentered
   variables.  Note that in a regression without a constant term, the
   equality $TSS = ESS + SSR$ is still true.

3. Most importantly, $R^2$ increases whenever an additional regressor
   is included in a multiple regression model, unless the estimated
   coefficient on the added regressor is exactly zero.

   Consider two regression models
   \begin{align}
   \mathbf{Y} &= \beta_0 + \beta_1 \mathbf{X}_1 + \mathbf{u}
   \label{eq:ex-eq-1} \\
   \mathbf{Y} &= \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \mathbf{u} \label{eq:ex-eq-2}
   \end{align}

   Since both models use the same $\mathbf{Y}$, $TSS$ must be the
   same. If the OLS estimator $\hat{\beta}_2$ does not equal 0, then
   $SSR$ in Equation (\ref{eq:ex-eq-1}) is always larger than that of
   Equation (\ref{eq:ex-eq-2})  since the former $SSR$ is minimized
   with respect to $\beta_0, \beta_1$ and with the constraint of
   $\beta_2 = 0$ and the latter is minimized without the constraint
   over $\beta_2$.


** The adjusted $R^2$

*** The definition of the adjusted $R^2$

The adjusted $R^2$ is, or $\bar{R}^2$, is a modified version of
$R^2$ in Equation (\ref{eq:r2-center}). $\bar{R}^2$ improves $R^2$ in the sense that it does not
necessarily increase when a new regressor is added.

$\bar{R}^2$ is
defined as
\begin{equation}
\label{eq:adj-r2}
\bar{R}^2 = 1 - \frac{SSR / (n-k-1)}{TSS / (n-1)} = 1 - \frac{n-1}{n-k-1}\frac{SSR}{TSS} = 1 - \frac{s^2_u}{s^2_Y}
\end{equation}

- The adjustment is made by dividing $SSR$ and $TSS$ by their
  corresponding degrees of freedom, which is $n-k-1$ and $n-1$,
  respectively.
- $s^2_u$ is the sample variance of the OLS residuals, which is given
  in Equation (\ref{eq:ser-m}); $s^2_Y$ is the sample variance of $Y$.
- The definition of $\bar{R}^2$ in Equation (\ref{eq:adj-r2}) is
  valid only when a constant term is included in the regression
  model.
- Since $\frac{n-1}{n-k-1} > 1$, then it is always true that
  the $\bar{R}^2 < R^2$.
- On one hand, $k \uparrow\, \Rightarrow\, \frac{SSR}{TSS} \downarrow$. On
  the other hand, $k \uparrow\, \Rightarrow \frac{n-1}{n-k-1}
  \uparrow$. Whether $\bar{R}^2$ increases or decreases depends on
  which of these effects is stronger.
- The $\bar{R}^2$ can be negative. This happens when the regressors,
  taken together, reduce the sum of squared residuals by such a small
  amount that his reduction fails to offset the factor $\frac{n-1}{n-k-1}$.

*** The usefulness of $R^{2}$ and $\bar{R}^2$

- Both $R^2$ and $\bar{R}^2$ are valid when the regression model is
  estimated by the OLS estimators. $R^2$ or $\bar{R}^2$ computed with the
  estimators other than the OLS estimators is usually called /pseudo/
  $R^2$.

- Their importance as measures of fit cannot be overstated. We cannot heavily
  reply on $R^{2}$ or $\bar{R}^2$ to judge whether some regressors should
  be included in the model or not.


* The Frisch-Waugh-Lovell Theorem

** The grouped regressors

Consider a multiple regression model
\begin{equation}
\label{eq:fwl-eq}
Y_i = \underbrace{\beta_0 + \beta_1 X_{1i} +
\cdots + \beta_{k1} X_{k1,i}}_{\text{k1+1 regressors}} + \underbrace{
\beta_{k1+1} X_{k1+1,i} + \cdots \beta_k X_k}_{\text{k2 regressors}} + u_i
\end{equation}

In Equation eqref:eq:fwl-eq, among k regressors, $X_1, \ldots, X_k$,
we collect $k1$ regressors and an intercept into a group and the rest
$k2 = k - k1$ regressors into a second group. In matrix notation,
we write
\begin{equation}
\label{eq:mult-reg-2g}
\mathbf{Y} = \mathbf{X}_1\boldsymbol{\beta}_1 + \mathbf{X}_2 \boldsymbol{\beta}_2 + \mathbf{u}
\end{equation}
where $\mathbf{X}_1$ is an $n \times (k1+1)$ matrix composed of the
intercept and the first $k1+1$ regressors in Equation eqref:eq:fwl-eq,
and $\mathbf{X}_2$ is an $n \times k2$ matrix composed of the rest
$k_2$ regressors. $\boldsymbol{\beta}_1 = (\beta_0, \beta_1, \ldots,
\beta_{k1})^{\prime}$ and $\boldsymbol{\beta}_2 = (\beta_{k1+1},
\ldots, \beta_k)^{\prime}$.


** Two estimation strategies

Suppose that we are interested in $\boldsymbol{\beta}_2$ but not much
in $\boldsymbol{\beta}_1$ in Equation eqref:eq:mult-reg-2g. How can we
estimate $\boldsymbol{\beta}_2$?

*** The first strategy: the standard OLS estimation

We can obtain the OLS estimation of $\boldsymbol{\beta}_2$ with
Equation eqref:eq:betahat-mult, i.e., $\hat{\boldsymbol{\beta}} =
(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
\mathbf{Y}$. $\hat{\boldsymbol{\beta}}_2$ is a vector consisting of
the last $k2$ elements in $\hat{\boldsymbol{\beta}}$.

In matrix notation, we can get $\hat{\boldsymbol{\beta}}_2$ from the
following equation
\begin{equation*}
\begin{pmatrix}
\hat{\boldsymbol{\beta}}_1 \\
\hat{\boldsymbol{\beta}}_2
\end{pmatrix} =
\begin{pmatrix}
\mathbf{X}_1^{\prime} \mathbf{X}_1 & \mathbf{X}_1^{\prime} \mathbf{X}_2 \\
\mathbf{X}_2^{\prime} \mathbf{X}_1 & \mathbf{X}_2^{\prime} \mathbf{X}_2
\end{pmatrix}^{-1}
\begin{pmatrix}
\mathbf{X}_1^{\prime} \mathbf{Y} \\
\mathbf{X}_2^{\prime} \mathbf{Y}
\end{pmatrix}
\end{equation*}

*** The second strategy: the step OLS estimation

Alternatively, we can perform the following steps to estimate
$\boldsymbol{\beta}_2$:
1. Regress each regressor in $\mathbf{X}_2$ on all regressors in
   $\mathbf{X}_1$, including the intercept, and get the residuals from
   this regression, denoted as $\widetilde{\mathbf{X}}_2$.

   That is, for each regressor $\mathbf{X}_i$ in $\mathbf{X}_2$,
   $i=k1+1, \ldots, k$, we estimate a multiple regression,
   \[
   \mathbf{X}_i = \gamma_0 + \gamma_1 \mathbf{X}_1 + \cdots +
   \gamma_{k1} \mathbf{X}_{k1} + v \]
   The residuals from this regression is
   \[ \widetilde{\mathbf{X}}_i = X_i - \hat{\gamma}_0 -
   \hat{\gamma}_1 \mathbf{X}_1 - \cdots - \hat{\gamma}_{k1}
   \mathbf{X}_{k1} \]
   As such, we can get an $n \times k2$ matrix
   composed of all the residuals $\widetilde{\mathbf{X}}_2 =
   (\widetilde{\mathbf{X}}_{k1+1} \cdots \widetilde{\mathbf{X}}_k)$.

2. Regress $\mathbf{Y}$ on all regressors in $\mathbf{X}_1$, denoting
   the residuals from this regression as $\widetilde{\mathbf{Y}}$.

3. Regress $\widetilde{\mathbf{Y}}$ on $\widetilde{\mathbf{X}}_2$, and
   obtain the estimates of $\boldsymbol{\beta}_2$ as
   $\boldsymbol{\beta}_2=(\widetilde{\mathbf{X}}_2^{\prime} \widetilde{\mathbf{X}}_2)^{-1}
   \widetilde{\mathbf{X}}_2^{\prime} \widetilde{\mathbf{Y}}$.


** The Frisch-Waugh-Lovell Theorem

The Frisch-Waugh-Lovell (FWL) Theorem states that
1) the OLS estimates of $\boldsymbol{\beta}_2$ using the second strategy
   and that from the first strategy are numerically identical.
2) the residuals from the regression of $\widetilde{\mathbf{Y}}$ on
   $\widetilde{\mathbf{X}}_2$ and the residuals from Equation
   (\ref{eq:mult-reg-2g}) are numerically identical.

The proof of the FWL theorem is beyond the scope of this
proof. Interested students may refer to Exercise 18.7.
Understanding the meaning of this theorem is much more important than
understanding the proof.

The FWL theorem provides a mathematical statement of how the multiple
regression coefficients in $\hat{\boldsymbol{\beta}}_2$ capture the
effects of $\mathbf{X}_2$ on $\mathbf{Y}$, controlling for other
$\mathbf{X}$.

- Step 1 purges the effects of the regressors in $\mathbf{X}_1$ on the
  regressors in $\mathbf{X}_2$
- Step 2 purges the effects of the regressors in $\mathbf{X}_1$ on
  $\mathbf{Y}$.
- Step 3 estimates the effect of the regressors in $\mathbf{X}_2$ on
  $\mathbf{Y}$ using the parts in $\mathbf{X}_2$ and
  $\mathbf{Y}$ that have excluded the effects of $\mathbf{X}_1$.


** An example of the FWL theorem
Consider a regression model with single regressor
\[ Y_i = \beta_0 +
\beta_1 X_i + u_i,\; i=1, \ldots, n
\]

Following the estimation strategy in the FWL theorem, we can carry out
the following regressions,
1. Regress $Y_i$ on 1. That is, estimate the model
   \[ Y_i = \alpha + e_i \]
   Then, the OLS estimator of $\alpha$ is
   $\bar{Y}$ and the residuals is $y_i = Y_i - \bar{Y}$
2. Similarly, regress $X_{i}$ on 1. Then
   the residuals from this regression is $x_{i} = X_{i} -
   \bar{X}$.
3. Regress $y_i$ on $x_{i}$ without intercept. That is,
   estimate the model
   \[ y_i = \beta_1 x_{i} + v_i \text{ or in matrix notation }
   \mathbf{y} = \beta_1 \mathbf{x} + \mathbf{v} \]
4. We can obtain $\hat{\beta_1}$ directly by applying the formula in
   Equation (\ref{eq:betahat-mult}). That is
   \[ \hat{\beta}_1 = (\mathbf{x}^{\prime} \mathbf{x})^{-1}
   \mathbf{x}^{\prime} \mathbf{y} = \frac{\sum_i x_{i} y_i}{\sum_i
   x_{i}^2} = \frac{\sum_i (X_i-\bar{X})(Y_i-\bar{Y})}{\sum_i(X_i-\bar{X})^2} \]
   which is exactly the same as the OLS estimator of $\beta_1$ in $Y_i
   = \beta_0 + \beta_1 X_i + u_i$.


* The Least Squares Assumptions in Multiple Regression

We introduce four least squares assumptions for a multiple regression
model. The first three assumptions are the same as those in the
simple regression model with just minor modifications to suit multiple
regressors. The fourth assumption is a new one.

- Assumption #1 :: $E(u_i | \mathbi{x}_i) = 0$. The conditional mean
                   of $u_i$ given $X_{1i}, X_{2i}, \ldots, X_{ki}$ has
                   mean of zero. This is the key assumption to assure
                   that the OLS estimators are unbiased.

- Assumption #2 :: $(Y_i, \mathbi{x}_i^{\prime}),\; i=1, \ldots, n$, are
                   i.i.d. This assumption holds automatically if the
                   data are collected by simple random sampling.

- Assumption #3 :: Large outliers are unlikely, i.e.,, $0 <
                   E(\mathbf{X}^4) < \infty$ and $0 < E(\mathbf{Y}^4)
                   < \infty$. That is, the dependent variables and
                   regressors have finite kurtosis.
- Assumption #4 :: No *perfect multicollinearity*. The regressors are
                   said to exhibit perfect multicollinearity if one of
                   the regressor is a perfect linear function of the
                   other regressors.


* The Statistical Properties of the OLS Estimators in Multiple Regression

** Unbiasedness and consistency

Under the least squares assumptions the OLS estimator
$\hat{\boldsymbol{\beta}}$ can be shown to be *unbiased* and
*consistent* estimator of $\boldsymbol{\beta}$ in the multiple
regression model of Equation (\ref{eq:multi-regress-m}).

*** Unbiasedness

The OLS estimators $\hat{\boldsymbol{\beta}}$ is unbiased if
$E(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}$.

To show the unbiasedness, we can rewrite $\hat{\boldsymbol{\beta}}$ as
follows,
\begin{equation}
\label{eq:bhat-m-a}
\hat{\boldsymbol{\beta}} = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y}
= \left(\mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{X}^{\prime} (\mathbf{X} \boldsymbol{\beta} + \mathbf{u})
= \boldsymbol{\beta} + \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{u}
\end{equation}

Thus, the conditional expectation of $\hat{\boldsymbol{\beta}}$ is
\begin{equation}
\label{eq:bhat-unbias}
E(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \boldsymbol{\beta} + \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} E(\mathbf{u} | \mathbf{X}) = \boldsymbol{\beta}
\end{equation}
in which $E(\mathbf{u} | \mathbf{X}) = 0$ from the first least squares
assumption.

Using the law of iterated expectation, we have
\[ E(\hat{\boldsymbol{\beta}}) = E(E(\hat{\boldsymbol{\beta}} |
\mathbf{X})) = E(\boldsymbol{\beta}) = \boldsymbol{\beta} \]
Therefore, $\hat{\boldsymbol{\beta}}$ is an unbiased estimator of
$\boldsymbol{\beta}$.

*** Consistency

The OLS estimator $\hat{\boldsymbol{\beta}}$ is consistent if as $n
\rightarrow \infty$, $\hat{\boldsymbol{\beta}}$ will converge to
$\boldsymbol{\beta}$ in probability, that is, $\plim_{n \rightarrow
\infty} \hat{\boldsymbol{\beta}} = \boldsymbol{\beta}$.

From Equation (\ref{eq:bhat-m-a}), we can have
\begin{equation*}
\plim_{n \rightarrow \infty} \hat{\boldsymbol{\beta}} = \boldsymbol{\beta} + \plim_{n \rightarrow \infty} \left(\frac{\mathbf{X}^{\prime} \mathbf{X}}{n} \right)^{-1} \plim_{n \rightarrow \infty}\left( \frac{\mathbf{X}^{\prime} \mathbf{u}}{n} \right)
\end{equation*}
Let us first  make an assumption, which is usually true, that
\begin{equation}
\label{eq:plim-bhat-m}
 \plim_{n \rightarrow \infty} \frac{1}{n} \mathbf{X}^{\prime}
\mathbf{X} = \underset{(k+1) \times (k+1)}{\mathbf{Q_X}}
\end{equation}
which means as $n$ goes to very large, $\mathbf{X}^{\prime}
\mathbf{X}$ converge to a nonstochastic matrix $\mathbf{Q_X}$ with
full rank $(k + 1)$. In Chapter 18, we will see that $\mathbf{Q_X} =
E(\mathbf{X}_i \mathbi{x}_i^{\prime})$ where $\mathbi{x}_i = [1,
X_{1i}, \ldots, X_{ki}]^{\prime}$ is the $i^{th}$ row of
$\mathbf{X}$.

Now let us look at $\plim_{n \rightarrow \infty} \frac{1}{n}
\mathbf{X}^{\prime} \mathbf{u}$ which can be rewritten as
\[ \plim_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^n \mathbi{x}_i
u_i \]
Since $E(u_i | \mathbi{x}_i) = 0$, we know that $E(\mathbi{x}_i u_i) =
E(\mathbi{x}_iE(u_i | \mathbi{x}_i)) = 0$. Also, by Assumptions #2 and
#3, we know that $\mathbi{x}_i u_i$ are i.i.d. and have positive
finite variance. Thus, by the law of large number
\[ \plim_{n \rightarrow  \infty} \frac{1}{n} \sum_{i=1}^n \mathbi{x}_i u_i = E(\mathbi{x}_i
u_i) = 0 \]

Therefore, we can conclude that
\[ \plim_{n \rightarrow \infty} \hat{\boldsymbol{\beta}} = \boldsymbol{\beta}  \]
That is, $\hat{\boldsymbol{\beta}}$ is consistent.


** The Gauss-Markov theorem and efficiency

*** The Gauss-Markov conditions

The Gauss-Markov conditions for multiple regression are
1. $E(\mathbf{u} | \mathbf{X}) = 0$,
2. $\var(\mathbf{u} | \mathbf{X}) = E(\mathbf{uu}^{\prime} |
   \mathbf{X}) = \sigma^2_u \mathbf{I}_n$ (homoskedasticity),
3. $\mathbf{X}$ has full column rank (no perfect multicollinearity).

*** The Gauss-Markov Theorem

#+BEGIN_QUOTE
If the Gauss-Markov conditions hold in the multiple regression model,
then the OLS estimator $\hat{\boldsymbol{\beta}}$ is more efficient
than any other linear unbiased estimator $\tilde{\boldsymbol{\beta}}$
in the sense that $\var(\tilde{\boldsymbol{\beta}}) -
\var(\hat{\boldsymbol{\beta}})$ is a positive semidefinite
matrix. That is, the OLS estimator is BLUE.
#+END_QUOTE

That $\var(\tilde{\boldsymbol{\beta}}) -
\var(\hat{\boldsymbol{\beta}})$ is a positive semidefinite matrix
means that for any nonzero $(k+1) \times 1$ vector $\mathbf{c}$,
\[ \mathbf{c}^{\prime}\left(\var(\tilde{\boldsymbol{\beta}}) -
\var(\hat{\boldsymbol{\beta}})\right) \mathbf{c} \geq 0 \]
or we can simply write as
\[ \var(\tilde{\boldsymbol{\beta}}) \geq
\var(\hat{\boldsymbol{\beta}})  \]
The equality holds only when $\tilde{\boldsymbol{\beta}} =
\hat{\boldsymbol{\beta}}$.[fn:2]

*** Understanding the Gauss-Markov conditions

Like in the regression model with single regressor, the least
squares assumptions can be summarized by the Gauss-Markov conditions
as
- Assumptions #1 and #2 imply that $E(\mathbf{u} | \mathbf{X}) = \mathbf{0}_n$.
  \[E(u_i | \mathbf{X}) = E(u_i | [\mathbf{X_1}, \ldots, \mathbi{x}_i,
  \ldots, \mathbi{x}_n]^{\prime}) = E(u_i | \mathbi{x}_i) = 0\]
  in  which the second equality follows Assumption #2 that
  $\mathbi{x}_i,\,\text{ for } i = 1,\ldots,n$ are independent.

- Assumption #1, #2, and the additional assumption of homoskedasticity
  imply that $\var(\mathbf{u} | \mathbf{X}) = \sigma^2_u \mathbf{I}_n$.

  For a random vector $\mathbf{x}$, the variance of $\mathbf{x}$ is a
  covariance matrix defined as
  \[ \var(\mathbf{x}) =
  E\left((\mathbf{x}-E(\mathbf{x}))(\mathbf{x}-E(\mathbf{x}))^{\prime}\right)
  \]
  which also holds for the conditional variance by replacing the
  expectation operator with the conditional expectation operator.

  Since $E(\mathbf{u} | \mathbf{X}) = 0$, its covariance matrix,
  conditioned on $\mathbf{X}$, is
  \[ \var(\mathbf{u} | \mathbf{X}) = E(\mathbf{u} \mathbf{u}^{\prime} | \mathbf{X})
  \]
  where
  \begin{equation*}
  \mathbf{u} \mathbf{u}^{\prime} =
  \begin{pmatrix}
  u_1^2 & u_1 u_2 & \cdots &u_1 u_n \\
  u_2 u_1 & u_2^2 & \cdots & u_2 u_n \\
  \vdots & \vdots & \ddots & \vdots \\
  u_n u_1 & u_n u_2 & \cdots & u_n^2 \\
  \end{pmatrix}
  \end{equation*}
  Thus, in the matrix $\mathbf{u} \mathbf{u}^{\prime}$,
  - the expectation of the diagonal elements, conditioned on $\mathbf{X}$,
    are the conditional variance of $u_i$ which is $\sigma^2_u$ because
    of homoskedasticity.
  - The conditional expectation of the off-diagonal elements are the
    covariance of $u_i$ and $u_j$, conditioned on $\mathbf{X}$. Since
    $u_i$ and $u_j$ are independent according to Assumption #2, $E(u_i
    u_j | \mathbf{X}) = 0$.

  Therefore, the conditional covariance matrix of $\mathbf{u}$ is
  \begin{equation*}
  \var(\mathbf{u} | \mathbf{X}) =
  \begin{pmatrix}
  \sigma^2_u & 0 & \cdots & 0 \\
  0 & \sigma^2_u & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \cdots & \sigma^2_u
  \end{pmatrix}
  = \sigma^2_u \mathbf{I}_n
  \end{equation*}

*** Linear conditionally unbiased estimators

Any linear estimator of $\boldsymbol{\beta}$ can be written as
\[ \tilde{\boldsymbol{\beta}} = \mathbf{Ay} = \mathbf{AX}\boldsymbol{\beta} + \mathbf{Au} \]
where $\mathbf{A}$ is a weight matrix depending only on $\mathbf{X}$
not on $\mathbf{y}$.

For $\tilde{\boldsymbol{\beta}}$ to be conditionally unbiased, we must
have
\begin{equation*}
E(\tilde{\boldsymbol{\beta}} | \mathbf{X}) = \mathbf{AX} \boldsymbol{\beta} + \mathbf{A} E(\mathbf{u} | \mathbf{X}) = \boldsymbol{\beta}
\end{equation*}
which only holds when $\mathbf{AX} = \mathbf{I}_{k+1}$ and the first
Gauss-Markov condition holds.

The OLS estimator $\hat{\boldsymbol{\beta}}$ is a linear conditionally
unbiased estimator with $\mathbf{A} = \left(\mathbf{X}^{\prime}
\mathbf{X}\right)^{-1} \mathbf{X}^{\prime}$. Obviously, $\mathbf{AX} =
\mathbf{I}_{k+1}$ is true for $\hat{\boldsymbol{\beta}}$.

*** The conditional covariance matrix of $\hat{\boldsymbol{\beta}}$

The conditional variance matrix of $\hat{\boldsymbol{\beta}}$ can be
derived as follows
\begin{equation*}
\begin{split}
\var(\hat{\boldsymbol{\beta}} | \mathbf{X}) &= E\left[ (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^{\prime} | \mathbf{X}\right] \\
&= E\left[ \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{u} \left(\left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{u} \right)^{\prime} | \mathbf{X} \right] \\
&= E\left[ \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{u} \mathbf{u}^{\prime} \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1} | \mathbf{X} \right] \\
&= \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} E(\mathbf{uu}^{\prime} | \mathbf{X}) \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{split}
\end{equation*}

Then, by the second Gauss-Markov condition, we have
\begin{equation*}
\var(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} (\sigma^2_u \mathbf{I}_n) \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1} = \sigma^2_u (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{equation*}

The *homoskedasticity-only* covariance matrix of $\hat{\boldsymbol{\beta}}$ is
\begin{equation}
\label{eq:varbhat-hm}
\var(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \sigma^2_u (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{equation}

If the homoskedasticity assumption does not hold, denote the
covariance matrix of $\mathbf{u}$ as
\[ \var(\mathbf{u} | \mathbf{X}) = \mathbf{\Omega} \]

Heteroskedasticity means that the diagonal elements of
$\mathbf{\Omega}$ can be different (i.e. $\var(u_i | \mathbf{X}) =
\sigma^2_i \text{ for } i=1, \ldots, n)$, while the off-diagonal
elements are zeros, that is
\begin{equation*}
\mathbf{\Omega} =
\begin{pmatrix}
\sigma^2_1 & 0 & \cdots & 0 \\
0 & \sigma^2_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2_n
\end{pmatrix}
\end{equation*}

Define $\mathbf{\Sigma} = \mathbf{X}^{\prime} \mathbf{\Omega}
\mathbf{X}$. Then the *heteroskedasticity-robust covariance matrix* of
$\hat{\boldsymbol{\beta}}$ is
\begin{equation}
\label{eq:varbhat-ht}
\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{\Sigma} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{equation}


** The asymptotic normal distribution

In large samples, the OLS estimator $\hat{\boldsymbol{\beta}}$ has the
multivariate normal asymptotic distribution as
\begin{equation}
\label{eq:normal-bhat-m}
\hat{\boldsymbol{\beta}} \rarrowd{d} N(\boldsymbol{\beta}, \mathbf{\Sigma_{\hat{\boldsymbol{\beta}}}})
\end{equation}
where $\mathbf{\Sigma_{\hat{\boldsymbol{\beta}}}} =
\var(\hat{\boldsymbol{\beta}} | \mathbf{X})$ for which use
Equation (\ref{eq:varbhat-hm}) for the homoskedastic case and Equation
(\ref{eq:varbhat-ht}) for the heteroskedastic case.

The proof of the asymptotic normal distribution and the multivariate
central limit theorem are given in Chapter 18.


* The Omitted Variable Bias

** The definition of the omitted variable bias
 The *omitted variable bias* is the bias in the OLS esitmator that arises
 when the included regressors, $\mathbf{X}$, are correlated with
 omitted variables, $\mathbf{Z}$, where $\mathbf{X}$ may include $k$
 regressors, $\mathbf{X}_1, \ldots, \mathbf{X}_k$, and $\mathbf{Z}$
 may include $l$ omitted variables, $\mathbf{Z}_1, \ldots,
 \mathbf{Z}_m$. The omitted variable bias occurs
 when two conditions are met
 1. $\mathbf{X}$ is correlated with some omitted variables in $\mathbf{Z}$.
 2. The omitted variables are determinants of the dependent variable
    $\mathbf{Y}$.


** The reason for the omitted variable bias
Suppose that the true model is
\begin{equation}
\label{eq:omb-1}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \mathbf{u}
\end{equation}
in which the first least squares assumption, $E(\mathbf{u} |
\mathbf{X}, \mathbf{Z}) = 0$, holds. We further assume that $\cov(\mathbf{X}, \mathbf{Z})
\neq 0$

However, we mistakenly exclude $\mathbf{Z}$ in regression analysis and
estimate a short model
\begin{equation}
\label{eq:omb-2}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
\end{equation}
Since $\boldsymbol{\epsilon}$ represents all other factors that are not
in Equation (\ref{eq:omb-2}), including $\mathbf{Z}$, and
$\cov(\mathbf{X}, \mathbf{Z}) \neq 0$, this means that
$\cov(\mathbf{X}, \boldsymbol{\epsilon}) \neq 0$, which implies that
$E(\boldsymbol{\epsilon} | \mathbf{X}) \neq 0$. (Recall that in
Chapter 4, we prove that $E(u_i | X_i) = 0 \Rightarrow \cov(u_i, X_i)
= 0$, which implies that $\cov(u_i, X_i) \neq 0 \Rightarrow E(u_i |
X_i) \neq 0)$.) Therefore, Assumption #1 does not hold for the short
model, which means that the OLS estimator of Equation (\ref{eq:omb-2})
is biased.

An informal proof of the OLS estimator of Equation (\ref{eq:omb-2}) is
biased is given as follows.

The OLS estimator of Equation (\ref{eq:omb-2}) is $\hat{\boldsymbol{\beta}} &=
(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
\mathbf{Y}$. Plugging $\mathbf{Y}$ with the true model, we have
\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\mathbf{X}^{\prime} (\mathbf{X}\boldsymbol{\beta} +
\mathbf{Z}\boldsymbol{\gamma} + \mathbf{u})
= \boldsymbol{\beta} + (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\mathbf{X}^{\prime} \mathbf{Z} \boldsymbol{\gamma} +
(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{u} \]

Taking the expectation of $\hat{\boldsymbol{\beta}}$,
conditioned on $\mathbf{X}$, we have
\begin{equation}
\label{eq:omb-3}
E(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \boldsymbol{\beta}
+ \underbrace{(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Z} \boldsymbol{\gamma}}_{\mathclap{\text{omitted variable bias}}} + \mathbf{0}
\end{equation}
The second term in the equation above usually does not equal zero
unless either
1) $\boldsymbol{\gamma} = \mathbf{0}$, which means that
   $\mathbf{Z}$ are not determinants of $\mathbf{Y}$ in the true model,
   or
2) $\mathbf{X}^{\prime} \mathbf{Z} = 0$, which means that
   $\mathbf{X}$ and $\mathbf{Z}$ are not correlated.
Therefore, if these two conditions do not hold,
$\hat{\boldsymbol{\beta}}$ for the short model is biased. And the
magnitude and direction of the bias is determined by
$\mathbf{X}^{\prime} \mathbf{Z} \boldsymbol{\gamma}$.


** An illustration using a linear model with two regressors
Suppose the true model is
\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i,\; i=1,
\ldots, n \]
with $E(u_i | X_{1i}, X_{2i}) = 0$

However, we estimate a wrong model of
\[ Y_i = \beta_0 + \beta_1 X_{1i} + \epsilon_i,\; i=1, \ldots, n \]
In Lecture 5 we showed that $\beta_1$ can be expressed as
\[ \hat{\beta}_1 = \beta_1 + \frac{\frac{1}{n}\sum_i
(X_{1i} - \bar{X}_1) \epsilon_i}{\frac{1}{n}\sum_i (X_i - \bar{X}_1)^2} \]

As $n \rightarrow \infty$, the numerator of the second term converges
to $\cov(X_1, \epsilon) = \rho_{{\scriptscriptstyle X_1} \epsilon} \sigma_{\scriptscriptstyle X_1} \sigma_{\epsilon}$
and the denominator converges to $\sigma^2_{\scriptscriptstyle X_1}$, where
$\rho_{{\scriptscriptstyle X_1} \epsilon}$ is the correlation coefficient between $X_{1i}$ and
$\epsilon$. Therefore, we have

\begin{equation}
\label{eq:omb-4}
\hat{\beta}_1 \rarrowd{p} \beta_1 +
\underbrace{\rho_{{\scriptscriptstyle X_1} \epsilon}
\frac{\sigma_{\epsilon}}{\sigma_{\scriptscriptstyle X_1}}}
_{\mathclap{\text{omitted variable bias}}}
\end{equation}

From Equations (\ref{eq:omb-3}) and (\ref{eq:omb-4}), we can summarize some facts about the omitted variable bias:
- Omitt variable bias is a problem irregardless of whether the sample
  size is large or small. $\hat{\beta}$ is biased and inconsistent
  when there is omitted variable bias.
- Whether this bias is large or small in practice depends on
  $|\rho_{{\scriptscriptstyle X_1} \epsilon}|$ or $|\mathbf{X}^{\prime} \mathbf{Z}
  \boldsymbol{\gamma}|$.
- The direction of this bias is determined by the sign of
  $\rho_{{\scriptscriptstyle X_1} \epsilon}$ or $\mathbf{X}^{\prime} \mathbf{Z} \boldsymbol{\gamma}$.
- One easy way to detect the existence of the omitted variable bias is
  that when adding a new regressor, the estimated coefficients on some
  previously included regressors change substantially.


* Multicollinearity

** Perfect multicollinearity

*Perfect multicollinearity* refers to the situation when one of the
regressor is a perfect linear function of the other regressors.
- In the terminology of linear algebra, perfect multicollinearity
  means that the vectors of regressors are linearly dependent.
- That is, the vector of a regressor can be expressed as a linear
  combination of vectors of the other regressors.

Remember that the matrix of regressors $\mathbf{X}$ can be written in
terms of column vectors as
\[
\mathbf{X} = [\boldsymbol{\iota}, \mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_k ]
\]
where $\mathbf{X}_i = [X_{i1}, X_{i2}, \ldots, X_{in}]^{\prime}$
is a $n \times 1$ vector of $n$ observations of the i^{th}
regressor. $\boldsymbol{\iota}$ is a vector of 1s, representing the
constant term.

That the $k+1$ column vectors are linearly dependent means that there
exist some $(k+1) \times 1$ nonzero vector $\boldsymbol{\beta} =
[\beta_0, \beta_1, \ldots, \beta_k]^{\prime}$ such that
\[
\beta_0 \boldsymbol{\iota} + \beta_1 \mathbf{X}_1 + \cdots + \beta_k
\mathbf{X}_k = 0 \]

If $\mathbf{X}_i$, for $i=1,\ldots,n$, are linearly dependent,
then it follows
- $\mathbf{X}$ does not have full column rank.
- If $\mathbf{X}$ does not have full column rank, then
  $\mathbf{X}^{\prime} \mathbf{X}$ is singular, that is, the inverse
  of $\mathbf{X}^{\prime} \mathbf{X}$ does not exist. Therefore, we
  can state the assumption of requiring no perfect multicollinearity
  in another way as assuming that $\mathbf{X}$ has full column rank.
- If $\mathbf{X}^{\prime} \mathbf{X}$ is not invertible, the OLS
  estimator based on the formula of $\boldsymbol{\hat{\beta}} =
  (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
  \mathbf{Y}$ does not exist.


** Examples of perfect multicollinearity

Remember that perfect multicollinearity occurs when one regressor can
be expressed as a linear combination of other regressors. This problem
belongs to the logic error when the researcher sets up the regression
model. That is, the researcher uses some redundant regressors in the
model to provide the same information that merely one regressor can
sufficiently provide.

*** Possible linear combination
Suppose we have a multiple regression model
\[ \mathbf{Y} = \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2
\mathbf{X}_2 + \mathbf{u}  \]
And we want to add a new variable $Z$ into this model. The following
practices cause perfect multicollinearity
- $Z = a X_1$ or $Z = b X_2$
- $Z = 1 - a X_1$
- $Z = a X_1 + b X_2$

However, we can add a $Z$ that is not a linear function of $X_1$ or
$X_2$ such that there is no perfect multicollinearity problem. For example,
- $Z = X_1^2$
- $Z = \ln X_1$
- $Z = X_1 X_2$


** The dummy variable trap
The dummy variable trap is a good case of perfect multicollinearity
that a modeler often encounters. Recall that a *binary variable* (or
*dummy variable*) $D_i$, taking values of one or zero, can be used in
a regression model to distinguish two mutually exclusive groups of
samples, for instance, the male and the female. In fact, dummy
variables can be constructed to represent more than two groups and be
used in multiple regression to examine the difference between these
groups.

Suppose that we have a data composed of people of four ethnic groups:
White, African American, Hispanic, and Asian. And we want to estimate
a regression model to see whether wages among these four groups are
different. We may (mistakenly as we will see) set up a multiple
regression model as follows
\begin{equation}
\label{eq:dummy-trap}
Wage_i = \beta_0 + \beta_1 White_i + \beta_2 African_i + \beta_3 Hispanic_i + \beta_4 Asian_i + u_i
\end{equation}
where $White_i$ is a dummy variable which equal 1 if the i^{th}
observation is a white people and equal 0 if he/she is not, similarly
for $African_i, Hispanic_i, \text{ and } Asian_i$.

*** A concrete example

To be concrete, suppose we have four observations: Chuck,
Mike, Juan, and Li, who are White, African American, Hispanic, and
Asian, respectively. Then the dummy variables are
\begin{equation*}
White =
\begin{pmatrix}
1 \\
0 \\
0 \\
0
\end{pmatrix},\,
African =
\begin{pmatrix}
0 \\
1 \\
0 \\
0
\end{pmatrix},\,
Hispanic =
\begin{pmatrix}
0 \\
0 \\
1 \\
0
\end{pmatrix},\,
Asian =
\begin{pmatrix}
0 \\
0 \\
0 \\
1
\end{pmatrix}
\end{equation*}

However, when we construct a model like Equation
(\ref{eq:dummy-trap}), we fall into the dummy variable trap, suffering
perfect multicollinearity. This is because this model has a constant
term $\beta_0 \times 1$ which is the sum of all dummy variables. That
is,
\begin{equation*}
\begin{pmatrix}
1 \\
1 \\
1 \\
1 \\
\end{pmatrix}
= White + African + Hispanic + Asian
\end{equation*}
Let see when the observation is Chuck, then the model is
\[ Wage = \beta_0 + \beta_1 + u \]
Estimating this model yields $\widehat{\beta_0 + \beta_1}$, from which
we cannot get a unique solution for $\beta_1$.

To avoid the dummy variable trap, we can either of the following two
methods:
1. drop the constant term
2. drop one dummy variable
The difference between these two methods lies in how we interpret the
coefficients on dummy variables.

*** Drop the constant term

If we drop the constant term, the model becomes
\begin{equation}
\label{eq:dummy-trap-1}
Wage = \beta_1 White + \beta_2 African + \beta_3 Hispanic + \beta_4 Asian + u
\end{equation}
For Chuck or all white people, the model becomes
\[ Wage = \beta_1 + u \]
Then $\beta_1$ is the population mean wage of whites, that is, $\beta_1 =
E(Wage | White = 1)$. Similarly,
$\beta_2, \beta_3, \text{ and } \beta_4$ are the population mean wage
of African Americans, Hispanics, and Asians, respectively.

*** Drop one dummy variable

If we drop the dummy variable for white people, then the model becomes
\begin{equation}
\label{eq:dummy-trap-2}
Wage = \beta_1 + \beta_2 African + \beta_3 Hispanic + \beta_4 Asian + u
\end{equation}
For white people, the model is
\[Wage = \beta_1 + u_i \]
And the constant term $\beta_1$ is just the population mean of
whites, that is,
\[\beta_1 = E(Wage | White = 1)\]
So we say that white people
serve as a reference case in Model (\ref{eq:dummy-trap-2}).

For African Americans, the model is
\[ Wage = \beta_1 + \beta_2 + u  \]
From it we have $E(Wage | African=1) = \beta_1 + \beta_2$ so that
\[\beta_2 = E(Wage | African = 1) - \beta_1 = E(Wage | African = 1) -
E(Wage | White = 1)\]
Similarly, we can get that
\begin{align*}
\beta_3 &= E(Wage | Hispanic = 1) - E(Wage | White = 1) \\
\beta_4 &= E(Wage | Asian = 1) - E(Wage | White = 1)
\end{align*}
Therefore, when we adopt the second method by dropping a dummy
variable for the reference case, then the coefficients on other dummy
variables represent the difference in the population means between the
interested case and the reference case.


** Imperfect Multicollinearity
*** Definition of imperfect multicollinearity
*Imperfect multicollinearity* is a problem of regression when two or
more regressors are highly correlated. Although they bear similar
names, imperfect multicollinearity and perfect multicollinearity are
two different concepts.
- Perfect multicollinearity is a problem of modeling building,
  resulting in a total failure to estimate a linear model.
- Imperfect multicollinearity is usually a problem of data when some
  regressors are highly correlated.
- Imperfect multicollinearity does not affect the unbiasedness of the
  OLS estimators. However, it does affect the efficiency, i.e., the
  variance of the OLS estimators.
*** An illustration using a regression model with two regressors
Suppose we have a linear regression model with two regressors.
\begin{equation}
\label{eq:ex-collin}
\mathbf{Y} = \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \mathbf{u}
\end{equation}
where, for simplicity, $\mathbf{u}$ is assumed to be homoskedastic.

By the FWL theorem, estimating Equation (\ref{eq:ex-collin}) will get
the same OLS estimators of $\beta_1$ and $\beta_2$ as estimating the
following model,
\begin{equation}
\label{eq:ex-collin-1}
\mathbf{y} = \beta_1 \mathbf{x}_1 + \beta_2 \mathbf{x}_2 + \mathbf{v}
\end{equation}
where $\mathbf{y} = \mathbf{Y} - \bar{Y} \boldsymbol{\iota}$, $\mathbf{x}_1 =
\mathbf{X}_1 - \bar{X}_1 \boldsymbol{\iota}$, and $\mathbf{x}_2 = \mathbf{X}_2 -
\bar{X}_2 \boldsymbol{\iota}$, that is, $\mathbf{y}, \mathbf{x}_1, \text{ and }
\mathbf{x}_2}$ are in the form of the deviation from the mean. And
denote $\mathbf{x} = [\mathbf{x}_1\;  \mathbf{x}_2]$ as the matrix of
all regressors in Model (\ref{eq:ex-collin-1}).

Suppose that $X_1$ and $X_2$ are correlated so that their correlation
coefficient $|\rho_{12}| > 0$. And the square of the sample
correlation coefficient is

\begin{equation}
r^2_{12} = \frac{\left(\sum (X_1 - \bar{X}_1)(X_2 - \bar{X}_2)\right)^2}{\sum (X_1 - \bar{X}_1)^2 \sum (X_2 - \bar{X}_2)^2}
= \frac{\left( \sum x_1 x_2\right)^2}{\sum x_1 \sum x_2}
\end{equation}

The OLS estimator of Model (\ref{eq:ex-collin-1}) is
\begin{equation}
\label{eq:bhat-ex-collin}
\hat{\boldsymbol{\beta}} = \left(\mathbf{x}^{\prime} \mathbf{x}\right)^{-1} \mathbf{x}^{\prime} \mathbf{y}
\end{equation}
with the homoskedasticity-only covariance matrix as
\begin{equation}
\label{eq:bhat-cov-ex-collin}
\var(\hat{\boldsymbol{\beta}} | \mathbf{x}) = \sigma^2_u \left(\mathbf{x}^{\prime} \mathbf{x}\right)^{-1}
\end{equation}

- $\hat{\boldsymbol{\beta}}$ is still unbiased since the assumption of
  $E(\mathbf{u} | X) = 0$ holds and so does $E(\mathbf{v} |
  \mathbf{x}) = 0$.

- The variance of $\hat{\beta}_1$, which is the first diagonal element
  of $\sigma^2_u \left(\mathbf{x}^{\prime} \mathbf{x}\right)^{-1}$, is
  affected by $r_{12}$. To see this, we write $\var(\hat{\beta}_1 | \mathbf{x})$
  explicitly as
  \begin{equation*}
  \begin{split}
  \var(\hat{\beta}_1 | \mathbf{x}) &=  \frac{\sigma^2_u \sum_i x_2^2}{\sum_i x_1^2 \sum_i x_2^2 - (\sum_i x_1 x_2)^2} \\
  &= \frac{\sigma^2_u \sum_i x_2^2}{\displaystyle \sum_i x_1^2 \sum_i x_2^2 \left(1 - \frac{(\sum_i x_1 x_2)^2}{\sum_i x_1^2 \sum_i x_2^2}\right)} \\
  &= \frac{\sigma^2_u}{\sum_i x_1^2} \frac{1}{(1 - r^2_{12})}
  \end{split}
  \end{equation*}
  Therefore, when $X_1$ and $X_2$ are highly correlated, that is
  $r^2_{12}$ gets close to 1, then $\var(\hat{\beta}_1 | \mathbf{x})$
  becomes very large.

- The consequence of multicollinearity is that it may lead us to
  wrongly fail to reject the zero hypothesis in the t-test for a
  coefficient.

- The variance inflation factor (VIF) is a commonly used indicator for
  detecting multicollinearity. The definition is

  \begin{equation*}
  \mathrm{VIF} = \frac{1}{1 - r^2_{12}}
  \end{equation*}

  The smaller VIF is for a regressor, the less severe the problem of
  multicollinearity is. However, there is no widely accepted cut-off
  value for VIF to detect multicollinearity. $VIF > 10$ for a
  regressor is often seen as an indication of multicollinearity, but
  we cannot always trust this.

*** Possible remedies for multicollinearity
- Include more sample in hope of the variation in $\mathbf{X}$ getting
  widened, i.e., increasing $\sum_i (X_{1i} - \bar{X}_1)$.
- Drop the variable(s) that is highly correlated with other
  regressors. Notice that by doing this we are at the risk of
  suffering the omitted variable bias. There is always a trade-off
  between including all relevant regressors and making the regression
  model /parsimonious/.[fn:: The word "parsimonious" in Econometrics
  means that we always want to make the model as concise as possible
  without any redundant variables included.]


* Footnotes

[fn:2] The complete proof of the Gauss-Markov
theorem in multiple regression is in Appendix 18.5.

[fn:1] $\boldsymbol{\iota}$ has the following properties:
(1) $\boldsymbol{\iota}^{\prime} \mathbf{x} = \sum_{i=1}^n x_i$ for an
  $n \times 1$ vector $\mathbf{x}$, (2) $\boldsymbol{\iota}^{\prime}
\boldsymbol{\iota} = n$ and $\left(\boldsymbol{\iota}^{\prime}
\boldsymbol{\iota} \right)^{-1} = 1/n$, (3)
$\boldsymbol{\iota}^{\prime} \left(
\boldsymbol{\iota}^{\prime}\boldsymbol{\iota} \right)^{-1} \mathbf{x}
= \bar{x}$, and (4) $\boldsymbol{\iota}^{\prime} \mathbf{X} \boldsymbol{\iota} =
  \sum_{i=1}^n \sum_{j=1}^n x_{ij}$ for an $n \times n$ matrix $\mathbf{X}$.


