#+TITLE: Lecture 8: Linear Regression with Multiple Regressors
#+AUTHOR: Zheng Tian
#+DATE:
#+STARTUP: beamer
#+OPTIONS: toc:1 H:2
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,10pt]
#+BEAMER_THEME: CambridgeUS
#+BEAMER_COLOR_THEME: beaver
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 :ETC
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newtheorem{mydef}{Definition}
#+LATEX_HEADER: \newtheorem{mythm}{Theorem}
#+LATEX_HEADER: \newcommand{\dx}{\mathrm{d}}
#+LATEX_HEADER: \newcommand{\var}{\mathrm{var}}
#+LATEX_HEADER: \newcommand{\cov}{\mathrm{cov}}
#+LATEX_HEADER: \newcommand{\corr}{\mathrm{corr}}
#+LATEX_HEADER: \newcommand{\pr}{\mathrm{Pr}}
#+LATEX_HEADER: \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
#+LATEX_HEADER: \DeclareMathOperator*{\plim}{plim}
#+LATEX_HEADER: \newcommand{\plimn}{\plim_{n \rightarrow \infty}}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \def\mathbi#1{\textbf{\em #1}}


* The Multiple Regression Model
#+TOC: headlines [currentsection]

** The problem of a simple linear regression

*** The simple linear regression model
\begin{equation*}
TestScore = \beta_0 + \beta_1 \times STR + OtherFactors
\end{equation*}

*** Question: Is this model adequate to characterize the determination of test scores?

- It ignores many important factors, simply lumped into
  /OtherFactors/, the error term, $u_i$, in the regression model.

- What are possible other important factors?
  - School district characteristics: average income level, demographic
    components
  - School characteristics: teachers' quality, school buildings, 
  - Student characteristics: family economic conditions, individual
    ability

** Percentage of English learners as an example
:PROPERTIES:
:BEAMER_opt:
:END:

The percentage of English learners in a school district could be an
relevant and important determinant of test scores, which is omitted
in the simple regression model.

*** How can it affect the estimate of the effect of student-teacher ratios on test score?

- High percentage of English learners \Rightarrow large student-teacher ratios.

- High percentage of English learners \Rightarrow lower test scores.

- The estimated effect of student-teacher ratios may in fact include
  the influence from the high percentage of English learners. 

- In the terminology of statistics, the magnitude of the coefficient
  on student-teacher ratio is *overestimated*.

- The problem is called *the omitted variable bias*

** Solutions to the problem of ignoring important factors

We can include these important but ignored variables, like the
percentage of English learners ($PctEL$), in the regression model.  

\[
TestScore_i = \beta_0 + \beta_1 STR_i + \beta_2 PctEL_i +
OtherFactors_i 
\] 

A regression model with more than one regressors is a multiple
regression model. 

** A multiple regression model
:PROPERTIES:
:BEAMER_opt:
:END:

The general form of a *multiple regression model* is
\begin{equation}
\label{eq:multi-regress-1}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_k X_{ki} + u_i,\; i = 1, \ldots, n
\end{equation}
where
- $Y_i$ is the i^{th} observation on the dependent variable;
- $X_{1i}, X_{2i}, \ldots, X_{ki}$ are the i^{th} observation on each
  of the $k$ regressors; and
- $u_i$ is the error term associated with the i^{th} observation,
  representing all other factors that are not included in the model.

** The components in a multiple regression model
:PROPERTIES:
:BEAMER_opt:
:END:

- The population regression line (or population regression
  function)
  \begin{equation*}
  E(Y_i | X_{1i}, \ldots, X_{ki}) = \beta_0 + \beta_1 X_{1i} + \cdots + \beta_k X_{ki}
  \end{equation*}

- $\beta_1, \ldots, \beta_k$ are the coefficients on the corresponding
  $X_i,\, i = 1, \ldots, k$.

- $\beta_0$ is the intercept, which can also be thought of the
  coefficient on a regressor $X_{0}$ that equals 1 for all
  observations.

  - Including $X_{0}$, there are $k+1$ regressors in the multiple
    regression model.

** The interpretation of $\beta_i$: Holding other things constant

\begin{equation}
\label{eq:multi-regress-1a}
Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k + u
\end{equation}

The coefficient $\beta_i$ on the regressor
$X_i$ for $i=1, \ldots, k$ measures the effect on $Y$ of a unit change
in $X_i$, *holding other $X$ constant*. 

*** An example

Suppose we have two regressors $X_1$ and $X_2$ and we are interested
in the effect of $X_1$ on $Y$. We can let $X_1$ change by $\Delta X_1$
and holding $X_2$ constant. Then, the new value of $Y$ is

\[ Y + \Delta Y = \beta_0 + \beta_1 (X_1 + \Delta X_1) + \beta_2 X_2  \]

Subtracting $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2$, we have
$\Delta Y = \beta_1 \Delta X_1$. That is
\[ \beta_1 = \frac{\Delta Y}{\Delta X}, \text{ holding } X_2 \text{ constant} \]

** The partial effect

If $Y$ and $X_i$ for $i = 1, \ldots, k$ are continuous and
differentiable variables, $\beta_i$ is as simply as the partial
derivative of $Y$ with respect to $X_i$. That is

\[\beta_i = \frac{\partial Y}{\partial X_i}\]

By the definition of a partial derivative, $\beta_i$ is just
the effect of a marginal change in $X_i$ on $Y$ holding other $X$
constant.

** Look at the data in terms of vectors and matrix
:PROPERTIES:
:BEAMER_opt:
:END:

#+NAME: fig:data-snapshot
#+CAPTION: The California data set in Excel
#+ATTR_LATEX: :width 0.4\textwidth :height 0.5\textheight
[[file:img/data_snapshot.png]]

- Each row represents an observation of all variables pertaining to a
  school district.
- Each column represents a variable with all
  observations.
- The whole dataset can be seen as a matrix. 

** Define variables in matrix notation
:PROPERTIES:
:BEAMER_opt: shrink
:END:

*** Write all the variables in vector and matrix notation

\begin{equation*}
\underbrace{
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{pmatrix},}_{\text{Dependent variable}}
\underbrace{
\mathbf{X} =
\begin{pmatrix}
1 & X_{11} & \cdots & X_{k1} \\
1 & X_{12} & \cdots & X_{k2} \\
\vdots & \vdots & \ddots & \vdots \\
1 & X_{1n} & \cdots & X_{kn}
\end{pmatrix},}_{\text{Independent variables}}
\underbrace{
\mathbf{u} =
\begin{pmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{pmatrix},\,}_{\text{Errors}}
\underbrace{
\boldsymbol{\beta} =
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{pmatrix}}_{\text{Coefficients}}
\end{equation*}

*** Write the multiple regression model in matrix notation

\begin{equation}
\label{eq:multi-regress-m}
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}
\end{equation}

*** Why do we use matrix notation
Concise, easy to derive properties; big-picture perspective.


** Two other ways to write the regression model

*** Write $\mathrm{X}$ in row vectors

- The i^{th} row in $\mathrm{X}$ is a $(k+1) \times 1$ vector
  \begin{equation*}
  \mathbi{x}_i =
  \begin{pmatrix}
  1 \\
  X_{1i} \\
  \vdots \\
  X_{ki} \\
  \end{pmatrix}. \text{ Thus, its transpose is }
  \mathbi{x}_i^{\prime} = (1, X_{1i}, \cdots, X_{ki})
  \end{equation*}

- We can write the regression model (Equation
  \ref{eq:multi-regress-m}) as

  \begin{equation}
  Y_i = \mathbi{x}^{\prime}_i \boldsymbol{\beta} + u_i,\; i = 1, \ldots, n
  \end{equation}

** Two other ways to write the regression model (cont'd)
*** Write $\mathrm{X}$ in vector vectors
- The i^{th} column in $\mathbf{X}$ is a $n \times 1$ vector
  \begin{equation*}
  \boldsymbol{X}_i =
  \begin{pmatrix}
  X_{i1} \\
  \vdots \\
  X_{in} \\
  \end{pmatrix}. \text{ The first column is }
  \boldsymbol{\iota} = 
  \begin{pmatrix}
  1 \\
  \vdots \\
  1
  \end{pmatrix}. \text{ Thus }
  \mathbf{X} = \left(\boldsymbol{\iota}, \boldsymbol{X}_1, \ldots, \boldsymbol{X}_k \right)
  \end{equation*}

- The regression model (Equation \ref{eq:multi-regress-m}) can be
  re-written as
  \begin{equation}
  \label{eq:multi-regress-m2}
  \mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1\boldsymbol{X}_1 + \cdots + \beta_k\boldsymbol{X}_k + \mathbf{u}
  \end{equation}


* The OLS Estimator in Multiple Regression
#+TOC: headlines [currentsection]

** The minimization problem and the OLS estimator

- The core idea of the OLS estimator for a multiple regression model
  remains the same as in a simple regression model: 
  *minimizing the sum of the squared residuals*. 

- Let $\mathbf{b} = [b_0, b_1, \ldots, b_k]^{\prime}$ be some estimators
  of $\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots,
  \beta_k]^{\prime}$.

- The predicted $Y_i$ is
  \begin{gather*}
  \hat{Y}_i = b_0 + b_1 X_{1i} + \cdots + b_k X_{ki} = \mathbi{x}^{\prime}_i
  \mathbf{b},\, i = 1, \ldots, \\
  \text{ or in matrix notation }  \hat{\mathbf{Y}} = \mathbf{Xb}
  \end{gather*}
  
- The residuals, i.e., the prediction mistakes, with $\mathbf{b}$ is
  \begin{gather*}
  \hat{u}_i = Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} = Y_i -
  \mathbi{x}^{\prime}_i \mathbf{b} \\
  \text{ or in matrix notation }  \hat{\mathbf{u}} = \mathbf{Y} - \mathbf{Xb}
  \end{gather*}

** The minimization problem and the OLS estimator (cont'd)

- The sum of the squared residuals is
  \begin{align*}
  S(\mathbf{b}) & = S(b_0, b_1, \ldots, b_k) = \sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki})^2 \\
  & = \sum_{i=1}^n (Y_i - \mathbf{x}^{\prime}_i \mathbf{b})^2 = (\mathbf{Y} -
  \mathbf{Xb})^{\prime}(\mathbf{Y}-\mathbf{Xb}) \\
  & = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} = \sum_{i=1}^n \hat{u}_i^2
  \end{align*}

- The OLS estimator is the solution to the following minimization problem:
  \begin{equation}
  \label{eq:ols-multi-regress}
  \operatorname*{min}_{\mathbf{b}}\: S(\mathbf{b}) = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}}
  \end{equation}

** The OLS estimator of $\boldsymbol{\beta}$ as a solution to the minimization problem

- Solve the minimization problem: 
  
  $$\text{F.O.C.: } \frac{\partial S(\mathbf{b})}{\partial b_j} = 0,
  \text{ for } j =
  0, 1, \ldots, k$$

- The derivative of $S(b_0, \ldots, b_k)$ with respect to $b_j$ is
  \begin{gather*}
  \label{eq:ols-wrt-bj}
  \frac{\partial }{\partial b_j} \sum_{i=1}^n \left(Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} \right)^2 = \\
  -2 \sum_{i=1}^n X_{ji} \left(Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} \right) = 0
  \end{gather*}

- There are $k+1$ such equations. Solving the system of equations, we
  obtain the OLS estimator $\hat{\boldsymbol{\beta}} = (\hat{\beta}_0, \ldots,
  \hat{\beta}_k)^{\prime}$.

** The OLS estimator in matrix notation

Let $\boldsymbol{\hat{\beta}}$ denote the OLS estimator. Then the
expression of $\boldsymbol{\hat{\beta}}$ is given by
\begin{equation}
\label{eq:betahat-mult}
\boldsymbol{\hat{\beta}} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}
\end{equation}

*** Some useful results of matrix calculus
To prove Equation (\ref{eq:betahat-mult}), we need to use some results
of matrix calculus.
\begin{equation}
\label{eq:matrix-calc}
\frac{\partial \mathbf{a}^{\prime} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a},\; \frac{\partial \mathbf{x}^{\prime} \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a},\; \text{ and } \frac{\partial \mathbf{x}^{\prime} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = (\mathbf{A} + \mathbf{A}^{\prime}) \mathbf{x}
\end{equation}
when $\mathbf{A}$ is symmetric, then $(\partial \mathbf{x}^{\prime}
\mathbf{A} \mathbf{x}) / (\partial \mathbf{x}) = 2\mathbf{A}
\mathbf{x}$

** The proof
:PROPERTIES:
:BEAMER_opt:
:END:

\begin{proof}[Proof of Equation (\ref{eq:betahat-mult})]
  \begin{equation*}
  S(\mathbf{b}) = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} = \mathbf{Y}^{\prime} \mathbf{Y} - \mathbf{b}^{\prime} \mathbf{X}^{\prime} \mathbf{Y} - \mathbf{Y}^{\prime} \mathbf{Xb} - \mathbf{b}^{\prime} \mathbf{X}^{\prime} \mathbf{Xb}
  \end{equation*}

  The first order conditions for minimizing $S(\mathbf{b})$ with respect to $\mathbf{b}$ is
  \begin{gather}
  -2 \mathbf{X}^{\prime} \mathbf{Y} - 2 \mathbf{X}^{\prime} \mathbf{Xb} = \mathbf{0} \notag \\
  \mathbf{X}^{\prime} \mathbf{Xb} = \mathbf{X}^{\prime} \mathbf{Y} \label{eq:ols-mult-eqs}
  \end{gather}

  Then
  \begin{equation*}
  \mathbf{b} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}
  \end{equation*}
  given that $\mathbf{X}^{\prime} \mathbf{X}$ is invertible.
\end{proof}

Note that Equation (\ref{eq:ols-mult-eqs}) represents a system of
equations with $k+1$ equations.

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model

The simple linear regression model written in matrix notation is

\begin{equation*}
\mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1 \mathbf{X}_1 + \mathbf{u} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}
\end{equation*}

where

\begin{equation*}
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix},\,
\mathbf{X} =
\begin{pmatrix}
\boldsymbol{\iota} & \mathbf{X}_1
\end{pmatrix}
=
\begin{pmatrix}
1 & X_{11} \\
\vdots & \vdots \\
1 & X_{1n}
\end{pmatrix},\,
\mathbf{u} =
\begin{pmatrix}
u_1 \\
\vdots \\
u_n
\end{pmatrix},\,
\boldsymbol{\beta} =
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\end{pmatrix}
\end{equation*}

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:

Let's get the components in Equation (\ref{eq:betahat-mult}) step by
step.

*** Step (1): compute $\left(\mathbf{X}^{\prime}\mathbf{X}\right)$

\begin{align*}
\mathbf{X}^{\prime}\mathbf{X} & =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \\
\mathbf{X}_1^{\prime}
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{\iota} & \mathbf{X}_1
\end{pmatrix} =
\begin{pmatrix}
1 & \cdots & 1 \\
X_{11} & \cdots & X_{1n}
\end{pmatrix}
\begin{pmatrix}
1 & X_{11} \\
\vdots & \vdots \\
1 & X_{1n}
\end{pmatrix} \\
& =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \boldsymbol{\iota} & \boldsymbol{\iota}^{\prime} \mathbf{X}_1 \\
\mathbf{X}_1^{\prime} \boldsymbol{\iota} & \mathbf{X}_1^{\prime} \mathbf{X}_1
\end{pmatrix} =
\begin{pmatrix}
n & \sum_{i=1}^n X_{1i} \\
\sum_{i=1}^n X_{1i} & \sum_{i=1}^n X_{1i}^2
\end{pmatrix}
\end{align*}

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:
*** Step (2): compute $\left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}$

**** The inverse of a $2 \times 2$ matrix
  \begin{equation*}
  \begin{pmatrix}
  a_{11} & a_{12} \\
  a_{21} & a_{22}
  \end{pmatrix}^{-1}
  =\frac{1}{a_{11}a_{22} - a_{12}a_{21}}
  \begin{pmatrix}
  a_{22} & -a_{12} \\
  -a_{21} & a_{11}
  \end{pmatrix}
  \end{equation*}

**** The inverse of $\mathbf{X}^{\prime}\mathbf{X}$
  \begin{equation*}
  \left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1} =
  \frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
  \begin{pmatrix}
  \sum_{i=1}^n X_{1i}^2 & - \sum_{i=1}^n X_{1i} \\
  -\sum_{i=1}^n X_{1i} & n
  \end{pmatrix}
  \end{equation*}

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:

*** Step (3): compute $\mathbf{X}^{\prime} \mathbf{Y}$

\begin{equation*}
\mathbf{X}^{\prime} \mathbf{Y} =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \\
\mathbf{X}_1^{\prime}
\end{pmatrix}
\mathbf{Y} =
\begin{pmatrix}
1 & \cdots & 1 \\
X_{11} & \cdots & X_{1n}
\end{pmatrix}
\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix} =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \mathbf{Y} \\
\mathbf{X}_1^{\prime} \mathbf{Y}
\end{pmatrix} =
\begin{pmatrix}
\sum_{i=1}^n Y_i \\
\sum_{i=1}^n X_{1i} Y_i
\end{pmatrix}
\end{equation*}

*** Step (4): compute $\boldsymbol{\hat{\beta}}=(\mathbf{X}^{\prime}\mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}$

\begin{align*}
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix} & =
\frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
\begin{pmatrix}
\sum_{i=1}^n X_{1i}^2 & - \sum_{i=1}^n X_{1i} \\
-\sum_{i=1}^n X_{1i} & n
\end{pmatrix}
\begin{pmatrix}
\sum_{i=1}^n Y_i \\
\sum_{i=1}^n X_{1i} Y_i
\end{pmatrix} \\
& =
\frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
\begin{pmatrix}
\sum_{i=1}^n X_{1i}^2 \sum_{i=1}^n Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n X_{1i}Y_i \\
-\sum_{i=1}^n X_{1i} \sum_{i=1}^n Y_i + n \sum_{i=1}^n X_{1i} Y_i
\end{pmatrix}
\end{align*}

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:

*** The formula of $\hat{\beta}_1$
  \begin{equation*}
  \hat{\beta}_1 = \frac{n \sum_{i=1}^n X_{1i} Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n Y_i}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2} = \frac{\sum_{i=1}^n (X_{1i} - \bar{X}_1)(Y_i - \bar{Y})}{\sum_{i=1}^n (X_{1i} - \bar{X}_1)^2}
  \end{equation*}

*** The formula of $\hat{\beta}_0$
  \begin{equation*}
  \hat{\beta}_0 = \frac{\sum_{i=1}^n X_{1i}^2 \sum_{i=1}^n Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n X_{1i}Y_i}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2} = \bar{Y} - \hat{\beta}_1 \bar{X}_1
  \end{equation*}

** Application to Test Scores and the Student-Teacher Ratio
:PROPERTIES:
:BEAMER_opt: shrink,plain
:END:
*** The simple regression compared with the multiple regression
The estimated simple linear regression model is
\[ \widehat{TestScore} = 698.9 - 2.28 \times STR \]

The estimated multiple linear regression model is
\[ \widehat{TestScore} = 686.0 - 1.10 \times STR - 0.65 \times PctEL
\]

*** Explanations
- The interpretation of the new estimated coefficient on /STR/ is,
  *holding the percentage of English learners constant*, a unit
  decrease in /STR/ is estimated to increase test scores by 1.10
  points.
- We can also interpret the estimated coefficient on /PctEL/ as,
  holding /STR/ constant, one unit decrease in /PctEL/ increases test
  scores by 0.65 point.
- The magnitude of the negative effect of /STR/ on test scores in the
  multiple regression is approximately half as large as when /STR/ is
  the only regressor.

** COMMENT Warm-up exercises
:PROPERTIES:
:BEAMER_opt: shrink
:END:
*** 1) In the multiple regression model you estimate the effect on Yi of a unit change in one of the Xi while holding all other regressors constant. This
- A) :: makes little sense, because in the real world all other variables change.
- B) :: corresponds to the economic principle of mutatis mutandis.
- C) :: leaves the formula for the coefficient in the single explanatory variable case unaffected.
- D) :: corresponds to taking a partial derivative in mathematics.
\pause
Answer:  D

*** 2) The multiple regression model can be written in matrix form as follows:
- A) :: $\mathbf{Y} = \mathbf{X} \boldsymbol{\beta}$
- B) :: $\mathbf{Y} = \mathbf{X} + \mathbf{U}$
- C) :: $\mathbf{Y} = \boldsymbol{\beta} \mathbf{X} + \mathbf{U}$
- D) :: $\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{U}$
\pause
Answer:  D

*** 3) Minimization of $\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki})^2$ results in
- A) :: $\mathbf{X}^{\prime}\mathbf{Y} = \mathbf{X} \hat{\boldsymbol{\beta}}$
- B) :: $\mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{0}_{k+1}$
- C) :: $\mathbf{X}^{\prime} (\mathbf{Y} - \mathbf{X} \hat{\boldsymbol{\beta}}) = \mathbf{0}_{k+1}$
- D) :: $\mathbf{R} \boldsymbol{\beta} = \mathbf{r}$
\pause
Answer:  C


* TODO Measures of Fit in Multiple Regression
#+TOC: headlines [currentsection]

** The Standard errors of the regression (SER)

- The standard error of regression (SER) estimates the standard deviation of the error term
  $\mathbf{u}$. In multiple regression, the SER is
  \begin{equation}
  \label{eq:ser-m}
  SER = s_{\hat{u}},\, \text{ where } s^2_{\hat{u}} = \frac{\sum_{i=1}^n \hat{u}_i^2}{n-k-1} =\frac{\mathbf{\hat{u}}^{\prime} \mathbf{\hat{u}}}{n-k-1} = \frac{SSR}{n-k-1}
  \end{equation}

- $SSR$ is divided by $(n-k-1)$ because there are $n$ observations and
  $(k+1)$ coefficients to be estimated.

** The R^2

*** TSS, ESS, and SSR in multiple regression models

- The total sum of squares (TSS) :: $TSS = \sum_{i=1}^n (Y_i - \bar{Y})^2$
- The explained sum of squares (ESS) :: $ESS = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$
- The sum of squared residuals (SSR) :: $SSR = \sum_{i=1}^n \hat{u}_i^2$

*** Matrix notation 
\begin{equation*}
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{pmatrix},\;
\boldsymbol{\iota} =
\begin{pmatrix}
1 \\
1 \\
\vdots \\
1
\end{pmatrix},\;
\mathbf{y} = 
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix}
 =
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{pmatrix}
-
\begin{pmatrix}
\bar{Y} \\
\bar{Y} \\
\vdots \\
\bar{Y}
\end{pmatrix}
=
\mathbf{Y} - \bar{Y} \boldsymbol{\iota}
\end{equation*}
Therefore, $\mathbf{y}$ represents *the deviation from the mean* of
$Y_i,\; i=1,\ldots,n$. Similarly, we can define the deviation from the
mean of $\hat{Y}_i,\, i=1, \ldots, n$ as $\hat{\mathbf{y}} =
\hat{\mathbf{Y}} - \bar{Y} \boldsymbol{\iota}$.


** R^2 (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:
Then we can rewrite
$TSS, ESS,\, \text{ and } SSR$ as
\[ TSS = \mathbf{y}^{\prime} \mathbf{y},\; ESS =
\hat{\mathbf{y}}^{\prime} \hat{\mathbf{y}},\; \text{ and } SSR =
\hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} \]

In multiple regression, the relationship that
\[ TSS = ESS + SSR, \text{ or } \mathbf{y}^{\prime} \mathbf{y} =
\hat{\mathbf{y}}^{\prime} \hat{\mathbf{y}} + \hat{\mathbf{u}}^{\prime}
\hat{\mathbf{u}}\]
still holds so that we can define R^2 as
\begin{equation}
\label{eq:r2-center}
R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}
\end{equation}

** Limitations of R^2
:PROPERTIES:
:BEAMER_opt:
:END:
1. R^{2} is valid only if a regression model is estimated using the OLS
   since otherwise it would not be true that $TSS = ESS + SSR$.
2. R^{2} that is defined using the deviation from the mean is only valid
   when a constant term is included in regression. Otherwise, use the
   uncentered version of R^{2}, which is also defined as
   \begin{equation}
   \label{eq:r2-uncenter}
   R^2_u = \frac{EES}{TSS} = 1 - \frac{SSR}{TSS}
   \end{equation}
   where $TSS = \sum_{i=1}^n Y_i^2 = \mathbf{Y}^{\prime} \mathbf{Y}$,
   $ESS = \sum_{i=1}^2 \hat{Y}_i^2 = \hat{\mathbf{Y}}^{\prime}
   \hat{\mathbf{Y}}$, and $SSR = \sum_{i=1}^n \hat{u}_i^2 =
   \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}}$, using the uncentered
   variables.  Note that in a regression without a constant term, the
   equality $TSS = ESS + SSR$ is still true.

** Limitation of R^2 (cont'd)
:PROPERTIES:
:BEAMER_opt:
:END:
3. R^{2} increases whenever an additional regressor is included in a
   multiple regression model, unless the estimated coefficient on the
   added regressor is exactly zero. Consider two regression models
   \begin{align}
   \mathbf{Y} &= \beta_0 + \beta_1 \mathbf{X}_1 + \mathbf{u}
   \label{eq:ex-eq-1} \\
   \mathbf{Y} &= \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \mathbf{u} \label{eq:ex-eq-2}
   \end{align}
   Since both models use the same $\mathbf{Y}$, $TSS$ must be the
   same. If the OLS estimator $\hat{\beta}_2$ does not equal 0, then
   $SSR$ in Equation (\ref{eq:ex-eq-1}) is always larger than that of
   Equation (\ref{eq:ex-eq-2})  since the former $SSR$ is minimized
   with respect to $\beta_0, \beta_1$ and with the constraint of
   $\beta_2 = 0$ and the latter is minimized without the constraint
   over $\beta_2$.

** The adjusted R^2
The adjusted R^2 is, or $\bar{R}^2$, is a modified version of R^2 in
Equation (\ref{eq:r2-center}). The $\bar{R}^2$ improves R^2 in the
sense that it does not necessarily increase when a new regressor is
added. The $\bar{R}^2$ is

\begin{equation}
\label{eq:adj-r2}
\bar{R}^2 = 1 - \frac{SSR / (n-k-1)}{TSS / (n-1)} = 1 - \frac{n-1}{n-k-1}\frac{SSR}{TSS} = 1 - \frac{s^2_u}{s^2_Y}
\end{equation}

where $s^2_u$ is the sample variance of the OLS residuals, which is given
in Equation (\ref{eq:ser-m}); $s^2_Y$ is the sample variance of $Y$.

** Properties of $\bar{R}^2$
:PROPERTIES:
:BEAMER_opt:
:END:
- The adjustment is made by dividing $SSR$ and $TSS$ by their
  corresponding degrees of freedom, which is $n-k-1$ and $n-1$
  respectively.
- $s^2_u$ is the sample variance of the OLS residuals, which is given
  in Equation (\ref{eq:ser-m}); $s^2_Y$ is the sample variance of $Y$.
- The definition of the $\bar{R}^2$ in Equation (\ref{eq:adj-r2}) is
  valid only when a constant term is included in the regression
  model.
- Since $\frac{n-1}{n-k-1} > 1$, then it is always true that
  the $\bar{R}^2 < R^2$.
- On one hand $k \uparrow\, \Rightarrow\, \frac{SSR}{TSS} \downarrow$. On
  the other hand, $k \uparrow\, \Rightarrow \frac{n-1}{n-k-1}
  \uparrow$. Whether $\bar{R}^2$ increases or decreases depends on
  which of these effects is stronger.
- The $\bar{R}^2$ can be negative. This happens when the regressors,
  taken together, reduce the sum of squared residuals by such a small
  amount that his reduction fails to offset the factor $\frac{n-1}{n-k-1}$.

** The usefulness of the R^2 and $\bar{R}^2$
- Both $R^2$ and $\bar{R}^2$ are valid when the regression model is
  estimated by the OLS estimators. R^2 computed with estimators other
  than the OLS ones is usually called /pseudo/ R^2.
- Their importance as measures of fit cannot be overstated. We cannot heavily
  reply on R^2 or $\bar{R}^2$ to judge whether some regressors should
  be included in the model or not.

* TODO COMMENT The Frisch-Waugh-Lovell Theorem
#+TOC: headlines [currentsection]
** The grouped regressors
Consider a multiple regression model
\begin{equation*}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{u}
\end{equation*}
which has $k$ regressors. We can group these $k$ regressors into two
subset, $\mathbf{X}_1$ with $k_1$ regressors and $\mathbf{X}_2$ with
$k_2$ regressors, with which we rewrite the multiple regression model
above as
\begin{equation}
\label{eq:mult-reg-2g}
\mathbf{Y} = \mathbf{X}_1\boldsymbol{\beta}_1 + \mathbf{X}_2 \boldsymbol{\beta}_2 + \mathbf{u}
\end{equation}

** An estimation strategy
Suppose that we are interested in $\boldsymbol{\beta}_1$ in Equation
(\ref{eq:mult-reg-2g}). We can perform the following steps to
estimate $\boldsymbol{\beta}_1$:
1. Regress each regressor in $\mathbf{X}_1$ on all regressors in $\mathbf{X}_2$,
   denoting the residuals from this regression as
   $\widetilde{\mathbf{X}}_1$.
2. Regress $\mathbf{Y}$ on all regressors in $\mathbf{X}_2$, denoting
   the residuals from this regression as $\widetilde{\mathbf{Y}}$.
3. Regress $\widetilde{\mathbf{Y}}$ on $\widetilde{\mathbf{X}}_1$, and
   obtain the estimates of $\boldsymbol{\beta}_1$ as
   $(\widetilde{\mathbf{X}}_1^{\prime} \widetilde{\mathbf{X}}_1)^{-1}
   \widetilde{\mathbf{X}}_1^{\prime} \widetilde{\mathbf{Y}}$.

** The Frisch-Waugh-Lovell Theorem
The Frisch-Waugh-Lovell (FWL) Theorem states that
1) the OLS estimates of
  $\boldsymbol{\beta}_1$ using the steps above and the OLS estimates of
  $\boldsymbol{\beta}_1$ computed directly from Equation
  (\ref{eq:mult-reg-2g}) are numerically identical.
2) the residuals from the regression of $\widetilde{\mathbf{Y}}$ on
  $\widetilde{\mathbf{X}}_1$ and the residuals from Equation
  (\ref{eq:mult-reg-2g}) are numerically identical.

The proof of the FWL theorem is beyond the scope of this
proof. Interested students may refer to Exercise 18.7.
Understanding the meaning of this theorem is much more important than
understanding the proof.

** An understanding of the FWL theorem
The FWL theorem provides a mathematical statement of how the multiple
regression coefficient $\hat{\boldsymbol{\beta}}_1$ estimates the
effect on $\mathbf{Y}$ of $\mathbf{X}_1$, controlling for other
$\mathbf{X}$.

- Step 1 purges the effects of other X's on X_1
- Step 2 purges the effects of other X's on Y
- Step 3 estimates the effect of X_1 on Y using what is left over
  after removing the effect of other X's.

** An example of the FWL theorem
:PROPERTIES:
:BEAMER_opt: shrink
:END:
Consider a regression model with single regressor
\[ Y_i = \beta_0 +
\beta_1 X_i + u_i,\; i=1, \ldots, n
\]

Following the estimation strategy in the FWL theorem, we can carry out the following regressions,
1. Regress $Y_i$ on 1. That is, estimate the model $Y_i = \alpha + e_i$
   Then, the OLS estimator of $\alpha$ is
   $\bar{Y}$ and the residuals is $y_i = Y_i - \bar{Y}$
2. Similarly, regress $X_{i}$ on 1. Then
   the residuals from these two regressions are $x_{i} = X_{i} -
   \bar{X}$.
3. Regress $y_i$ on $x_{i}$ without intercept. That is,
   estimate the model
   \[ y_i = \beta_1 x_{i} + v_i \]
Then the OLS estimate of $\beta_1$ in the reduced model is the same as that in the original model.

We can obtain $\hat{\beta_1}$ directly by applyin the formula in Equation (\ref{eq:betahat-mult}). That is
\[ \hat{\beta}_1 = (\mathbf{x}^{\prime} \mathbf{x})^{-1} \mathbf{x}^{\prime} \mathbf{y} = \frac{\sum_i x_{i} y_i}{\sum_i x_{i}^2} \]

* TODO COMMENT The Least Squares Assumptions in Multiple Regression
#+TOC: headlines [currentsection]
** The least squares assumptions in Multiple Regression
- Assumption #1 :: $E(u_i | \mathbf{X}_i) = 0$. The conditional mean
                   of $u_i$ given $X_{1i}, X_{2i}, \ldots, X_{ki}$ has
                   mean of zero. This is the key assumption to assure
                   that the OLS estimators are unbiased.

- Assumption #2 :: $(Y_i, \mathbf{X}_i^{\prime})\, i=1, \ldots, n$ are
                   i.i.d. This assumption holds automatically if the
                   data are collected by simple random sampling.

- Assumption #3 :: Large outliers are unlikely, i.e.,, $0 <
                   E(\mathbf{X}^4) < \infty$ and $0 < E(\mathbf{Y}^4)
                   < \infty$. That is, the dependent variables and
                   regressors have finite kurtosis.
- Assumption #4 :: No *perfect multicollinearity*. The regressors are
                   said to exhibit perfect multicollinearity (or to
                   be perfectly multicollinear) if one of the
                   regressor is a perfect linear function of the other
                   regressors.

* TODO COMMENT The Statistical Properties of the OLS Estimators in Multiple Regression
#+TOC: headlines [currentsection]
** Unbiasedness and consistency \\ \small Unbiasedness
:PROPERTIES:
:BEAMER_opt: shrink
:END:
*** The definition of unbiasedness
The OLS estimators $\hat{\boldsymbol{\beta}}$ is unbiased if
$E(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}$.

*** The proof
To show the unbiasedness, we can rewrite $\hat{\boldsymbol{\beta}}$ as
follows,
#+BEGIN_LaTeX
\begin{equation}
\label{eq:bhat-m-a}
\hat{\boldsymbol{\beta}} = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{Y}
= \left(\mathbf{X}^{\prime} \mathbf{X} \right)^{-1} \mathbf{X}^{\prime} (\mathbf{X} \boldsymbol{\beta} + \mathbf{u})
= \boldsymbol{\beta} + \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{u}
\end{equation}
#+END_LaTeX
Thus, the conditional expectation of $\hat{\boldsymbol{\beta}}$ is
#+BEGIN_LaTeX
\begin{equation}
\label{eq:bhat-unbias}
E(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \boldsymbol{\beta} + \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} E(\mathbf{u} | \mathbf{X}) = \boldsymbol{\beta}
\end{equation}
#+END_LaTeX
in which $E(\mathbf{u} | \mathbf{X}) = 0$ from the first least squares
assumption.

\vspace{0.2cm}
Using the law of iterated expectation, we have
#+BEGIN_LaTeX
\[ E(\hat{\boldsymbol{\beta}}) = E(E(\hat{\boldsymbol{\beta}} |
\mathbf{X})) = E(\boldsymbol{\beta}) = \boldsymbol{\beta} \]
#+END_LaTeX
Therefore, $\hat{\boldsymbol{\beta}}$ is an unbiased estimator of
$\boldsymbol{\beta}$.

** Unbiasedness and consistency \\ \small Consistency
*** Definition of consistency
The OLS estimator $\hat{\boldsymbol{\beta}}$ is consistent if as $n
\rightarrow \infty$, $\hat{\boldsymbol{\beta}}$ will converge to
$\boldsymbol{\beta}$ in probability, that is, $\plim_{n \rightarrow
\infty} \hat{\boldsymbol{\beta}} = \boldsymbol{\beta}$.

*** A scratch of the proof
From Equation (\ref{eq:bhat-m-a}), we can have
\begin{equation*}
\plim_{n \rightarrow \infty} \hat{\boldsymbol{\beta}} = \boldsymbol{\beta} + \plim_{n \rightarrow \infty} \left(\frac{\mathbf{X}^{\prime} \mathbf{X}}{n} \right)^{-1} \plim_{n \rightarrow \infty}\left( \frac{\mathbf{X}^{\prime} \mathbf{u}}{n} \right)
\end{equation*}
Let us first make an assumption, which is usually true, that
\begin{equation}
\label{eq:plim-bhat-m}
 \plim_{n \rightarrow \infty} \frac{1}{n} \mathbf{X}^{\prime}
\mathbf{X} = \underset{(k+1) \times (k+1)}{\mathbf{Q_X}}
\end{equation}
where $\mathbf{Q_X}$ is a nonstochastic matrix with full rank.

** Unbiasedness and consistency \\ \small Consistency (cont'd)
*** A scratch of the proof
Then let us look at $\plim_{n \rightarrow \infty} \frac{1}{n}
\mathbf{X}^{\prime} \mathbf{u}$ which can be rewritten as
#+BEGIN_LaTeX
\[ \plim_{n \rightarrow  \infty} \frac{1}{n} \sum_{i=1}^n \mathbf{X}_i u_i = E(\mathbf{X}_i
u_i) = 0 \]
#+END_LaTeX
Here we use Assumption #1 $E(u_i | \mathbf{X}_i) = 0$, implying that $E(\mathbf{X}_i u_i) =
E(\mathbf{X}_iE(u_i | \mathbf{X}_i)) = 0$.
\vspace{0.2cm}
And by Assumptions #2 and #3, we know that $\mathbf{X}_i u_i$ are i.i.d. and have
positive finite variance.
\vspace{0.2cm}
Therefore, we can conclude that
\[ \plim_{n \rightarrow \infty} \hat{\boldsymbol{\beta}} = \boldsymbol{\beta}  \]
That is, $\hat{\boldsymbol{\beta}}$ is consistent.

** The efficiency \\ \small The Gauss-Markov conditions
The Gauss-Markov conditions for multiple regression are
1. $E(\mathbf{u} | \mathbf{X}) = 0$,
2. $\var(\mathbf{u} | \mathbf{X}) = E(\mathbf{uu}^{\prime} |
   \mathbf{X}) = \sigma^2_u \mathbf{I}_n$ (homoskedasticity),
3. $\mathbf{X}$ has full column rank (no perfect multicollinearity).

** Understanding the Gauss-Markov conditions
:PROPERTIES:
:BEAMER_opt: shrink
:END:
Like in the regression model with single regressor, the five least
squares assumptions can be summarized by the Gauss-Markov conditions
as
- Assumptions #1 and #2 imply that $E(\mathbf{u} | \mathbf{X}) = \mathbf{0}_n$.
  \[E(u_i | \mathbf{X}) = E(u_i | [\mathbf{X_1}, \ldots, \mathbf{X}_i,
  \ldots, \mathbf{X}_n]^{\prime}) = E(u_i | \mathbf{X}_i) = 0\]
  in  which the second equality follows Assumption #2 that
  $\mathbf{X}_i,\,\text{ for } i = 1,\ldots,n$ are independent.

- Assumption #1, #2, and the additional assumption of homoskedasticity
  imply that $\var(\mathbf{u} | \mathbf{X}) = \sigma^2_u
  \mathbf{I}_n$.
  \begin{equation*}
  \var(\mathbf{u} | \mathbf{X}) =
  \begin{pmatrix}
  \sigma^2_u & 0 & \cdots & 0 \\
  0 & \sigma^2_u & \cdots & 0 \\
  \vdots & \vdots & \ddots & \vdots \\
  0 & 0 & \cdots & \sigma^2_u
  \end{pmatrix}
  = \sigma^2_u \mathbf{I}_n
  \end{equation*}

** A digression for the covariance matrix of a vector
:PROPERTIES:
:BEAMER_opt: shrink,plain
:END:
For a random vector $\mathbf{x}$, the variance of $\mathbf{x}$ is a
covariance matrix defined as
  #+BEGIN_LaTeX
  \[ \var(\mathbf{x}) =
  E\left((\mathbf{x}-E(\mathbf{x}))(\mathbf{x}-E(\mathbf{x}))^{\prime}\right) \]
  #+END_LaTex
Since $E(\mathbf{u} | \mathbf{X}) = 0$, its covariance matrix,
conditioned on $\mathbf{X}$, is
  #+BEGIN_LaTeX
  \[ \var(\mathbf{u} | \mathbf{X}) = E(\mathbf{u} \mathbf{u}^{\prime} | \mathbf{X})
  \]
  #+END_LaTeX
where
  #+BEGIN_LaTeX
  \begin{equation*}
  \mathbf{u} \mathbf{u}^{\prime} =
  \begin{pmatrix}
  u_1^2 & u_1 u_2 & \cdots &u_1 u_n \\
  u_2 u_1 & u_2^2 & \cdots & u_2 u_n \\
  \vdots & \vdots & \ddots & \vdots \\
  u_n u_1 & u_n u_2 & \cdots & u_n^2 \\
  \end{pmatrix}
  \end{equation*}
  #+END_LaTeX
Thus, in the matrix $\mathbf{u} \mathbf{u}^{\prime}$,
- the expectation of the diagonal elements, conditioned on $\mathbf{X}$,
  are the conditional variance of $u_i$ which is $\sigma^2_u$ because
  of homoskedasticity.
- The conditional expectation of the off-diagonal elements are the
  covariance of $u_i$ and $u_j$, conditioned on $\mathbf{X}$. Since
  $u_i$ and $u_j$ are independent according to Assumption #2, $E(u_i
  u_j | \mathbf{X}) = 0$.

** The Gauss-Markov Theorem
:PROPERTIES:
:BEAMER_opt:
:END:
#+BEGIN_QUOTE
If the Gauss-Markov conditions hold in the multiple regression model,
then the OLS estimator $\hat{\boldsymbol{\beta}}$ is more efficient
than any other linear unbiased estimator $\tilde{\boldsymbol{\beta}}$,
in the sense that $\var(\tilde{\boldsymbol{\beta}}) -
\var(\hat{\boldsymbol{\beta}})$ is a positive semidefinite
matrix. That is, the OLS estimator is BLUE.
#+END_QUOTE

\vspace{0.3cm}
That $\var(\tilde{\boldsymbol{\beta}}) -
\var(\hat{\boldsymbol{\beta}})$ is a positive semidefinite matrix
means that for any nonzero $(k+1) \times 1$ vector $\mathbf{c}$,
\[ \mathbf{c}^{\prime}\left(\var(\tilde{\boldsymbol{\beta}}) -
\var(\hat{\boldsymbol{\beta}})\right) \mathbf{c} \geq 0 \]
or we can simply write as
\[ \var(\tilde{\boldsymbol{\beta}}) \geq \var(\hat{\boldsymbol{\beta}})  \]
The equality holds only when $\tilde{\boldsymbol{\beta}} =
\hat{\boldsymbol{\beta}}$.

** Linear conditionally unbiased estimators
:PROPERTIES:
:END:
Any linear estimator of $\boldsymbol{\beta}$ can be written as
\[ \tilde{\boldsymbol{\beta}} = \mathbf{Ay} =
\mathbf{AX}\boldsymbol{\beta} + \mathbf{Au} \]
where $\mathbf{A}$ is a weight matrix depending only on $\mathbf{X}$
not on $\mathbf{y}$.

For $\tilde{\boldsymbol{\beta}}$ to be conditionally unbiased, we must
have
\begin{equation*}
E(\tilde{\boldsymbol{\beta}} | \mathbf{X}) = \mathbf{AX} \boldsymbol{\beta} + \mathbf{A} E(\mathbf{u} | \mathbf{X}) = \boldsymbol{\beta}
\end{equation*}
which only holds when $\mathbf{AX} = \mathbf{I}_{k+1}$.

The OLS estimator $\hat{\boldsymbol{\beta}}$ is a linear conditionally
unbiased estimator with $\mathbf{A} = \left(\mathbf{X}^{\prime}
\mathbf{X}\right)^{-1} \mathbf{X}^{\prime}$. Obviously, $\mathbf{AX} =
\mathbf{I}_{k+1}$ is true for $\hat{\boldsymbol{\beta}}$.

** The conditional covariance matrix of $\hat{\boldsymbol{\beta}}$ :B_frame:
:PROPERTIES:
:BEAMER_env: frame
:END:
The conditional variance matrix of $\hat{\boldsymbol{\beta}}$ can be
derived as follows
\begin{equation*}
\begin{split}
\var(\hat{\boldsymbol{\beta}} | \mathbf{X}) &= E\left[ (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})^{\prime} | \mathbf{X}\right] \\
&= E\left[ \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} \mathbf{u} \mathbf{u}^{\prime} \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1} | \mathbf{X} \right] \\
&= \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} E(\mathbf{uu}^{\prime} | \mathbf{X}) \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{split}
\end{equation*}
Then, by the second Gauss-Markov condition, we have
\begin{equation}
\label{eq:varbhat-hm}
\var(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \mathbf{X}^{\prime} (\sigma^2_u \mathbf{I}_n) \mathbf{X} (\mathbf{X}^{\prime} \mathbf{X})^{-1} = \sigma^2_u (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{equation}
which is the *homoskedasticity-only* covariance matrix.

** The conditional covariance matrix of $\hat{\boldsymbol{\beta}}$ (cont'd)
:PROPERTIES:
:BEAMER_opt: shrink
:END:
If the homoskedasticity assumption does not hold, denote the
covariance matrix of $\mathbf{u}$ as
\[ \var(\mathbf{u} | \mathbf{X}) = \boldsymbol{\Omega} \]

Heteroskedasticity means that the diagonal elements of
$\mathbf{\Omega}$ can be different (i.e. $\var(u_i | \mathbf{X}) =
\sigma^2_i \text{ for } i=1, \ldots, n)$, while the off-diagonal
elements are zeros, that is
#+BEGIN_LaTeX
\begin{equation*}
\boldsymbol{\Omega} =
\begin{pmatrix}
\sigma^2_1 & 0 & \cdots & 0 \\
0 & \sigma^2_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2_n
\end{pmatrix}
\end{equation*}
#+END_LaTeX

Define $\boldsymbol{\Sigma} = \mathbf{X}^{\prime} \boldsymbol{\Omega}
\mathbf{X}$. Then the *heteroskedasticity-robust covariance matrix* of
$\hat{\boldsymbol{\beta}}$ is
#+BEGIN_LaTeX
\begin{equation}
\label{eq:varbhat-ht}
\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \boldsymbol{\Sigma} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{equation}
#+END_LaTeX

** The asymptotic normal distribution
In large samples, the OLS estimator $\hat{\boldsymbol{\beta}}$ has the
multivariate normal asymptotic distribution as
\begin{equation}
\label{eq:normal-bhat-m}
\hat{\boldsymbol{\beta}} \rarrowd{d} N(\boldsymbol{\beta}, \boldsymbol{\Sigma_{\hat{\boldsymbol{\beta}}}})
\end{equation}
where $\boldsymbol{\Sigma_{\hat{\boldsymbol{\beta}}}} =
\var(\hat{\boldsymbol{\beta}} | \mathbf{X})$ for which use
Equation (\ref{eq:varbhat-hm}) for the homoskedastic case and Equation
(\ref{eq:varbhat-ht}) for the heteroskedastic case.

\vspace{0.3cm}
The proof of the asymptotic normal distribution and the multivariate
central limit theorem are given in Chapter 18.

* TODO COMMENT The Omitted Variable Bias
#+TOC: headlines [currentsection]
** The definition of the omitted variable bias
:PROPERTIES:
:END:
 The *omitted variable bias* is the bias in the OLS esitmator that arises
 when the included regressors, $\mathbf{X}$, are correlated with
 omitted variables, $\mathbf{Z}$, where $\mathbf{X}$ may include $k$
 regressors, $\mathbf{X}_1, \ldots, \mathbf{X}_k$, and $\mathbf{Z}$
 may include $l$ omitted variables, $\mathbf{Z}_1, \ldots,
 \mathbf{Z}_m$. The omitted variable bias occurs
 when two conditions are met
 1. $\mathbf{X}$ is correlated with some omitted variables in $\mathbf{Z}$.
 2. The omitted variables are determinants of the dependent variable
    $\mathbf{Y}$.

** The reason for the omitted variable bias
:PROPERTIES:
:BEAMER_opt: shrink,plain
:END:

\vspace{0.2cm}

Suppose that the true model is

\begin{equation}
\label{eq:omb-1}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \mathbf{u}
\end{equation}
in which Assumption #1 is true. That is, $E(\mathbf{u} | \mathbf{X},
\mathbf{Z}) = 0$. We further assume that $\cov(\mathbf{X}, \mathbf{Z})
\neq 0$

\vspace{0.2cm}

However, we mistakenly exclude $\mathbf{Z}$ in regression analysis and
estimate a short model

\begin{equation}
\label{eq:omb-2}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
\end{equation}

Since $\boldsymbol{\epsilon}$ represents all other factors that are not
in Equation (\ref{eq:omb-2}), including $\mathbf{Z}$, and
$\cov(\mathbf{X}, \mathbf{Z}) \neq 0$, this means that
$\cov(\mathbf{X}, \boldsymbol{\epsilon}) \neq 0$, which implies that
$E(\boldsymbol{\epsilon} | \mathbf{X}) \neq 0$.

\vspace{0.2cm}
Therefore, Assumption #1 does not hold for the short
model, which means that the OLS estimator of Equation (\ref{eq:omb-2})
is biased.

** An informal explanation of the omitted variable bias
:PROPERTIES:
:BEAMER_opt: shrink
:END:
The OLS estimator of Equation (\ref{eq:omb-2}) is $\hat{\boldsymbol{\beta}} &=
(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
\mathbf{Y}$. Plugging $\mathbf{Y}$ with the true model, we have
\[\hat{\boldsymbol{\beta}} = (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\mathbf{X}^{\prime} (\mathbf{X}\boldsymbol{\beta} +
\mathbf{Z}\boldsymbol{\gamma} + \mathbf{u})
= \boldsymbol{\beta} + (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\mathbf{X}^{\prime} \mathbf{Z} \boldsymbol{\gamma} +
(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{u} \]

Taking the expectation of $\hat{\boldsymbol{\beta}}$,
conditioned on $\mathbf{X}$, we have
\begin{equation}
\label{eq:omb-3}
E(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \boldsymbol{\beta}
+ \underbrace{(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Z} \boldsymbol{\gamma}}_{\mathclap{\text{omitted variable bias}}} + \mathbf{0}
\end{equation}
The second term in the equation above usually does not equal zero
unless either
1) $\boldsymbol{\gamma} = \mathbf{0}$, which means that
   $\mathbf{Z}$ are not determinants of $\mathbf{Y}$ in the true model, or
2) $\mathbf{X}^{\prime} \mathbf{Z} = 0$, which means that
   $\mathbf{X}$ and $\mathbf{Z}$ are not correlated.
\vspace{0.2cm}
Therefore, if these two conditions do not hold,
$\hat{\boldsymbol{\beta}}$ for the short model is biased. And the
magnitude and direction of the bias is determined by
$\mathbf{X}^{\prime} \mathbf{Z} \boldsymbol{\gamma}$.

** An illustration using a linear model with two regressors
:PROPERTIES:
:BEAMER_opt: shrink
:END:
Suppose the true model is
\[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i,\; i=1,
\ldots, n \]
with $E(u_i | X_{1i}, X_{2i}) = 0$

However, we estimate a wrong model of
\[ Y_i = \beta_0 + \beta_1 X_{1i} + \epsilon_i,\; i=1, \ldots, n \]

In Lecture 5 we showed that $\beta_1$ can be expressed as
\[ \hat{\beta}_1 = \beta_1 + \frac{\frac{1}{n}\sum_i
(X_{1i} - \bar{X}_1) \epsilon_i}{\frac{1}{n}\sum_i (X_i - \bar{X}_1)^2} \]

As $n \rightarrow \infty$, $\frac{1}{n}\sum_i
(X_{1i} - \bar{X}_1) \epsilon_i \rarrow{p} \cov(X_1, \epsilon) = \rho_{{\scriptscriptstyle X_1} \epsilon} \sigma_{\scriptscriptstyle X_1} \sigma_{\epsilon}$
and $\frac{1}{n}\sum_i (X_i - \bar{X}_1)^2 \rarrow{p}
\sigma^2_{\scriptscriptstyle X_1}$. Therefore, we have

\begin{equation}
\label{eq:omb-4}
\hat{\beta}_1 \rarrowd{p} \beta_1 + \underbrace{\rho_{x_1 \epsilon} \frac{\sigma_{\epsilon}}{\sigma_{x_1}}}_{\mathclap{\text{omitted variable bias}}}
\end{equation}

** A summary of the Omitted variable bias
:PROPERTIES:
:BEAMER_opt: shrink
:END:
The omitted variable bias occurs when two conditions are met
 1. $\mathbf{X}$ is correlated with some omitted variables in $\mathbf{Z}$.
 2. The omitted variables are determinants of the dependent variable
    $\mathbf{Y}$.

The formula to quantify the omitted variable bias is
\begin{equation*}
E(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \boldsymbol{\beta}
+ (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Z} \boldsymbol{\gamma}
\left(\text{ or }
\hat{\beta}_1 \rarrowd{p} \beta_1 + \rho_{x_1 \epsilon} \frac{\sigma_{\epsilon}}{\sigma_{x_1}} \right)
\end{equation*}

*** Some facts summarized from the formula above
- Omitt variable bias is a problem irregardless of whether the sample
  size is large or small. $\hat{\beta}$ is biased and inconsistent
  when there is omitted variable bias.
- Whether this bias is large or small in practice depends on
  $|\rho_{X_{1} \epsilon}|$ or $|\mathbf{X}^{\prime} \mathbf{Z}
  \boldsymbol{\gamma}|$.
- The direction of this bias is determined by the sign of $\rho_{X_{1} \epsilon}$ or $\mathbf{X}^{\prime} \mathbf{Z}
  \boldsymbol{\gamma}$.
- One easy way to detect the existence of the omitted variable bias is
  that when adding a new regressor, the estimated coefficients on some
  previously included regressors change substantially.

* TODO COMMENT Multicollinearity
#+TOC: headlines [currentsection]
** Perfect multicollinearity
:PROPERTIES:
:BEAMER_opt: shrink
:END:
*Perfect multicollinearity* refers to the situation when one of the
regressor is a perfect linear function of the other regressors.
- In the terminology of linear algebra, perfect multicollinearity
  means that the vectors of regressors are linearly dependent.
- That is, the vector of a regressor can be expressed as a linear
  combination of vectors of the other regressors.

Remember that the matrix of regressors $\mathbf{X}$ can be written in
terms of column vectors as
#+BEGIN_LaTeX
\[
\mathbf{X} = [\boldsymbol{\iota}, \boldsymbol{X}_1, \boldsymbol{X}_2, \ldots, \boldsymbol{X}_k ]
\]
#+END_LaTeX
where $\boldsymbol{X}_i = [X_{i1}, X_{i2}, \ldots, X_{in}]^{\prime}$
is a $n \times 1$ vector of $n$ observations of the i^{th}
regressor. $\boldsymbol{\iota}$ is a vector of 1s, representing the
constant term.

That the $k+1$ column vectors are linearly dependent means that there
exist some $(k+1) \times 1$ nonzero vector $\boldsymbol{\beta} =
[\beta_0, \beta_1, \ldots, \beta_k]^{\prime}$ such that
#+BEGIN_LaTeX
\[
\beta_0 \boldsymbol{\iota} + \beta_1 \boldsymbol{X}_1 + \cdots + \beta_k
\boldsymbol{X}_k = 0 \]
#+END_LaTeX

** Perfect multicollinearity (cont'd)
If $\boldsymbol{X}_i$'s are linearly dependent, then it follows
- $\mathbf{X}$ does not have full column rank.
- If $\mathbf{X}$ does not have full column rank, then
  $\mathbf{X}^{\prime} \mathbf{X}$ is singular, that is, the inverse
  of $\mathbf{X}^{\prime} \mathbf{X}$ does not exist. Therefore, we
  can state the assumption of requiring no perfect multicollinearity
  in another way as assuming that $\mathbf{X}$ has full column rank.
- If $\mathbf{X}^{\prime} \mathbf{X}$ is not invertible, the OLS
  estimator based on the formula of $\boldsymbol{\hat{\beta}} =
  (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
  \mathbf{Y}$ does not exist.

** Examples of perfect multicollinearity
Remember that perfect multicollinearity occurs when one regressor can
be expressed as a linear combination of other regressors. This problem
arises from the logic error from the researcher. That is, the
researcher uses two or more variables to provide exactly the same
information that only one variable can provide.

*** Possible linear combination
Suppose we have a multiple regression model
\[ \mathbf{Y} = \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2
\mathbf{X}_2 + \mathbf{u}  \]
And we want to add a new variable $Z$ into this model. The following
practices cause perfect multicollinearity
- $Z = a X_1$ or $Z = b X_2$
- $Z = 1 - a X_1$
- $Z = a X_1 + b X_2$

\vspace{0.2cm}
However, we can add a $Z$ that is not a linear function of $X_1$ or
$X_2$ such that there is no perfect multicollinearity problem. For example,
- $Z = X_1^2$
- $Z = \ln X_1$
- $Z = X_1 X_2$

** The dummy variable trap
:PROPERTIES:
:BEAMER_opt: shrink
:END:
The dummy variable trap is a good case of perfect multicollinearity
that a modeler often encounters. Recall that a *binary variable* (or
*dummy variable*) $D_i$, taking values of one or zero, can be used in
a regression model to distinguish two mutually exclusive groups of
samples, for instance, the male and the female. In fact, dummy
variables can be constructed to represent more than two groups and be
used in multiple regression to examine the difference between these
groups.

\vspace{0.2cm}
Suppose that we have a data composed of people of four ethnic groups:
White, African American, Hispanic, and Asian. And we want to estimate
a regression model to see whether wages among these four groups are
different. The model looks like the following,
\begin{equation}
\label{eq:dummy-trap}
Wage_i = \beta_0 + \beta_1 White_i + \beta_2 African_i + \beta_3 Hispanic_i + \beta_4 Asian_i + u_i
\end{equation}

To be concrete, suppose we have four ordered observations: Chuck,
Mike, Juan, and Li, who are White, African American, Hispanic, and
Asian, respectively. Then the dummy variables are
\begin{equation*}
White =
\begin{pmatrix}
1 \\
0 \\
0 \\
0
\end{pmatrix},\,
African =
\begin{pmatrix}
0 \\
1 \\
0 \\
0
\end{pmatrix},\,
Hispanic =
\begin{pmatrix}
0 \\
0 \\
1 \\
0
\end{pmatrix},\,
Asian =
\begin{pmatrix}
0 \\
0 \\
0 \\
1
\end{pmatrix}
\end{equation*}

** The dummy variable trap (cont'd)
:PROPERTIES:
:BEAMER_opt: plain,shrink
:END:
However, when we construct a model like Equation
(\ref{eq:dummy-trap}), we fall into the dummy variable trap and commit
a perfect multicollinearity mistake. This is because this model has a
constant term.
\begin{equation*}
\begin{pmatrix}
1 \\
1 \\
1 \\
1 \\
\end{pmatrix}
= White + African + Hispanic + Asian
\end{equation*}

To avoid the dummy variable trap, we can either of the following two
methods:
1. drop the constant term
2. drop one dummy variable
The difference between these two methods lies in how we interpret the
coefficients on dummy variables.

** Drop the constant term
:PROPERTIES:
:END:

If we drop the constant term, the model becomes
\begin{equation}
\label{eq:dummy-trap-1}
Wage = \beta_1 White + \beta_2 African + \beta_3 Hispanic + \beta_4 Asian + u
\end{equation}
For Chuck or all white people, the model becomes
\[ Wage = \beta_1 + u \]
Then $\beta_1$ is the population mean wage of whites, that is,
\[\beta_1 = E(Wage | White = 1)\]

Similarly, $\beta_2, \beta_3, \text{ and } \beta_4$ are the population mean wage
of African Americans, Hispanics, and Asians, respectively.

** Drop one dummy variable
:PROPERTIES:
:BEAMER_opt: shrink,plain
:END:

If we drop the dummy variable for white people, then the model becomes
\begin{equation}
\label{eq:dummy-trap-2}
Wage = \beta_1 + \beta_2 African + \beta_3 Hispanic + \beta_4 Asian + u
\end{equation}
For white people, the model is
\[Wage = \beta_1 + u_i \]
And the constant term $\beta_1$ is just the population mean of
whites, that is,
\[\beta_1 = E(Wage | White = 1)\]
So we say that white people
serve as a reference case in Model (\ref{eq:dummy-trap-2}).

For African Americans, the model is
\[ Wage = \beta_1 + \beta_2 + u  \]
From it we have $E(Wage | African=1) = \beta_1 + \beta_2$ so that
\[\beta_2 = E(Wage | African = 1) - \beta_1 = E(Wage | African = 1) -
E(Wage | White = 1)\]
Similarly, we can get that
\begin{align*}
\beta_3 &= E(Wage | Hispanic = 1) - E(Wage | White = 1) \\
\beta_4 &= E(Wage | Asian = 1) - E(Wage | White = 1)
\end{align*}

** Definition of imperfect multicollinearity
:PROPERTIES:
:END:
*Imperfect multicollinearity* is a problem of regression when two or
more regressors are highly correlated. Although they bear similar
names, imperfect multicollinearity and perfect multicollinearity are
two different concepts.
- Perfect multicollinearity is a problem of modeling building,
  resulting in a total failure to estimate a linear model.
- Imperfect multicollinearity is usually a problem of data in the
  sense that data for two variables are highly correlated.
- Imperfect multicollinearity does not affect the unbiasedness of the
  OLS estimators. However, it does affect the efficiency, i.e., the
  variance of the OLS estimators.

** An illustration using a regression model with two regressors
:PROPERTIES:
:END:

Suppose we have a linear regression model with two regressors

\begin{equation}
\label{eq:ex-collin}
\mathbf{Y} = \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \mathbf{u}
\end{equation}

By the FWL theorem, estimating Equation (\ref{eq:ex-collin}) will get
the same OLS estimators of $\beta_1$ and $\beta_2$ as estimating the
following model,

\begin{equation}
\label{eq:ex-collin-1}
\mathbf{y} = \beta_1 \mathbf{x}_1 + \beta_2 \mathbf{x}_2 + \mathbf{v}
\end{equation}

where $\mathbf{y} = \mathbf{Y} - \bar{Y} \boldsymbol{\iota}$,
      $\mathbf{x}_1 =\mathbf{X}_1 - \bar{X}_1 \boldsymbol{\iota}$,
and $\mathbf{x}_2 = \mathbf{X}_2 -\bar{X}_2 \boldsymbol{\iota}$.
That is, $\mathbf{y}, \mathbf{x}_1,\, \text{and } \mathbf{x}_2$
are in the form of the deviation from the mean. And
denote $\mathbf{x} = [\mathbf{x}_1\; \mathbf{x}_2]$
as the matrix of all regressors in Model (\ref{eq:ex-collin-1}).

** An illustration using a regression model with two regressors (cont'd)
:PROPERTIES:
:BEAMER_opt:
:END:
Suppose that $X_1$ and $X_2$ are correlated so that their correlation
coefficient $|\rho_{12}| > 0$. And the square of the sample
correlation coefficient is

\begin{equation}
r^2_{12} = \frac{\left(\sum (X_1 - \bar{X}_1)(X_2 - \bar{X}_2)\right)^2}{\sum (X_1 - \bar{X}_1)^2 \sum (X_2 - \bar{X}_2)^2}
= \frac{\left( \sum x_1 x_2\right)^2}{\sum x_1 \sum x_2}
\end{equation}

The OLS estimator of Model (\ref{eq:ex-collin-1}) is
\begin{equation}
\label{eq:bhat-ex-collin}
\hat{\boldsymbol{\beta}} = \left(\mathbf{x}^{\prime} \mathbf{x}\right)^{-1} \mathbf{x}^{\prime} \mathbf{y}
\end{equation}
with the covariance matrix as
\begin{equation}
\label{eq:bhat-cov-ex-collin}
\var(\hat{\boldsymbol{\beta}} | \mathbf{x}) = \sigma^2_u \left(\mathbf{x}^{\prime} \mathbf{x}\right)^{-1}
\end{equation}

** An illustration using a regression model with two regressors (cont'd)
:PROPERTIES:
:BEAMER_opt: shrink
:END:
- $\hat{\boldsymbol{\beta}}$ is still unbiased since the assumption of
  $E(\mathbf{u} | X)$ holds and so does $E(\mathbf{v} | \mathbf{x})$.

- The variance of $\hat{\beta}_1$, which is the first diagonal element
  of $\sigma^2_u \left(\mathbf{x}^{\prime} \mathbf{x}\right)^{-1}$, is
  affected by $r_{12}$. To see this, we write $\var(\hat{\beta}_1 | \mathbf{x})$
  explicitly as
  \begin{equation*}
  \begin{split}
  \var(\hat{\beta}_1 | \mathbf{x}) &=  \frac{\sigma^2_u \sum x_2^2}{\sum_i x_1^2 \sum x_2^2 - (\sum x_1 x_2)^2} \\
  &= \frac{\sigma^2_u \sum x_2^2}{\displaystyle \sum x_1^2 \sum x_2^2 \left(1 - \frac{(\sum x_1 x_2)^2}{\sum x_1^2 \sum x_2^2}\right)} \\
  &= \frac{\sigma^2_u}{\sum x_1^2} \frac{1}{(1 - r^2_{12})}
  \end{split}
  \end{equation*}
  Therefore, when $X_1$ and $X_2$ are highly correlated, that is
  $r^2_{12}$ gets close to 1, then $\var(\hat{\beta}_1 | \mathbf{x})$
  becomes very large.

** The consequence and remedies of imperfect multicollinearity
:PROPERTIES:
:BEAMER_opt: shrink
:END:
*** The consequence and detection
- The consequence of multicollinearity is that it may lead us to
  wrongly fail to reject the zero hypothesis in the t-test for a
  coefficient.

- The variance inflation factor (VIF) is a commonly used indicator for
  detecting multicollinearity. The definition is

  \begin{equation*}
  \mathrm{VIF} = \frac{1}{1 - r^2_{12}}
  \end{equation*}

  The smaller VIF is for a regressor, the less severe the problem of
  multicollinearity is. However, there is no widely accepted cut-off
  value for VIF to detect multicollinearity. $VIF > 10$ for a
  regressor is often seen as an indication of multicollinearity, but
  we cannot always trust this.

*** The remedies
- Include more sample in hope of the variation in $\mathbf{X}$ getting
  widened, i.e., increasing $\sum_i (X_{1i} - \bar{X}_1)$.
- Drop the variable(s) that is highly correlated with other
  regressors. Notice that by doing this we are at the risk of
  suffering the omitted variable bias. There is always a trade-off
  between including all relevant regressors and making the regression
  model /parsimonious/.
