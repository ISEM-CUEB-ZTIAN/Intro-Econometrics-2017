#+TITLE: Lecture 8: Linear Regression with Multiple Regressors
#+AUTHOR: Zheng Tian
#+DATE:
#+STARTUP: beamer
#+OPTIONS: toc:1 H:2
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,10pt]
#+BEAMER_THEME: CambridgeUS
#+BEAMER_COLOR_THEME: beaver
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 :ETC
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newtheorem{mydef}{Definition}
#+LATEX_HEADER: \newtheorem{mythm}{Theorem}
#+LATEX_HEADER: \newcommand{\dx}{\mathrm{d}}
#+LATEX_HEADER: \newcommand{\var}{\mathrm{Var}}
#+LATEX_HEADER: \newcommand{\cov}{\mathrm{Cov}}
#+LATEX_HEADER: \newcommand{\corr}{\mathrm{corr}}
#+LATEX_HEADER: \newcommand{\pr}{\mathrm{Pr}}
#+LATEX_HEADER: \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
#+LATEX_HEADER: \DeclareMathOperator*{\plim}{plim}
#+LATEX_HEADER: \newcommand{\plimn}{\plim_{n \rightarrow \infty}}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \def\mathbi#1{\textbf{\em #1}}
#+LATEX_HEADER: \setlength{\parskip}{1em}


* The Multiple Regression Model
#+TOC: headlines [currentsection]

** The problem of a simple linear regression

*** The simple linear regression model
\begin{equation*}
TestScore = \beta_0 + \beta_1 \times STR + OtherFactors
\end{equation*}

*** Question: Is this model adequate to characterize the determination of test scores?

- It ignores many important factors, simply lumped into
  /OtherFactors/, the error term, $u_i$, in the regression model.

- What are possible other important factors?
  - School district characteristics: average income level, demographic
    components
  - School characteristics: teachers' quality, school buildings, 
  - Student characteristics: family economic conditions, individual
    ability

** Percentage of English learners as an example
:PROPERTIES:
:BEAMER_opt:
:END:

The percentage of English learners in a school district could be an
relevant and important determinant of test scores, which is omitted
in the simple regression model.

*** How can it affect the estimate of the effect of student-teacher ratios on test score?

- High percentage of English learners \Rightarrow large student-teacher ratios.

- High percentage of English learners \Rightarrow lower test scores.

- The estimated effect of student-teacher ratios may in fact include
  the influence from the high percentage of English learners. 

- In the terminology of statistics, the magnitude of the coefficient
  on student-teacher ratio is *overestimated*.

- The problem is called *the omitted variable bias*

** Solutions to the problem of ignoring important factors

We can include these important but ignored variables, like the
percentage of English learners ($PctEL$), in the regression model.  

\[
TestScore_i = \beta_0 + \beta_1 STR_i + \beta_2 PctEL_i +
OtherFactors_i 
\] 

A regression model with more than one regressors is a multiple
regression model. 

** A multiple regression model
:PROPERTIES:
:BEAMER_opt:
:END:

The general form of a *multiple regression model* is
\begin{equation}
\label{eq:multi-regress-1}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_k X_{ki} + u_i,\; i = 1, \ldots, n
\end{equation}
where
- $Y_i$ is the i^{th} observation on the dependent variable;
- $X_{1i}, X_{2i}, \ldots, X_{ki}$ are the i^{th} observation on each
  of the $k$ regressors; and
- $u_i$ is the error term associated with the i^{th} observation,
  representing all other factors that are not included in the model.

** The components in a multiple regression model
:PROPERTIES:
:BEAMER_opt:
:END:

- The population regression line (or population regression
  function)
  \begin{equation*}
  E(Y_i | X_{1i}, \ldots, X_{ki}) = \beta_0 + \beta_1 X_{1i} + \cdots + \beta_k X_{ki}
  \end{equation*}

- $\beta_1, \ldots, \beta_k$ are the coefficients on the corresponding
  $X_i,\, i = 1, \ldots, k$.

- $\beta_0$ is the intercept, which can also be thought of the
  coefficient on a regressor $X_{0}$ that equals 1 for all
  observations.

  - Including $X_{0}$, there are $k+1$ regressors in the multiple
    regression model.

** The interpretation of $\beta_i$: Holding other things constant

\begin{equation}
\label{eq:multi-regress-1a}
Y = \beta_0 + \beta_1 X_1 + \cdots + \beta_k X_k + u
\end{equation}

The coefficient $\beta_i$ on the regressor
$X_i$ for $i=1, \ldots, k$ measures the effect on $Y$ of a unit change
in $X_i$, *holding other $X$ constant*. 

*** An example

Suppose we have two regressors $X_1$ and $X_2$ and we are interested
in the effect of $X_1$ on $Y$. We can let $X_1$ change by $\Delta X_1$
and holding $X_2$ constant. Then, the new value of $Y$ is

\[ Y + \Delta Y = \beta_0 + \beta_1 (X_1 + \Delta X_1) + \beta_2 X_2  \]

Subtracting $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2$, we have
$\Delta Y = \beta_1 \Delta X_1$. That is
\[ \beta_1 = \frac{\Delta Y}{\Delta X}, \text{ holding } X_2 \text{ constant} \]

** The partial effect

If $Y$ and $X_i$ for $i = 1, \ldots, k$ are continuous and
differentiable variables, $\beta_i$ is as simply as the partial
derivative of $Y$ with respect to $X_i$. That is

\[\beta_i = \frac{\partial Y}{\partial X_i}\]

By the definition of a partial derivative, $\beta_i$ is just
the effect of a marginal change in $X_i$ on $Y$ holding other $X$
constant.

** Look at the data in terms of vectors and matrix
:PROPERTIES:
:BEAMER_opt:
:END:

#+NAME: fig:data-snapshot
#+CAPTION: The California data set in Excel
#+ATTR_LATEX: :width 0.4\textwidth :height 0.5\textheight
[[file:img/data_snapshot.png]]

- Each row represents an observation of all variables pertaining to a
  school district.
- Each column represents a variable with all
  observations.
- The whole dataset can be seen as a matrix. 

** Define variables in matrix notation
:PROPERTIES:
:BEAMER_opt: shrink
:END:

*** Write all the variables in vector and matrix notation

\begin{equation*}
\underbrace{
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
Y_2 \\
\vdots \\
Y_n
\end{pmatrix},}_{\text{Dependent variable}}
\underbrace{
\mathbf{X} =
\begin{pmatrix}
1 & X_{11} & \cdots & X_{k1} \\
1 & X_{12} & \cdots & X_{k2} \\
\vdots & \vdots & \ddots & \vdots \\
1 & X_{1n} & \cdots & X_{kn}
\end{pmatrix},}_{\text{Independent variables}}
\underbrace{
\mathbf{u} =
\begin{pmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{pmatrix},\,}_{\text{Errors}}
\underbrace{
\boldsymbol{\beta} =
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{pmatrix}}_{\text{Coefficients}}
\end{equation*}

*** Write the multiple regression model in matrix notation

\begin{equation}
\label{eq:multi-regress-m}
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}
\end{equation}

*** Why do we use matrix notation
Concise, easy to derive properties; big-picture perspective.


** Two other ways to write the regression model

*** Write $\mathrm{X}$ in row vectors

- The i^{th} row in $\mathrm{X}$ is a $(k+1) \times 1$ vector
  \begin{equation*}
  \mathbi{x}_i =
  \begin{pmatrix}
  1 \\
  X_{1i} \\
  \vdots \\
  X_{ki} \\
  \end{pmatrix}. \text{ Thus, its transpose is }
  \mathbi{x}_i^{\prime} = (1, X_{1i}, \cdots, X_{ki})
  \end{equation*}

- We can write the regression model (Equation
  \ref{eq:multi-regress-m}) as

  \begin{equation}
  Y_i = \mathbi{x}^{\prime}_i \boldsymbol{\beta} + u_i,\; i = 1, \ldots, n
  \end{equation}

** Two other ways to write the regression model (cont'd)
*** Write $\mathrm{X}$ in vector vectors
- The i^{th} column in $\mathbf{X}$ is a $n \times 1$ vector
  \begin{equation*}
  \boldsymbol{X}_i =
  \begin{pmatrix}
  X_{i1} \\
  \vdots \\
  X_{in} \\
  \end{pmatrix}. \text{ The first column is }
  \boldsymbol{\iota} = 
  \begin{pmatrix}
  1 \\
  \vdots \\
  1
  \end{pmatrix}. \text{ Thus }
  \mathbf{X} = \left(\boldsymbol{\iota}, \boldsymbol{X}_1, \ldots, \boldsymbol{X}_k \right)
  \end{equation*}

- The regression model (Equation \ref{eq:multi-regress-m}) can be
  re-written as
  \begin{equation}
  \label{eq:multi-regress-m2}
  \mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1\boldsymbol{X}_1 + \cdots + \beta_k\boldsymbol{X}_k + \mathbf{u}
  \end{equation}


* The OLS Estimator in Multiple Regression
#+TOC: headlines [currentsection]

** The minimization problem and the OLS estimator

- The core idea of the OLS estimator for a multiple regression model
  remains the same as in a simple regression model: 
  *minimizing the sum of the squared residuals*. 

- Let $\mathbf{b} = [b_0, b_1, \ldots, b_k]^{\prime}$ be some estimators
  of $\boldsymbol{\beta} = [\beta_0, \beta_1, \ldots,
  \beta_k]^{\prime}$.

- The predicted $Y_i$ is
  \begin{gather*}
  \hat{Y}_i = b_0 + b_1 X_{1i} + \cdots + b_k X_{ki} = \mathbi{x}^{\prime}_i
  \mathbf{b},\, i = 1, \ldots, \\
  \text{ or in matrix notation }  \hat{\mathbf{Y}} = \mathbf{Xb}
  \end{gather*}
  
- The residuals, i.e., the prediction mistakes, with $\mathbf{b}$ is
  \begin{gather*}
  \hat{u}_i = Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} = Y_i -
  \mathbi{x}^{\prime}_i \mathbf{b} \\
  \text{ or in matrix notation }  \hat{\mathbf{u}} = \mathbf{Y} - \mathbf{Xb}
  \end{gather*}

** The minimization problem and the OLS estimator (cont'd)

- The sum of the squared residuals is
  \begin{align*}
  S(\mathbf{b}) & = S(b_0, b_1, \ldots, b_k) = \sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki})^2 \\
  & = \sum_{i=1}^n (Y_i - \mathbf{x}^{\prime}_i \mathbf{b})^2 = (\mathbf{Y} -
  \mathbf{Xb})^{\prime}(\mathbf{Y}-\mathbf{Xb}) \\
  & = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} = \sum_{i=1}^n \hat{u}_i^2
  \end{align*}

- The OLS estimator is the solution to the following minimization problem:
  \begin{equation}
  \label{eq:ols-multi-regress}
  \operatorname*{min}_{\mathbf{b}}\: S(\mathbf{b}) = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}}
  \end{equation}

** The OLS estimator of $\boldsymbol{\beta}$ as a solution to the minimization problem

- Solve the minimization problem: 
  
  $$\text{F.O.C.: } \frac{\partial S(\mathbf{b})}{\partial b_j} = 0,
  \text{ for } j =
  0, 1, \ldots, k$$

- The derivative of $S(b_0, \ldots, b_k)$ with respect to $b_j$ is
  \begin{gather*}
  \label{eq:ols-wrt-bj}
  \frac{\partial }{\partial b_j} \sum_{i=1}^n \left(Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} \right)^2 = \\
  -2 \sum_{i=1}^n X_{ji} \left(Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki} \right) = 0
  \end{gather*}

- There are $k+1$ such equations. Solving the system of equations, we
  obtain the OLS estimator $\hat{\boldsymbol{\beta}} = (\hat{\beta}_0, \ldots,
  \hat{\beta}_k)^{\prime}$.

** The OLS estimator in matrix notation

Let $\boldsymbol{\hat{\beta}}$ denote the OLS estimator. Then the
expression of $\boldsymbol{\hat{\beta}}$ is given by
\begin{equation}
\label{eq:betahat-mult}
\boldsymbol{\hat{\beta}} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}
\end{equation}

*** Some useful results of matrix calculus
To prove Equation (\ref{eq:betahat-mult}), we need to use some results
of matrix calculus.
\begin{equation}
\label{eq:matrix-calc}
\frac{\partial \mathbf{a}^{\prime} \mathbf{x}}{\partial \mathbf{x}} = \mathbf{a},\; \frac{\partial \mathbf{x}^{\prime} \mathbf{a}}{\partial \mathbf{x}} = \mathbf{a},\; \text{ and } \frac{\partial \mathbf{x}^{\prime} \mathbf{A} \mathbf{x}}{\partial \mathbf{x}} = (\mathbf{A} + \mathbf{A}^{\prime}) \mathbf{x}
\end{equation}
when $\mathbf{A}$ is symmetric, then $(\partial \mathbf{x}^{\prime}
\mathbf{A} \mathbf{x}) / (\partial \mathbf{x}) = 2\mathbf{A}
\mathbf{x}$

** The proof
:PROPERTIES:
:BEAMER_opt:
:END:

\begin{proof}[Proof of Equation (\ref{eq:betahat-mult})]
  \begin{equation*}
  S(\mathbf{b}) = \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} = \mathbf{Y}^{\prime} \mathbf{Y} - \mathbf{b}^{\prime} \mathbf{X}^{\prime} \mathbf{Y} - \mathbf{Y}^{\prime} \mathbf{Xb} - \mathbf{b}^{\prime} \mathbf{X}^{\prime} \mathbf{Xb}
  \end{equation*}

  The first order conditions for minimizing $S(\mathbf{b})$ with respect to $\mathbf{b}$ is
  \begin{gather}
  -2 \mathbf{X}^{\prime} \mathbf{Y} - 2 \mathbf{X}^{\prime} \mathbf{Xb} = \mathbf{0} \notag \\
  \mathbf{X}^{\prime} \mathbf{Xb} = \mathbf{X}^{\prime} \mathbf{Y} \label{eq:ols-mult-eqs}
  \end{gather}

  Then
  \begin{equation*}
  \mathbf{b} = (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}
  \end{equation*}
  given that $\mathbf{X}^{\prime} \mathbf{X}$ is invertible.
\end{proof}

Note that Equation (\ref{eq:ols-mult-eqs}) represents a system of
equations with $k+1$ equations.

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model

The simple linear regression model written in matrix notation is

\begin{equation*}
\mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1 \mathbf{X}_1 + \mathbf{u} = \mathbf{X} \boldsymbol{\beta} + \mathbf{u}
\end{equation*}

where

\begin{equation*}
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix},\,
\mathbf{X} =
\begin{pmatrix}
\boldsymbol{\iota} & \mathbf{X}_1
\end{pmatrix}
=
\begin{pmatrix}
1 & X_{11} \\
\vdots & \vdots \\
1 & X_{1n}
\end{pmatrix},\,
\mathbf{u} =
\begin{pmatrix}
u_1 \\
\vdots \\
u_n
\end{pmatrix},\,
\boldsymbol{\beta} =
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\end{pmatrix}
\end{equation*}

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:

Let's get the components in Equation (\ref{eq:betahat-mult}) step by
step.

*** Step (1): compute $\left(\mathbf{X}^{\prime}\mathbf{X}\right)$

\begin{align*}
\mathbf{X}^{\prime}\mathbf{X} & =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \\
\mathbf{X}_1^{\prime}
\end{pmatrix}
\begin{pmatrix}
\boldsymbol{\iota} & \mathbf{X}_1
\end{pmatrix} =
\begin{pmatrix}
1 & \cdots & 1 \\
X_{11} & \cdots & X_{1n}
\end{pmatrix}
\begin{pmatrix}
1 & X_{11} \\
\vdots & \vdots \\
1 & X_{1n}
\end{pmatrix} \\
& =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \boldsymbol{\iota} & \boldsymbol{\iota}^{\prime} \mathbf{X}_1 \\
\mathbf{X}_1^{\prime} \boldsymbol{\iota} & \mathbf{X}_1^{\prime} \mathbf{X}_1
\end{pmatrix} =
\begin{pmatrix}
n & \sum_{i=1}^n X_{1i} \\
\sum_{i=1}^n X_{1i} & \sum_{i=1}^n X_{1i}^2
\end{pmatrix}
\end{align*}

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:

- Step (2): compute $\left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}$ ::

**** The inverse of a $2 \times 2$ matrix
  \begin{equation*}
  \begin{pmatrix}
  a_{11} & a_{12} \\
  a_{21} & a_{22}
  \end{pmatrix}^{-1}
  =\frac{1}{a_{11}a_{22} - a_{12}a_{21}}
  \begin{pmatrix}
  a_{22} & -a_{12} \\
  -a_{21} & a_{11}
  \end{pmatrix}
  \end{equation*}

**** The inverse of $\mathbf{X}^{\prime}\mathbf{X}$
  \begin{equation*}
  \left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1} =
  \frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
  \begin{pmatrix}
  \sum_{i=1}^n X_{1i}^2 & - \sum_{i=1}^n X_{1i} \\
  -\sum_{i=1}^n X_{1i} & n
  \end{pmatrix}
  \end{equation*}

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:

*** Step (3): compute $\mathbf{X}^{\prime} \mathbf{Y}$

\begin{equation*}
\mathbf{X}^{\prime} \mathbf{Y} =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \\
\mathbf{X}_1^{\prime}
\end{pmatrix}
\mathbf{Y} =
\begin{pmatrix}
1 & \cdots & 1 \\
X_{11} & \cdots & X_{1n}
\end{pmatrix}
\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix} =
\begin{pmatrix}
\boldsymbol{\iota}^{\prime} \mathbf{Y} \\
\mathbf{X}_1^{\prime} \mathbf{Y}
\end{pmatrix} =
\begin{pmatrix}
\sum_{i=1}^n Y_i \\
\sum_{i=1}^n X_{1i} Y_i
\end{pmatrix}
\end{equation*}

*** Step (4): compute $\boldsymbol{\hat{\beta}}=(\mathbf{X}^{\prime}\mathbf{X})^{-1} \mathbf{X}^{\prime} \mathbf{Y}$

\begin{align*}
\begin{pmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{pmatrix} & =
\frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
\begin{pmatrix}
\sum_{i=1}^n X_{1i}^2 & - \sum_{i=1}^n X_{1i} \\
-\sum_{i=1}^n X_{1i} & n
\end{pmatrix}
\begin{pmatrix}
\sum_{i=1}^n Y_i \\
\sum_{i=1}^n X_{1i} Y_i
\end{pmatrix} \\
& =
\frac{1}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2}
\begin{pmatrix}
\sum_{i=1}^n X_{1i}^2 \sum_{i=1}^n Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n X_{1i}Y_i \\
-\sum_{i=1}^n X_{1i} \sum_{i=1}^n Y_i + n \sum_{i=1}^n X_{1i} Y_i
\end{pmatrix}
\end{align*}

** The OLS estimator of $\hat{\beta}_1$ in a simple regression model (cont'd)
:PROPERTIES:
:BEAMER_opt: plain
:END:

*** The formula of $\hat{\beta}_1$
  \begin{equation*}
  \hat{\beta}_1 = \frac{n \sum_{i=1}^n X_{1i} Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n Y_i}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2} = \frac{\sum_{i=1}^n (X_{1i} - \bar{X}_1)(Y_i - \bar{Y})}{\sum_{i=1}^n (X_{1i} - \bar{X}_1)^2}
  \end{equation*}

*** The formula of $\hat{\beta}_0$
  \begin{equation*}
  \hat{\beta}_0 = \frac{\sum_{i=1}^n X_{1i}^2 \sum_{i=1}^n Y_i - \sum_{i=1}^n X_{1i} \sum_{i=1}^n X_{1i}Y_i}{n \sum_{i=1}^n X_{1i}^2 - (\sum_{i=1}^n X_{1i})^2} = \bar{Y} - \hat{\beta}_1 \bar{X}_1
  \end{equation*}

** Application to Test Scores and the Student-Teacher Ratio
:PROPERTIES:
:BEAMER_opt: shrink,plain
:END:
*** The simple regression compared with the multiple regression
The estimated simple linear regression model is
\[ \widehat{TestScore} = 698.9 - 2.28 \times STR \]

The estimated multiple linear regression model is
\[ \widehat{TestScore} = 686.0 - 1.10 \times STR - 0.65 \times PctEL
\]

*** Explanations
- The interpretation of the new estimated coefficient on /STR/ is,
  *holding the percentage of English learners constant*, a unit
  decrease in /STR/ is estimated to increase test scores by 1.10
  points.
- We can also interpret the estimated coefficient on /PctEL/ as,
  holding /STR/ constant, one unit decrease in /PctEL/ increases test
  scores by 0.65 point.
- The magnitude of the negative effect of /STR/ on test scores in the
  multiple regression is approximately half as large as when /STR/ is
  the only regressor.

** COMMENT Warm-up exercises
:PROPERTIES:
:BEAMER_opt: shrink
:END:
*** 1) In the multiple regression model you estimate the effect on Yi of a unit change in one of the Xi while holding all other regressors constant. This
- A) :: makes little sense, because in the real world all other variables change.
- B) :: corresponds to the economic principle of mutatis mutandis.
- C) :: leaves the formula for the coefficient in the single explanatory variable case unaffected.
- D) :: corresponds to taking a partial derivative in mathematics.
\pause
Answer:  D

*** 2) The multiple regression model can be written in matrix form as follows:
- A) :: $\mathbf{Y} = \mathbf{X} \boldsymbol{\beta}$
- B) :: $\mathbf{Y} = \mathbf{X} + \mathbf{U}$
- C) :: $\mathbf{Y} = \boldsymbol{\beta} \mathbf{X} + \mathbf{U}$
- D) :: $\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{U}$
\pause
Answer:  D

*** 3) Minimization of $\sum_{i=1}^n (Y_i - b_0 - b_1 X_{1i} - \cdots - b_k X_{ki})^2$ results in
- A) :: $\mathbf{X}^{\prime}\mathbf{Y} = \mathbf{X} \hat{\boldsymbol{\beta}}$
- B) :: $\mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{0}_{k+1}$
- C) :: $\mathbf{X}^{\prime} (\mathbf{Y} - \mathbf{X} \hat{\boldsymbol{\beta}}) = \mathbf{0}_{k+1}$
- D) :: $\mathbf{R} \boldsymbol{\beta} = \mathbf{r}$
\pause
Answer:  C


* Measures of Fit in Multiple Regression
#+TOC: headlines [currentsection]

** The standard errors of the regression (SER)

- The standard error of regression (SER) estimates the standard
  deviation of the error term $\mathbf{u}$. In multiple regression,
  the SER is
  \begin{equation}
  \label{eq:ser-m}
  SER = s_{\hat{u}},\, \text{ where } s^2_{\hat{u}} = \frac{\sum_{i=1}^n \hat{u}_i^2}{n-k-1} = \frac{SSR}{n-k-1}
  \end{equation}

- $SSR$ is divided by $(n-k-1)$ because there are $n$ observations and
  $(k+1)$ coefficients to be estimated.

** $R^2$

*** TSS, ESS, and SSR

- The total sum of squares (TSS): $TSS = \sum_{i=1}^n (Y_i - \bar{Y})^2$
- The explained sum of squares (ESS): $ESS = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$
- The sum of squared residuals (SSR): $SSR = \sum_{i=1}^n \hat{u}_i^2$

*** COMMENT In matrix notation 
\begin{equation*}
\mathbf{Y} =
\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix},\;
\boldsymbol{\iota} =
\begin{pmatrix}
1 \\
\vdots \\
1
\end{pmatrix},\;
\mathbf{y} = 
\begin{pmatrix}
y_1 \\
\vdots \\
y_n
\end{pmatrix}
 =
\begin{pmatrix}
Y_1 \\
\vdots \\
Y_n
\end{pmatrix}
-
\begin{pmatrix}
\bar{Y} \\
\vdots \\
\bar{Y}
\end{pmatrix}
=
\mathbf{Y} - \bar{Y} \boldsymbol{\iota}
\end{equation*}

- $\mathbf{y}$ represents *the deviation from the mean* of $Y_i,\;
  i=1,\ldots,n$. Similarly, we can get the deviation-from-the-mean
  form of $\hat{Y}_i$ as $\mathbf{\hat{y}}$. 

- Rewrite $TSS, ESS, \text{ and } SSR$ as

  \[ TSS = \mathbf{y}^{\prime} \mathbf{y},\; ESS =
  \hat{\mathbf{y}}^{\prime} \hat{\mathbf{y}},\; \text{ and } SSR =
  \hat{\mathbf{u}}^{\prime} \hat{\mathbf{u}} \]

*** The equality still holds in multiple regression
  \[ TSS = ESS + SSR \]
  
*** Define $R^2$ as before
\begin{equation}
\label{eq:r2-center}
R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}
\end{equation}

** Limitations of R^2
:PROPERTIES:
:BEAMER_opt:
:END:
- $R^{2}$ is valid only if a regression model is estimated using the OLS
  since otherwise it would not be true that $TSS = ESS + SSR$.

- $R^2$ defined in the form of the deviation from the mean is only
  valid when a constant term is included in regression. 

  In a regression model without an intercept, 
  use the uncentered version of $R^2$, which is also defined as
  \begin{equation}
  \label{eq:r2-uncenter}
  R^2_u = \frac{EES}{TSS} = 1 - \frac{SSR}{TSS}
  \end{equation}
  where 
  - $TSS = \sum_{i=1}^n Y_i^2$, $ESS = \sum_{i=1}^2 \hat{Y}_i^2$, and
    $SSR = \sum_{i=1}^n \hat{u}_i^2$ 
  
  Note that in a regression without a constant term, the
  equality $TSS = ESS + SSR$ holds. 

** Limitation of R^2 (cont'd)
:PROPERTIES:
:BEAMER_opt:
:END:
- R^{2} increases whenever an additional regressor is included in a
  multiple regression model, unless the estimated coefficient on the
  added regressor is exactly zero. 
  
  \vspace{1cm}
  Consider two regression models
  \begin{align}
  \mathbf{Y} &= \beta_0 + \beta_1 \mathbf{X}_1 + \mathbf{u}
  \label{eq:ex-eq-1} \\
  \mathbf{Y} &= \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \mathbf{u} \label{eq:ex-eq-2}
  \end{align}

  Which model should have smaller /SSR/?

** Limitation of R^2 (cont'd)

- Equation (\ref{eq:ex-eq-2}) have the smaller /SSR/ than equation
  (\ref{eq:ex-eq-1}). Why?
  
  \vspace{0.5cm}   
  
  An additional $X_2$ \Rightarrow More in the total variation of $Y$
  is explained \Rightarrow Smaller /SSR/ (unless $\hat{\beta}_2=0$)
  
  \vspace{0.5cm}

- Since both models use the same $\mathbf{Y}$, $TSS$ must be the
  same. Because /SSR/ decreases as more regressors are added, $R^2$
  increases. 

- In mathematics, this is essentially because the OLS estimation
  for equation (\ref{eq:ex-eq-1}) solves a constrained minimization
  problem, while that for equation (\ref{eq:ex-eq-2}) solves an
  unconstrained minimization problem. 

** The adjusted R^2

- The adjusted R^2 is, or $\bar{R}^2$, is a modified version of R^2. 

- The $\bar{R}^2$ improves R^2 in the sense that it does not
  necessarily increase when a new regressor is added. The $\bar{R}^2$
  is

  \begin{equation}
  \label{eq:adj-r2}
  \bar{R}^2 = 1 - \frac{SSR / (n-k-1)}{TSS / (n-1)} = 1 - \frac{n-1}{n-k-1}\frac{SSR}{TSS} = 1 - \frac{s^2_u}{s^2_Y}
  \end{equation}

- The adjustment is made by dividing $SSR$ and $TSS$ by their
  corresponding degrees of freedom, which is $n-k-1$ and $n-1$
  respectively.

  \vspace{0.2cm}
- $s^2_u$ is the sample variance of the OLS residuals, and $s^2_Y$ is
  the sample variance of $Y$.

** Properties of $\bar{R}^2$

- The definition of the $\bar{R}^2$ in Equation (\ref{eq:adj-r2}) is
  valid only when a constant term is included in the regression
  model.

  \vspace{0.4cm}
- Since $\frac{n-1}{n-k-1} > 1$, then it is always true that
  the $\bar{R}^2 < R^2$.

  \vspace{0.4cm}
- $k \uparrow\, \Rightarrow\, \frac{SSR}{TSS} \downarrow$, but $k
  \uparrow\, \Rightarrow \frac{n-1}{n-k-1} \uparrow$. 
  
  \vspace{0.2cm}

  Whether $\bar{R}^2$ increases or decreases depends on which of these
  effects is stronger.

  \vspace{0.4cm}
- The $\bar{R}^2$ can be negative. This happens when the regressors,
  taken together, reduce the sum of squared residuals by such a small
  amount that his reduction fails to offset the factor
  $\frac{n-1}{n-k-1}$.

** The usefulness of the R^2 and $\bar{R}^2$

- Both $R^2$ and $\bar{R}^2$ are valid when the regression model is
  estimated by the OLS estimators. R^2 computed with estimators other
  than the OLS ones is usually called /pseudo/ R^2.
  
  \vspace{0.5cm}

- Their importance as measures of fit cannot be overstated. We cannot heavily
  reply on R^2 or $\bar{R}^2$ to judge whether some regressors should
  be included in the model or not.


* The Frisch-Waugh-Lovell Theorem
#+TOC: headlines [currentsection]

** The grouped regressors

Consider a multiple regression model
\begin{equation}
\label{eq:fwl-eq}
Y_i = \underbrace{\beta_0 + \beta_1 X_{1i} +
\cdots + \beta_{k1} X_{k1,i}}_{\text{k1+1 regressors}} + \underbrace{
\beta_{k1+1} X_{k1+1,i} + \cdots \beta_k X_k}_{\text{k2 regressors}} + u_i
\end{equation}

In matrix notation, we write
\begin{equation}
\label{eq:mult-reg-2g}
\mathbf{Y} = \mathbf{X}_1\boldsymbol{\beta}_1 + \mathbf{X}_2 \boldsymbol{\beta}_2 + \mathbf{u}
\end{equation}
where 
- $\mathbf{X}_1$ is an $n \times (k1+1)$ matrix composed of the
  intercept and the first $k1+1$ regressors in Equation eqref:eq:fwl-eq,
- $\mathbf{X}_2$ is an $n \times k2$ matrix composed of the rest
  $k_2$ regressors.
- $\boldsymbol{\beta}_1 = (\beta_0, \beta_1, \ldots,
  \beta_{k1})^{\prime}$ and $\boldsymbol{\beta}_2 = (\beta_{k1+1},
  \ldots, \beta_k)^{\prime}$. 

** Two estimation strategies

Suppose that we are interested in $\boldsymbol{\beta}_2$ but not much
in $\boldsymbol{\beta}_1$ in Equation eqref:eq:mult-reg-2g. How can we
estimate $\boldsymbol{\beta}_2$?

*** The first strategy: the standard OLS estimation

We can obtain the OLS estimation of $\boldsymbol{\beta}_2$ with
Equation eqref:eq:betahat-mult, i.e., $\hat{\boldsymbol{\beta}} =
(\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
\mathbf{Y}$. $\hat{\boldsymbol{\beta}}_2$ is a vector consisting of
the last $k2$ elements in $\hat{\boldsymbol{\beta}}$. 

In matrix notation, we can get $\hat{\boldsymbol{\beta}}_2$ from the
following equation
\begin{equation*}
\begin{pmatrix}
\hat{\boldsymbol{\beta}}_1 \\
\hat{\boldsymbol{\beta}}_2 
\end{pmatrix} =
\begin{pmatrix}
\mathbf{X}_1^{\prime} \mathbf{X}_1 & \mathbf{X}_1^{\prime} \mathbf{X}_2 \\
\mathbf{X}_2^{\prime} \mathbf{X}_1 & \mathbf{X}_2^{\prime} \mathbf{X}_2
\end{pmatrix}^{-1}
\begin{pmatrix}
\mathbf{X}_1^{\prime} \mathbf{Y} \\
\mathbf{X}_2^{\prime} \mathbf{Y}
\end{pmatrix}
\end{equation*}

** The second strategy: the step OLS estimation

1. Regress each regressor in $\mathbf{X}_2$ on all regressors in
   $\mathbf{X}_1$, including the intercept, and get the residuals from
   this regression, denoted as $\widetilde{\mathbf{X}}_2$. That is,
   for each regressor $\mathbf{X}_i$ in $\mathbf{X}_2$, $i=k1+1,
   \ldots, k$, we estimate a multiple regression,
   \[
   \mathbf{X}_i = \gamma_0 + \gamma_1 \mathbf{X}_1 + \cdots +
   \gamma_{k1} \mathbf{X}_{k1} + v \] 
   The residuals from this regression is 
   \[ \widetilde{\mathbf{X}}_i = X_i - \hat{\gamma}_0 -
   \hat{\gamma}_1 \mathbf{X}_1 - \cdots - \hat{\gamma}_{k1}
   \mathbf{X}_{k1} \] 
   As such, we can get an $n \times k2$ matrix
   composed of all the residuals $\widetilde{\mathbf{X}}_2 =
   (\widetilde{\mathbf{X}}_{k1+1} \cdots \widetilde{\mathbf{X}}_k)$.

2. Regress $\mathbf{Y}$ on all regressors in $\mathbf{X}_1$, denoting
   the residuals from this regression as $\widetilde{\mathbf{Y}}$.

3. Regress $\widetilde{\mathbf{Y}}$ on $\widetilde{\mathbf{X}}_2$, and
   obtain the estimates of $\boldsymbol{\beta}_2$ as
   $\boldsymbol{\beta}_2=(\widetilde{\mathbf{X}}_2^{\prime} \widetilde{\mathbf{X}}_2)^{-1}
   \widetilde{\mathbf{X}}_2^{\prime} \widetilde{\mathbf{Y}}$.

** The Frisch-Waugh-Lovell Theorem

The Frisch-Waugh-Lovell (FWL) Theorem states that
1) the OLS estimates of $\boldsymbol{\beta}_2$ using the second strategy
   and that from the first strategy are numerically identical.
2) the residuals from the regression of $\widetilde{\mathbf{Y}}$ on
   $\widetilde{\mathbf{X}}_2$ and the residuals from Equation
   (\ref{eq:mult-reg-2g}) are numerically identical.

** An understanding of the FWL theorem

The FWL theorem provides a mathematical statement of how the multiple
regression coefficients in $\hat{\boldsymbol{\beta}}_2$ capture the
effects of $\mathbf{X}_2$ on $\mathbf{Y}$, controlling for other
$\mathbf{X}$.

- Step 1 purges the effects of the regressors in $\mathbf{X}_1$ on the
  regressors in $\mathbf{X}_2$
- Step 2 purges the effects of the regressors in $\mathbf{X}_1$ on
  $\mathbf{Y}$. 
- Step 3 estimates the effect of the regressors in $\mathbf{X}_2$ on
  $\mathbf{Y}$ using the parts in $\mathbf{X}_2$ and
  $\mathbf{Y}$ that have excluded the effects of $\mathbf{X}_1$. 

** An example of the FWL theorem

Consider a regression model with single regressor
$Y_i = \beta_0 + \beta_1 X_i + u_i$. 

Following the estimation strategy in the FWL theorem, we can carry out
the following regressions,
1. Regress $Y_i$ on 1. That is, estimate the model
   $Y_i = \alpha + e_i$.
   Then, the OLS estimator of $\alpha$ is
   $\bar{Y}$ and the residuals is $y_i = Y_i - \bar{Y}$
2. Similarly, regress $X_{i}$ on 1. Then
   the residuals from this regression is $x_{i} = X_{i} -
   \bar{X}$.
3. Regress $y_i$ on $x_{i}$ without intercept. That is,
   estimate the model $y_i = \beta_1 x_{i} + v_i$
4. We can obtain $\hat{\beta_1}$ directly by applying the formula in
   Equation (\ref{eq:betahat-mult}). That is 
   \[ \hat{\beta}_1 =
   (\mathbf{x}_1^{\prime} \mathbf{x}_1)^{-1} \mathbf{x}_1^{\prime}
   \mathbf{y} = \frac{\sum_i x_{1i} y_i}{\sum_i x_{1i}^2} =
   \frac{\sum_i (X_i-\bar{X})(Y_i-\bar{Y})}{\sum_i(X_i-\bar{X})^2} 
   \]
   

* The Least Squares Assumptions in Multiple Regression
#+TOC: headlines [currentsection]

** The least squares assumptions in Multiple Regression

*** Assumption #1

$E(u_i | \mathbi{x}_i) = 0$. The conditional mean
of $u_i$ given $X_{1i}, X_{2i}, \ldots, X_{ki}$ has
mean of zero. This is the key assumption to assure
that the OLS estimators are unbiased.

*** Assumption #2

$(Y_i, \mathbi{x}_i^{\prime})\, i=1, \ldots, n$ are
i.i.d. This assumption holds automatically if the
data are collected by simple random sampling.

*** Assumption #3 

Large outliers are unlikely, i.e.,, $0 <
E(\mathbf{X}^4) < \infty$ and $0 < E(\mathbf{Y}^4)
< \infty$. That is, the dependent variables and
regressors have finite kurtosis.

*** Assumption #4 

No *perfect multicollinearity*. The regressors are said to exhibit
perfect multicollinearity if one of the regressor is a perfect linear
function of the other regressors.


* The Statistical Properties of the OLS Estimators in Multiple Regression
:PROPERTIES:
:BEAMER_env: quotation
:END:
#+TOC: headlines [currentsection]

** Unbiasedness and consistency

When all the least squares assumptions are true, especially, $E(u_i |
\mathbi{x}_i) = 0$, we can prove 

- $\hat{\boldsymbol{\beta}}$ is unbiased, that is,
  $E(\hat{\boldsymbol{\beta}}) = \boldsymbol{\beta}$. 

- $\hat{\boldsymbol{\beta}}$ is consistent, that is, as $n \rightarrow
  \infty$, $\hat{\boldsymbol{\beta}} \rarrowd{p} \boldsymbol{\beta}$. 

** The Gauss-Markov conditions and Theorem

*** The G-M conditions
The Gauss-Markov conditions for multiple regression are
1. $E(\mathbf{u} | \mathbf{X}) = 0$,
2. $\var(\mathbf{u} | \mathbf{X}) = E(\mathbf{uu}^{\prime} |
   \mathbf{X}) = \sigma^2_u \mathbf{I}_n$ (homoskedasticity),
3. $\mathbf{X}$ has full column rank (no perfect multicollinearity).

*** The G-M Theorem
If the Gauss-Markov conditions hold in the multiple regression model,
then the OLS estimator $\hat{\boldsymbol{\beta}}$ is more efficient
than any other linear unbiased estimator
$\tilde{\boldsymbol{\beta}}$. That is, the OLS estimator is BLUE.

** The conditional covariance matrix of $\hat{\boldsymbol{\beta}}$

*** The *homoskedasticity-only* covariance matrix.
\begin{equation}
\label{eq:varbhat-hm}
\var(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \sigma^2_u (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{equation}

*** The *heteroskedasticity-robust covariance matrix*
\begin{equation}
\label{eq:varbhat-ht}
\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \boldsymbol{\Sigma} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
\end{equation}
where $\boldsymbol{\Sigma} = \mathbf{X}^{\prime}
\boldsymbol{\Omega}\mathbf{X}$ and $\boldsymbol{\Omega} = \var(\mathbf{u} | \mathbf{X})$

** The asymptotic normal distribution

- With large samples, the OLS estimator $\hat{\boldsymbol{\beta}}$ has the
  multivariate normal asymptotic distribution as
  \begin{equation}
  \label{eq:normal-bhat-m}
  \hat{\boldsymbol{\beta}} \rarrowd{d} N(\boldsymbol{\beta}, \boldsymbol{\Sigma_{\hat{\boldsymbol{\beta}}}})
  \end{equation}

- $\boldsymbol{\Sigma_{\hat{\boldsymbol{\beta}}}} =
  \var(\hat{\boldsymbol{\beta}} | \mathbf{X})$. 
  - Use Equation (\ref{eq:varbhat-hm}) for the homoskedastic case
  - Use Equation (\ref{eq:varbhat-ht}) for the heteroskedastic case.

* The Omitted Variable Bias
#+TOC: headlines [currentsection]
** The definition of the omitted variable bias
:PROPERTIES:
:END:

 The *omitted variable bias* arises when two conditions are met
 1. The included regressors $\mathbf{X}$ is correlated with the
    omitted regressors, denoted as $\mathbf{Z}$.
 2. The omitted variables, $\mathbf{Z}$, are determinants of the
    dependent variable $\mathbf{Y}$.

** The reason for the omitted variable bias
:PROPERTIES:
:BEAMER_opt: shrink,plain
:END:
*** The true model
Suppose that the true model is
\begin{equation}
\label{eq:omb-1}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\gamma} + \mathbf{u}
\end{equation}
- We assume $E(\mathbf{u} | \mathbf{X}, \mathbf{Z}) = 0$. 
- The OLS estimators, $\hat{\boldsymbol{\beta}}$ and
  $\hat{\boldsymbol{\gamma}}$, from Equation eqref:eq:omb-1 are unbiased. 
- We also assume $\cov(\mathbf{X}, \mathbf{Z}) \neq 0$. 

*** The wrong model
\begin{equation}
\label{eq:omb-2}
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
\end{equation}

- $\boldsymbol{\epsilon}$ represents all other factors that are not
  in Equation (\ref{eq:omb-2}), including $\mathbf{Z}$
- $\cov(\mathbf{X}, \mathbf{Z}) \neq 0$ \Rightarrow $\cov(\mathbf{X},
  \boldsymbol{\epsilon}) \neq 0$ \Rightarrow
  $E(\boldsymbol{\epsilon} | \mathbf{X}) \neq 0$
- The OLS estimator, $\tilde{\boldsymbol{\beta}}$, from Equation
  eqref:eq:omb-2 is biased. 

** An illustration using a linear model with two regressors

- Suppose the true model is
  \[ Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + u_i,\; i=1,
  \ldots, n \]
  with $E(u_i | X_{1i}, X_{2i}) = 0$

- However, we estimate a wrong model of
  \[ Y_i = \beta_0 + \beta_1 X_{1i} + \epsilon_i,\; i=1, \ldots, n \]

** An illustration using a linear model with two regressors (cont'd)

- We can prove that $\beta_1$ can be expressed as
  \[ \hat{\beta}_1 = \beta_1 + \frac{\frac{1}{n}\sum_i
  (X_{1i} - \bar{X}_1) \epsilon_i}{\frac{1}{n}\sum_i (X_i - \bar{X}_1)^2} \]

- As $n \rightarrow \infty$, $\frac{1}{n}\sum_i
  (X_{1i} - \bar{X}_1) \epsilon_i \rarrowd{p} \cov(X_1, \epsilon) = \rho_{{\scriptscriptstyle X_1} \epsilon} \sigma_{\scriptscriptstyle X_1} \sigma_{\epsilon}$
  and $\frac{1}{n}\sum_i (X_i - \bar{X}_1)^2 \rarrowd{p}
  \sigma^2_{\scriptscriptstyle X_1}$. 

- We have the formula to quantify the omitted variable bias as
  \begin{equation}
  \label{eq:omb-4}
  \hat{\beta}_1 \rarrowd{p} \beta_1 + \underbrace{\rho_{x_1 \epsilon} \frac{\sigma_{\epsilon}}{\sigma_{x_1}}}_{\mathclap{\text{omitted variable bias}}}
  \end{equation}

** Some facts summarized from the formula

- Omitt variable bias is a problem irregardless of whether the sample
  size is large or small. 

- Whether this bias is large or small in practice depends on
  $|\rho_{X_{1} \epsilon}|$. 

- The direction of this bias is determined by the sign of $\rho_{X_{1}
  \epsilon}$. 

- One easy way to detect the existence of the omitted variable bias is
  that when adding a new regressor, the estimated coefficients on some
  previously included regressors change substantially.

* Multicollinearity
#+TOC: headlines [currentsection]

** Definition of perfect multicollinearity

*Perfect multicollinearity* refers to the situation when one of the
regressor is a perfect linear function of the other regressors.
- In the terminology of linear algebra, perfect multicollinearity
  means that the vectors of regressors are linearly dependent.
- That is, the vector of a regressor can be expressed as a linear
  combination of vectors of the other regressors.

** Understanding perfect multicollinearity

*** Linear dependence 

- Write the matrix of regressors $\mathbf{X}$ with column vectors
  \[
  \mathbf{X} = [\boldsymbol{\iota}, \boldsymbol{X}_1, \boldsymbol{X}_2, \ldots, \boldsymbol{X}_k ]
  \]
- That the $k+1$ column vectors are linearly dependent means that there
  exist some $(k+1) \times 1$ nonzero vector $\boldsymbol{\beta} =
  [\beta_0, \beta_1, \ldots, \beta_k]^{\prime}$ such that
  \[
  \beta_0 \boldsymbol{\iota} + \beta_1 \boldsymbol{X}_1 + \cdots + \beta_k
  \boldsymbol{X}_k = 0 
  \]

** Consequence of perfect multicollinearity

If $\boldsymbol{X}_i$ are linearly dependent, then
- $\mathbf{X}$ does not have full column rank.
- If $\mathbf{X}$ does not have full column rank, then
  $\mathbf{X}^{\prime} \mathbf{X}$ is singular.
- It means that the inverse
  of $\mathbf{X}^{\prime} \mathbf{X}$ does not exist. 
- If $\mathbf{X}^{\prime} \mathbf{X}$ is not invertible, the OLS
  estimator based on the formula of $\boldsymbol{\hat{\beta}} =
  (\mathbf{X}^{\prime} \mathbf{X})^{-1} \mathbf{X}^{\prime}
  \mathbf{Y}$ does not exist.

** Examples of perfect multicollinearity

Suppose we have a multiple regression model
\[ \mathbf{Y} = \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2
\mathbf{X}_2 + \mathbf{u}  \]

*** Cases that imply perfect multicollinearity
- $Z = a X_1$ or $Z = b X_2$
- $Z = 1 - a X_1$
- $Z = a X_1 + b X_2$

*** Cases that do not imply perfect multicollinearity
- $Z = X_1^2$
- $Z = \ln X_1$
- $Z = X_1 X_2$

** The dummy variable trap

- The dummy variable trap is a case of perfect multicollinearity
  that a modeler often encounters. 
- We use dummy variables to distinguish different groups of objects.
- Question: How many dummy variables should we include?

*** An example

- Four ethnic groups: White, African American, Hispanic, and
  Asian.
- We want to estimate a regression model to see whether wages among
  these four groups are different. 
- Suppose we have four observations: Chuck (White), 
  Mike (African American), Juan (Hispanic), and Li (Asian). Define
  dummy variables as
  \begin{equation*}
  White =
  \begin{pmatrix}
  1 \\
  0 \\
  0 \\
  0
  \end{pmatrix},\,
  African =
  \begin{pmatrix}
  0 \\
  1 \\
  0 \\
  0
  \end{pmatrix},\,
  Hispanic =
  \begin{pmatrix}
  0 \\
  0 \\
  1 \\
  0
  \end{pmatrix},\,
  Asian =
  \begin{pmatrix}
  0 \\
  0 \\
  0 \\
  1
  \end{pmatrix}
  \end{equation*}

** The wrong regression model 

- We set up a regression model as follows
  \begin{equation}
  \label{eq:dummy-trap}
  Wage_i = \beta_0 + \beta_1 White_i + \beta_2 African_i + \beta_3 Hispanic_i + \beta_4 Asian_i + u_i
  \end{equation}
- Dummy variable trap (perfect multicollinearity) occurs
  \begin{equation*}
  \begin{pmatrix}
  1 \\
  1 \\
  1 \\
  1 \\
  \end{pmatrix}
  = White + African + Hispanic + Asian
  \end{equation*}

** Remedy to dummy variable trap

To avoid the dummy variable trap, we can either of the following two
methods:
1. drop the constant term
2. drop one dummy variable
The difference between these two methods lies in how we interpret the
coefficients on dummy variables.

** Drop the constant term
:PROPERTIES:
:END:

If we drop the constant term, the model becomes
\begin{equation}
\label{eq:dummy-trap-1}
Wage = \beta_1 White + \beta_2 African + \beta_3 Hispanic + \beta_4 Asian + u
\end{equation}
For Chuck or all white people, the model becomes
\[ Wage = \beta_1 + u \]
Then $\beta_1$ is the population mean wage of whites, that is,
\[\beta_1 = E(Wage | White = 1)\]

Similarly, $\beta_2, \beta_3, \text{ and } \beta_4$ are the population mean wage
of African Americans, Hispanics, and Asians, respectively.

** Drop one dummy variable

If we drop the dummy variable for white people, then the model becomes
\begin{equation}
\label{eq:dummy-trap-2}
Wage = \beta_1 + \beta_2 African + \beta_3 Hispanic + \beta_4 Asian + u
\end{equation}
For white people, the model is
\[Wage = \beta_1 + u_i \]
And the constant term $\beta_1$ is just the population mean of
whites, that is,
\[\beta_1 = E(Wage | White = 1)\]
So we say that white people
serve as a reference case in Model (\ref{eq:dummy-trap-2}).

** Drop one dummy variable (cont'd)

For African Americans, the model is
\[ Wage = \beta_1 + \beta_2 + u  \]
From it we have $E(Wage | African=1) = \beta_1 + \beta_2$ so that
\[\beta_2 = E(Wage | African = 1) - \beta_1 = E(Wage | African = 1) -
E(Wage | White = 1)\]
Similarly, we can get that
\begin{align*}
\beta_3 &= E(Wage | Hispanic = 1) - E(Wage | White = 1) \\
\beta_4 &= E(Wage | Asian = 1) - E(Wage | White = 1)
\end{align*}

** Definition of imperfect multicollinearity

*Imperfect multicollinearity* is a problem of regression when two or
more regressors are highly correlated. 

\vspace{0.3cm}
Although they bear similar
names, imperfect multicollinearity and perfect multicollinearity are
two different concepts.
- Perfect multicollinearity is a problem of modeling building,
  resulting in a total failure to estimate a linear model.
- Imperfect multicollinearity is usually a problem of data in the
  sense that data for two variables are highly correlated.
- Imperfect multicollinearity does not affect the unbiasedness of the
  OLS estimators. However, it does affect the efficiency, i.e., the
  variance of the OLS estimators.

** An illustration using a regression model with two regressors

Suppose we have a linear regression model with two regressors
\begin{equation}
\label{eq:ex-collin}
\mathbf{Y} = \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \mathbf{u}
\end{equation}

- We can prove that 
  \[\var(\hat{\beta}_1 | \mathbf{X}) =
  \frac{\sigma^2_u}{\sum_i (X_{1i} - \bar{X})_1^2} \frac{1}{(1 - r^2_{12})}\]
  where $r_{12}$ is the correlation coefficient between $X_1$ and $X_2$. 

- When $X_1$ and $X_2$ are highly correlated, that is $r^2_{12}$ gets
  close to 1, then $\var(\hat{\beta}_1 | \mathbf{X})$ becomes very
  large.

** The consequence and detection of imperfect multicollinearity

- The consequence of imperfect multicollinearity is that I may more
  often fail to reject the null hypothesis of a zero coefficient with
  t-statistic. 

- The variance inflation factor (VIF) is a commonly used indicator for
  detecting multicollinearity. The definition is

  \begin{equation*}
  \mathrm{VIF} = \frac{1}{1 - r^2_{12}}
  \end{equation*}

  The smaller VIF is for a regressor, the less severe the problem of
  multicollinearity is. 

** The remedies to imperfect multicollinearity

- Include more sample in hope of the variation in $\mathbf{X}$ getting
  widened, i.e., increasing $\sum_i (X_{1i} - \bar{X}_1)^2$.

- Drop the variable(s) that is highly correlated with other
  regressors. Notice that by doing this we are at the risk of
  suffering the omitted variable bias. There is always a trade-off
  between including all relevant regressors and making the regression
  model /parsimonious/.
