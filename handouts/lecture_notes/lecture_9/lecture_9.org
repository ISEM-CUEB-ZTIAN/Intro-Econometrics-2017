#+TITLE: Lecture 9: Hypothesis Tests and Confidence Intervals in Multiple Regression
#+AUTHOR: Zheng Tian
#+DATE: 
#+OPTIONS: toc:nil H:3 num:2 tex:t todo:nil <:nil ^:{}
#+PROPERTY: header-args:R  :session my-r-session
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper,11pt]
#+LATEX_HEADER: \usepackage[margin=1.2in]{geometry}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{tabularx}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \hypersetup{colorlinks,citecolor=black,filecolor=black,linkcolor=black,urlcolor=black}
#+LATEX_HEADER: \newtheorem{mydef}{Definition}
#+LATEX_HEADER: \newtheorem{mythm}{Theorem}
#+LATEX_HEADER: \newcommand{\dx}{\mathrm{d}}
#+LATEX_HEADER: \newcommand{\var}{\mathrm{Var}}
#+LATEX_HEADER: \newcommand{\cov}{\mathrm{Cov}}
#+LATEX_HEADER: \newcommand{\corr}{\mathrm{Corr}}
#+LATEX_HEADER: \newcommand{\pr}{\mathrm{Pr}}
#+LATEX_HEADER: \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
#+LATEX_HEADER: \renewcommand\chaptername{Lecture}
#+LATEX_HEADER: \DeclareMathOperator*{\plim}{plim}
#+LATEX_HEADER: \newcommand{\plimn}{\plim_{n \rightarrow \infty}}

* Introduction

** Overview
This lecture presents the methods for testing the hypotheses
concerning the coefficients in a multiple regression model. Besides
the t-statistic that we have learned in Lecture 6, we introduce a new
test statistic, the F-statistic, which is used to test the joint
hypotheses that involve two or more coefficients. We will also learn
some basic ideas of assessing model specification.


** Learning goals
- Know how to test a hypothesis for a single coefficient using the
  t-statistic.
- Know how to test a joint hypotheses for more than one coefficients
  using the F-statistic.
- Understand the underlying ideas of the F-statistic, especially when
  using the homoskedasticity-only F-statistic.


** Reading materials
- Chapter 7 and Section 18.3 in /Introduction to Econometrics/ by
  Stock and Watson.


* Hypothesis Tests and Confidence Intervals For a Single Coefficient

We consider the general multiple regression model as follows
\begin{equation}
\label{eq:jnt-hyp-mod}
\mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \cdots + \beta_k \mathbf{X}_k + \mathbf{u}
\end{equation}
where $\mathbf{Y}, \mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_k, \text{ and } \mathbf{u}$ are $n
\times 1$ vectors of the dependent variable, regressors, and
errors. $\boldsymbol{\beta} = (\beta_0, \beta_1, \beta_2, \ldots,
\beta_k)^{\prime}$ is the $(k+1) \times 1$ vector of coefficients. And
$\boldsymbol{\iota}$ is the $n \times 1$ vector of 1s. 

** Standard errors for the OLS estimators

*** A review on $\var(\hat{\boldsymbol{\beta}}|X)$

Recall that in the last lecture, we concluded that the the covariance
matrix of the OLS estimators $\hat{\boldsymbol{\beta}}$ can take the
following forms:

- The homoskedasticity-only covariance matrix if $u_i$ is
  homoskedastic
   
  \begin{equation}
  \label{eq:varbhat-hm-1}
  \var(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \sigma^2_u (\mathbf{X}^{\prime} \mathbf{X})^{-1}
  \end{equation}
   
- The heteroskedasticity-robust covariance matrix if $u_i$ is
  heteroskedastic
   
  \begin{equation}
  \label{eq:varbhat-ht-1}
  \var_{\mathrm{h}}(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \boldsymbol{\Sigma} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
  \end{equation}
   
  where $\boldsymbol{\Sigma} = \mathbf{X}^{\prime} \boldsymbol{\Omega}
  \mathbf{X}$, and $\mathbf{\Omega} = \var(\mathbf{u} |
  \mathbf{X})$.

Also, we know that if the least squares assumptions hold,
$\hat{\boldsymbol{\beta}}$ has an asymptotic multivariate normal
distribution as

\begin{equation}
\label{eq:normal-bhat-m}
\hat{\boldsymbol{\beta}} \rarrowd{d} N(\boldsymbol{\beta}, \mathbf{\Sigma_{\hat{\boldsymbol{\beta}}}})
\end{equation}
 
where $\mathbf{\Sigma_{\hat{\boldsymbol{\beta}}}} =
\var(\hat{\boldsymbol{\beta}} | \mathbf{X})$ for which use
Equation (\ref{eq:varbhat-hm-1}) for the homoskedastic case and Equation
(\ref{eq:varbhat-ht-1}) for the heteroskedastic case.


*** The estimator of $\var(\hat{\boldsymbol{\beta}}|X)$ 

In practice, $\sigma^2_u$ and $\boldsymbol{\Sigma}$ are unknown so
that we need to estimate them using their sample counterparts.
- The estimator of $\sigma^2_u$ is

  \begin{equation}
  \label{eq:sigma2u}
  s^2_u = \frac{1}{n-k-1} \sum_{i=1}^n \hat{u}^2_i
  \end{equation}

  Thus, the estimator of the homoskedasticity-only covariance matrix
  is
   
  \begin{equation}
  \label{eq:hat-vbhat-hm}
  \widehat{\var(\hat{\boldsymbol{\beta}})} = s^2_u (\mathbf{X}^{\prime} \mathbf{X})^{-1}
  \end{equation}
    
- The estimator of $\boldsymbol{\Sigma}$ is
  $\widehat{\boldsymbol{\Sigma}}$ given by
   
  \begin{equation}
  \label{eq:Sigmahat}
  \widehat{\boldsymbol{\Sigma}} = \frac{n}{n-k-1} \sum_{i=1}^n
  \mathbf{X}_i \mathbf{X}_i^{\prime} \hat{u}^2_i
  \end{equation} 
  observation of $(k+1)$ regressors, including the constant term.

  Therefore, the heteroskedasticity-consistent (robust) covariance
  matrix estimator is
   
  \begin{equation}
  \label{eq:hat-vbhat-ht}
  \widehat{\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}})} = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \widehat{\boldsymbol{\Sigma}} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
  \end{equation}

- The estimator of $SE(\hat{\beta}_j)$

  Finally, we can get the standard error of $\hat{\beta}_j$ as the
  square root of the j^{th} diagonal element of
  $\widehat{\var(\hat{\boldsymbol{\beta}})}$ for homoskedasticity and
  $\widehat{\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}})}$ for
  heteroskedasticity. That is,

  - Homoskedasticity-only standard error: $SE(\hat{\beta}_j) =
    \left(\left[\widehat{\var(\hat{\boldsymbol{\beta}})}\right]_{(j,j)}\right)^{\frac{1}{2}}$

  - Heteroskedasticity-robust standard error: $SE(\hat{\beta}_j) =
    \left(\left[\widehat{\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}})}\right]_{(j,j)}\right)^{\frac{1}{2}}$


** The t-statistic

With $SE(\hat{\beta}_j)$ at hand, we can test if a single coefficient
$\beta_j$ takes on a specific value, $\beta_{j,0}$. A two-sided
hypothesis test suffices, that is,
\[ H_0:\, \beta_j = \beta_{j,0} \text{ vs. } H_1:\, \beta_j \neq
\beta_{j,0} \]
The basic ideas of hypothesis testing for a single coefficient in
multiple regression are the same as in single regression.  In this
two-sided test, we still use the t-statistic computed as
\[ t = \frac{\hat{\beta}_j - \beta_{j,0}}{SE(\hat{\beta}_j)} \]

Since $\boldsymbol{\hat{\beta}}$ has an asymptotic multivariate normal
distribution, $\hat{\beta}_j$ has an asymptotic normal
distribution. Under the null hypothesis that the true value of
$\beta_j$ is $\beta_{j,0}$, the t-statistic has a asymptotic standard
normal distribution in large samples. Therefore the p-value can still
be computed as
\[ \text{p-value} = 2\varPhi(-|t^{act}|) \]

The null hypothesis can be rejected at the 5% significant level when
the p-value is less than 0.05, or equivalently, if $|t^{act}| >
1.96$. (Replace the critical value with 1.64 at the 10% level and 2.58
at the 1% level.)


** Confidence intervals for a single coefficient

The confidence intervals for a single coefficient can be constructed
as before using the t-statistic.

Given large samples, a 95% two-sided confidence interval for the
coefficient $\beta_j$ is
\[ \left[\hat{\beta}_j - 1.96 SE(\hat{\beta}_j),\; \hat{\beta}_j +
1.96 SE(\hat{\beta}_j)\right] \]


** Application to test scores and the student-teacher ratio

*** The regression with two explanatory variables, /STR/ and /PctEL/

The regression of test has three estimated coefficients, the
intercept, the coefficient on /STR/ and the coefficient on
/PctEl/. The estimated model can be written in the following format
with the standard errors of the three coefficients reporeed in
parentheses them.
 
\begin{equation*}
\widehat{TestScore} = \underset{{\displaystyle (8.7)}}{686.0}
- \underset{{\displaystyle (0.43)}}{1.10} \times STR
- \underset{\displaystyle (0.031)}{0.650} \times PctEl
\end{equation*}
 
- We test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$. The t-statistic
  for this test can be computed as $t = (-1.10-0) / 0.43 = -2.54 <
  -1.96$, and the p-value is $2\Phi(-2.54) = 0.011 < 0.05$. Based on
  either the t-statistic or the p-value, we can reject the null
  hypothesis at the 5% level.

- The confidence interval that contains the
  true value of $\beta_1$ with a 95% probability can be computed as
  $-1.10 \pm 1.96 \times 0.43 = (-1.95, -0.26)$.

*** Adding expenditure per pupil to the equation

Now we add a new explanatory variable in the regression, /Expn/, that
is the expenditure per pupil in the district in thousands of dollars.
Note expenditure includes not only the spending on new computers,
maintenance, and other hardware but also the salaries paid to
teachers. So keep in mind that /Expn/ and /STR/ may be
correlated. The new OLS regression line is
\begin{equation*}
\widehat{TestScore} = \underset{{\displaystyle (15.5)}}{649.6}
- \underset{\displaystyle (0.48)}{0.29} \times STR
+ \underset{\displaystyle (1.59)}{3.87} \times Expn
- \underset{\displaystyle (0.032)}{0.656} \times PctEl
\end{equation*}
 

Let's see what's changed regarding /STR/ after /Expn/ is added.
- The magnitude of the coefficient on /STR/ decreases from 1.10 to
  0.29 after /Expn/ is added.
- The standard error of the coefficient on /STR/ increases from 0.43
  to 0.48 after /Expn/ is added.
- Consequently, in the new model, the t-statistic for the coefficient
  becomes $t = -0.29/0.48 = -0.60 > -1.96$ so that we cannot reject
  the zero hypothesis at the 5% level. (neither can we at the 10%
  level).

**** How can we interpret such changes?

- The decrease in the magnitude of the coefficient reflects that
  expenditure per pupil is an important factor that carry over most
  influence of student-teacher ratio on test scores. In other words,
  holding expenditure per pupil and the percentage of English-learners
  constant, reducing class sizes by hiring more teachers have only
  small effect on test scores. 

- The increase in the standard error reflects that /Expn/ and /STR/
  are correlated so that there is imperfect multicollinearity in this
  model. In fact, the correlation coefficient between the two
  variables is 0.48, which is relatively high.


* Tests of joint hypotheses

** The form of joint hypotheses involving more than one coefficients

Rewrite the multiple regression model here
\begin{equation}
\label{eq:fullmodel}
\mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \cdots + \beta_k \mathbf{X}_k + \mathbf{u}
\end{equation}
Since $\beta_0$ to $\beta_k$ can take any value without restrictions,
this model is referred to as the full model or *the unrestricted
model*.

*** Joint hypothesis: an illustration using two zero restrictions

Suppose we want to test whether the coefficients on the first two
regressors are zero. Then we can set up a joint hypothesis for these two
coefficients like the following
 \[ H_0:\, \beta_1 = 0, \beta_2 = 0, \text{ vs. }
H_1:\, \text{either } \beta_1 \neq 0 \text{ or } \beta_2 \neq 0 \text{
(or both)} \]
 
- This is a joint hypothesis because the two restrictions $\beta_1=0$
  and $\beta_2=0$ must hold at the same time. So if either of them is
  invalid, the null hypothesis is rejected as a whole.

- To test these two restrictions jointly requires that we use a
  single statistic to test these restrictions simultaneously.

- The null hypothesis of $\beta_1 = 0,\, \beta_2 = 0$ can be
  considered as two restrictions imposed on Equation
  (\ref{eq:fullmodel}). If the null hypothesis is true, we have a
  *restricted model*
  \begin{equation}
  \label{eq:restmodel-1}
  \mathbf{Y} = \beta_0 + \beta_3 \mathbf{X}_3 + \beta_4 \mathbf{X}_4 + \cdots + \beta_k \mathbf{X}_k + \mathbf{u}
  \end{equation}
  

*** Why not use t-statistic and test individual coefficients one at a time?

What if we test the joint null hypothesis using t-statistics for
$\beta_1$ and $\beta_2$ separately. That is, compute the t-statistics
$t_1$ for $\beta_1 = 0$ and $t_2$ for $\beta_2 = 0$. We call this
"one-at-a-time" testing procedure. For simplicity, we assume $t_1$ and
$t_2$ are independent.

We can show that the one-at-a-time procedure will commit a type I
error with a probability more than 5%.

- A type I error happens when the null hypothesis is rejected when it
  is true. The probability of committing a type I error is call the
  size of the test. We want to control the size to be small, so we set
  the significance level (the prespecified probability of a type I
  error) at 1%, 5%, or 10%.

- Using the one-at-a-time procedure, at the 5% significance level, we
  can reject the null hypothesis of $H_0: \beta_1 = 0 \text{ and }
  \beta_2 = 0$ when either $|t_1| > 1.96$ or $|t_2| > 1.96$ (or
  both). In other words, the null is not rejected only when both
  $|t_1| \leq 1.96$ and $|t_2| \leq 1.96$.

- Because the two t-statistics are assumed to be independent, it
  implies that
  \[\pr(|t_1| \leq 1.96 \text{ and } |t_2| \leq 1.96) = \pr(|t_1| \leq
  1.96) \times \pr(|t_2| \leq 1.96) = 0.95^2 = 90.25\%\]
  So the probability of rejecting the null when it is true is $1 -
  90.25\% = 9.75\%$.


*** More cases of joint hypothesis

**** Joint hypothesis involving one coefficient in each restriction

We can test whether the coefficients take some specific values.
\begin{align*}
&H_0: \beta_1 = \beta_{1,0},\ \beta_2 = \beta_{2,0},\ \ldots,\ \beta_q = \beta_{q,0} \text{ versus } \\
&H_1: \text{at least one restriction does not hold}
\end{align*}
 
Suppose that we are testing the joint zero hypotheses (i.e., $\beta_1
= \beta_2 = \cdots = \beta_q = 0$). This joint hypothesis imposes $q$
zero restrictions on the unrestricted model (Equation
(\ref{eq:fullmodel})) so that *the restricted model* is
\begin{equation}
\label{eq:restmodel-2}
\mathbf{Y} = \beta_0 + \beta_{q+1} \mathbf{X}_{q+1} + \beta_{q+2} \mathbf{X}_{q+2} + \cdots + \beta_k \mathbf{X}_k + \mathbf{u}
\end{equation}

**** Joint hypothesis involving multiple coefficients in each restriction

Besides testing the hypothesis like $\beta_j = \beta_{j,0}$, we can
also test *linear hypotheses* as follows,
\begin{equation*}
H_0:\, \beta_1 = \beta_2 \text{ vs. } H_1:\, \beta_1 \neq \beta_2
\end{equation*}
or
\begin{equation*}
H_0:\, \beta_1 + \beta_2 = 1 \text{ vs. } H_1:\, \beta_1 + \beta_2 \neq 1
\end{equation*}
or more generally,
\begin{align*}
&H_0: \beta_1 + \beta_2 = 0,\, 2\beta_2 + 4\beta_3 + \beta_4 = 3 \text{ vs. } \\
&H_1: \text{at least one restriction does not hold}
\end{align*}
 
All the null hypotheses above can be thought of being constructed
using a linear function of the coefficients. So we can refer to them
as linear hypotheses with regard to $\boldsymbol{\beta}$.


*** A general joint hypothesis using matrix notation

We can use a matrix form to represent all linear hypotheses regarding
the coefficients in Equation (\ref{eq:fullmodel}) as follows
 
\begin{equation}
\label{eq:jnt-hyp-g}
H_0:\, \mathbf{R}\boldsymbol{\beta} = \mathbf{r} \text{ vs. } H_1: \mathbf{R}\boldsymbol{\beta} \neq \mathbf{r}
\end{equation}
 
where $\mathbf{R}$ is a $q \times (k+1)$ matrix with the *full row rank*,
$\boldsymbol{\beta}$ represent the $k+1$ regressors, including
the intercept, and $\mathbf{r}$ is a $q \times 1$ vector of real
numbers.

For example
- For $H_0: \beta_1 = 0, \beta_2=0$
  \begin{equation*}
  \mathbf{R} =
  \bordermatrix{~ & \beta_0 & \beta_1 & \beta_2 & \beta_3 & \cdots & \beta_k \cr
  R1 & 0 & 1 & 0 & 0 & \cdots & 0 \cr
  R2 & 0 & 0 & 1 & 0 & \cdots & 0 \cr}
  \text{ and }
  \mathbf{r} =
  \begin{pmatrix}
  0 \\
  0
  \end{pmatrix}
  \end{equation*}

- For $H_0: \beta_1 + \beta_2 = 0,\, 2\beta_2 + 4\beta_3 + \beta_4 =
  3,\, \beta_1 = 2 \beta_3 + 1$
  \begin{equation*}
  \mathbf{R} =
  \bordermatrix{~ & \beta_0 & \beta_1 & \beta_2 & \beta_3 & \beta_4 & \cdots & \beta_k \cr
  R1 & 0 & 1 & 1 & 0 & 0 & \cdots & 0 \cr
  R2 & 0 & 0 & 2 & 4 & 1 & \cdots & 0 \cr
  R3 & 0 & 1 & 0 & -2 & 0 & \cdots & 0 \cr}
  \text{ and }
  \mathbf{r} =
  \begin{pmatrix}
  0 \\
  3 \\
  1
  \end{pmatrix}
  \end{equation*}


** The F-statistic
We can compute the F-statistic to test all joint hypotheses shown
above. Let's first review some properties of F distribution, which is
the probability distribution that the F-statistic follows under the
null hypothesis.

*** COMMENT Review of the F distribution

Let $W_1 \sim \chi^2(n_1)$, $W_2 \sim \chi^2(n_2)$, and $W_1$ and
$W_2$ are independent. Then the random variable
\[ F = \frac{W_1/n_1}{W_2/n_2}\]
has an F distribution with $(n_1, n_2)$ degrees of freedom, denoted as
$F \sim F(n_1, n_2)$

- If $t \sim t(n)$, then $t^2 \sim F(1, n)$
- As $n_2 \rightarrow \infty$, the $F(n_1, \infty)$ distribution is the
  same as the $\chi^2(n_1)$ distribution, divided by $n_1$. That is
  \[ \text{if } F \sim F(n_1, \infty),\, \text{ then } n_1 F \sim
  \chi^2(n_1), \]
  or
  \[ \text{if } x \sim \chi^2(m),\, \text{ then } \frac{x}{m} \sim
  F(m, \infty) \]

- The probability density function of the F distribution have the
  shape as in Figure [[fig:fdist]]
  #+NAME: fig:fdist
  #+CAPTION: The density function of F distribution
  #+ATTR_LATEX: :width 0.75\textwidth :placement
  [[file:img/fdist.png]]


*** The general form of the F-statistic for testing the null hypothesis $H_0:\, \mathbf{R}\boldsymbol{\beta} = \mathbf{r}$
 
\begin{equation}
\label{eq:ftest-gen}
F = \frac{1}{q}(\mathbf{R}\hat{\boldsymbol{\beta}} - \mathbf{r})^{\prime} \left[ \mathbf{R} \widehat{\var(\hat{\boldsymbol{\beta}})} \mathbf{R}^{\prime} \right]^{-1} (\mathbf{R}\hat{\boldsymbol{\beta}} - \mathbf{r})
\end{equation}
- $\hat{\boldsymbol{\beta}}$ is the estimated coefficients by OLS and
  $\widehat{\var(\hat{\boldsymbol{\beta}})}$ is the estimated covariance
  matrix.
  - For homoskedastic errors, we can compute
    $\widehat{\var(\hat{\boldsymbol{\beta}})}$ as in Equation (\ref{eq:hat-vbhat-hm})
  - For heteroskedastic errors, we can compute
    $\widehat{\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}})}$ as in
    Equation (\ref{eq:hat-vbhat-ht})

**** The F distribution, the critical value, and the p-value

If the least square assumptions hold, under the null hypothesis, the
F-statistic is asymptotically distributed as the $F_{q, \infty}$
distribution. That is, $F \overset{a}{\sim} F(q, \infty)$

The 5% critical value of the F distribution, $c_{\alpha}$, must
satisfy $\pr(F < c_{\alpha}) = 0.95$. In other words, the p-value of
the F test can be computed as $\pr(F > F^{act})$.

Note that we are computing the
critical value and the p-value using the F distribution as if I were
doing a one-sided test. This is because the F-statistic takes only
positive values and the F distribution function is defined only in the
domain of positive real numbers.

**** COMMENT Wald statistic

In fact we can define a wald statistic
 
  \begin{equation}
  \label{eq:wald-stat}
  W = (\mathbf{R}\hat{\boldsymbol{\beta}} - \mathbf{r})^{\prime} \left[ \mathbf{R} \widehat{\var(\hat{\boldsymbol{\beta}})} \mathbf{R}^{\prime} \right]^{-1} (\mathbf{R}\hat{\boldsymbol{\beta}} - \mathbf{r})
  \end{equation}
 
Since $F \sim F(q, \infty)$ and we know that $qF \sim \chi^2(q)$, we
then have $W \sim \chi^2(q)$.

Next we are going to show two special cases of the F-statistic, which can
help us better understand the meaning of F tests.


*** The F-statistic when $q=2$

When we test the null hypothesis of $H_0: \beta_1 = 0, \beta_2 = 0$
with the restricted model in Equation (\ref{eq:restmodel-1}), the
F-statistic for this test is
 
\begin{equation}
\label{eq:ftest-q2}
F = \frac{1}{2}\frac{t_1^2 + t_2^2 - 2 \hat{\rho}_{t_1,t_2}t_1t_2}{1 - \hat{\rho}_{t_1,t_2}^2}
\end{equation}
 
Equation (\ref{eq:ftest-q2}) is mostly for illustration purpose, which
shows how to use $t_1$ and $t_2$ in a joint hypothesis test.
- For simplicity, suppose $t_1$ and $t_2$ are independent so that
  $\hat{\rho}_{t_1,t_2} = 0$. Then $F = \frac{1}{2}(t_1^2 +
  t_2^2)$.
- Under the null hypothesis, both $t_1$ and $t_2$ have asymptotic
  standard normal distribution. Then $t^2_1 + t^2_2 \sim \chi^2(2)$.
- It follows that $F = \frac{1}{2}(t^2_1 + t^2_2) \sim F(2, \infty)$.
- The discussion about the F-statistic in Equation (\ref{eq:ftest-q2})
  will become complicated when $\hat{\rho}_{t_1,t_2} \neq 0$.


*** TODO COMMENT The homoskedasticity-only F-statistic

When the regressor errors are homoskedastic, then we can compute *the
homoskedasticity-only F-statistic* that bears more meaningful
implications for the F tests.

Suppose we test the restricted model with q restrictions in Equation
(\ref{eq:restmodel-2}) versus the unrestricted model in Equation
(\ref{eq:fullmodel}). Then the homoskedasticity-only F-statistic can
be computed as
 
\begin{equation}
\label{eq:ftest-hm}
F = \frac{(RSSR - USSR)/q}{USSR/(n-k-1)}
\end{equation}
 
where $RSSR$ is the sum of squared residuals of the restricted model
and $USSR$ is the sum of squared residuals of the unrestricted model.

Since both restricted and unrestricted models have the same
$\mathbf{Y}$, $TSS$ is the same for both models. Therefore, dividing
the numerator and the denominator in Equation (\ref{eq:ftest-hm}) by
$TSS$, we obtain another expression of the homoskedasticity-only
F-statistic in terms of $R^2$ as
\begin{equation}
\label{eq:ftest-hm-r}
F = \frac{(R^2_{unrestrict} - R^2_{restrict})/q}{(1 - R^2_{unrestrict})/(n-k-1)}
\end{equation}

Suppose that all least square assumptions hold, then we have
\[ F \sim F(q, n-k-1) \]

We can understand the meaning of the homoskedasticity-only F-statistic
by the following reasoning line
1. The unrestricted model have more regressors than the restricted
   model, on which the coefficients could be non-zero.
2. By the properties of the OLS estimation, $SSR$ will decrease whenever
   an additional regressor is included in the model and the
   coefficient on that new regressor is not zero.
3. In other words, given the same sample, $R^2$ in the unrestricted
   model will increase when a new regressor is added with a nonzero
   coefficient.
4. That means $RSSR \geq USSR$ and $R^2_{unrestrict} > R^2_{restrict}$
   are always true.
5. However, suppose that the null hypothesis is true. That is, the
   true model is really the restricted one.
6. Then, adding other regressors in the restricted model should not
   increase the explanatory power of the model by too much.
7. That means that $USSR$ cannot be too much smaller than $RSSR$, or
   $R^2_{unrestrict}$ cannot be too much larger than $R^2_{restrict}$
   if the null hypothesis is true.
8. That means $F$ should not be a large positive number under the null
   hypothesis.
9. If we compute an F-statistic that is large enough compared with a
   critical value at some significance level, then we can reject the
   null hypothesis.


** TODO COMMENT Transformation of joint hypothesis testing to single hypothesis testing
For some simple joint hypotheses, we can transform the model so that
tesing joint hypotheses is converted to testing a single
hypothesis. Consider the following model
 
\[ \mathbf{Y} = \beta_0 +
\beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \mathbf{u} \] And the
null hypothesis is \[ H_0:\, \beta_1 = \beta_2 \]
 
Then we can rewrite the model as
 
\begin{equation*}
\mathbf{Y} = \beta_0 + (\beta_1 - \beta_2) \mathbf{X}_1 + \beta_2 (\mathbf{X}_1 + \mathbf{X}_2) + \mathbf{u}
\end{equation*}
 
Define $\gamma = \beta_1 - \beta_2$ and $\mathbf{W} =
\mathbf{X}_1 + \mathbf{X}_2$. Then the original model becomes
 
\[ \mathbf{Y} = \beta_0 + \gamma \mathbf{X}_1 + \beta_2
\mathbf{W} + \mathbf{u} \]
 
Thus, instead of testing $\beta_1 - \beta_2 = 0$, we test $H_0: \gamma
= 0$ using the t-statistic computed from the transformed model.


** TODO COMMENT Application to test scores and the student-teacher ratio

We rewrite the estimated regression model of test scores against
the student-teacher ratio, expenditures per pupil, and the percentage of
English learners below.
 
\begin{equation*}
\widehat{TestScore} = \underset{{\displaystyle (15.5)}}{649.6}
- \underset{\displaystyle (0.48)}{0.29} \times STR
+ \underset{\displaystyle (1.59)}{3.87} \times Expn
- \underset{\displaystyle (0.032)}{0.656} \times PctEl,\, R^2 = 0.4366
\end{equation*}
 
The null hypothesis is $H_0:\, \beta_1 = 0,\,\text{ and } \beta_2 = 0$, and the
alternative hypothesis is $H_1:\, \beta_1 \neq 0\,\text{ or } \beta_2
\neq 0$.

- The heteroskedasticity-robust F statistic is 5.43, calculated by the
  computer program using the heteroskedasticity-consistent covariance
  matrix. The critical value of the $F_{2,\infty}$ distribution at the
  5% significance level is 3.00, and 4.61 at the 1% level. Since $F =
  5.43 > 4.61$, we can reject the null hypothesis saying that neither
  the student-teacher ratio nor expenditures per pupil have an effect
  on test scores, holding constant the percentage of English
  learners.

- The homoskedasticity-only F statistic. To compute the
  homoskedasticity-only F statistic, we need to estimate the
  restricted model by OLS, which yields
   
  \begin{equation*}
  \widehat{TestScore} = \underset{{\displaystyle (1.0)}}{664.7}
  - \underset{\displaystyle (0.032)}{0.671} \times PctEl,\, R^2 = 0.4149
  \end{equation*}
   
  Now we know that the unrestricted $R^2_{\text{unrestricted}}$ is
  0.4366, the restricted $R^2_{\text{restricted}}$ is 0.4149, the
  number of restrictions $q=2$, the number of observations $n = 420$,
  and the number of coefficients in the unrestricted model $k =
  3$. Then, the homoskedasticity-only F statistic is computed as
  
  \[F=\frac{(0.4366 - 0.4149)/2}{(1-0.4366)/(420-3-1)} = 8.01 \]
   
  Because 8.01 exceeds the 1% critical value of 4.61 from the
  $F_{2,\infty}$ distribution, the null hypothesis is rejected.


* TODO COMMENT Confidence Sets for multiple coefficients
** Definition
A *95% confidence set* for two or more coefficients is
- a set that contains the true population values of these coefficients
  in 95% of randomly drawn samples.
- Equivalently, the set of coefficient values that cannot be rejected
  at the 5% significance level.
** How to construct a confidence set
Suppose that we construct the confidence set for $\beta_1 =
\beta_{1,0}, \beta_2 = \beta_{2,0}$.

- Let $F_{\beta_1, \beta_2}$ be the heteroskedasticity-robust
  F-statistic computed according to Equation (\ref{eq:ftest-gen}). If the
  homoskedasticity assumption holds, then F-statistic can be computed
  based on Equation (\ref{eq:ftest-hm}).
- A 95% confidence set $=\{\beta_1, \beta_2:\, F_{\beta_1,\beta_2} <
  c_F\}$, where $c_F$ is the 5% critical value of the $F(2, \infty)$
  distribution, which is close to 3 in this case.
- This set has coverage rate 95% because the test on which it is based
  has the size of 5%. That is, 5% of the time, the test incorrectly
  rejects the null when the null is true, so 95% of the time it does
  not.
- Therefore the confidence set constructed as the nonrejected values
  contains the true value 95% of the time.
** The confidence set based on the F-statistic is an ellipse
According to Equation (\ref{eq:ftest-q2}), the confidence set for $\beta_1, \text{ and } \beta_2$ is
\begin{gather*}
\left\{ \beta_1, \beta_2:\, F = \frac{1}{2}\frac{t^2_1 + t^2_2 - 2 \hat{\rho}_{t_1,t_2}t_1t_2}{1 - \hat{\rho}_{t_1,t_2}^2} \leq 3 \right\}
\end{gather*}
Plugging the formula of $t_1$ and $t_2$, the F-statistic becomes
\begin{equation*}
F = \left[ \left(\frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}\right)^2 + \left(\frac{\hat{\beta}_2 - \beta_{2,0}}{SE(\hat{\beta}_2)}\right)^2 + 2 \hat{\rho}_{t_1,t_2}\left(\frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}\right) \left(\frac{\hat{\beta}_2 - \beta_{2,0}}{SE(\hat{\beta}_2)}\right) \right] \leq 3
\end{equation*}
which is an ellipse containing the pairs of values of $\beta_1$ and
$\beta_2$ that cannot be rejected using the F-statistic at the 5%
significance level. See Figure [[fig:fig-7-1]].

#+CAPTION: 95% Confidence Set for Coefficients on STR and Expn
#+NAME: fig:fig-7-1
[[file:img/fig-7-1.png]]


* TODO COMMENT Model specification for multiple regression
** Omitted variable bias in multiple regression
Omitted variable bias is the bias in the OLS estimator that arises
when one or more included regressors are correlated with an omitted
variable.

For omitted variable bias to arise, two things must be true:
1. At least one of the included regressors must be correlated with the
   omitted variable.
2. The omitted variable must be a determinant of the dependent
   variable, $Y$.

With omitted variable bias, the least square assumption $E(u | X) = 0$
does not hold any more. The OLS estimator $\hat{\beta}$ is biased
however large the sample size is.

** A conundrum regarding to the OLS estimation and omitted variable bias
We want to get an unbiased estimate of the effect on test scores of
changing class size, holding constant factors outside the school
committee’s control – such as outside learning opportunities (museums,
etc), parental involvement in education, etc.

If we could run an experiment, we would randomly assign students
(and teachers) to different sized classes.  Then $STR_i$ would be
independent of all the things that go into $u_i$, so $E(u_i|STR_i) = 0$ and
the OLS slope estimator in the regression of $TestScore_i$ on $STR_i$ will
be an unbiased estimator of the desired causal effect.

But with observational data, $u_i$ depends on additional factors
(museums, parental involvement, knowledge of English etc).
- If you can observe those factors (e.g. /PctEL/), then include them in
  the regression.
- But usually you can’t observe all these omitted causal factors
  (e.g. parental involvement in homework).  In this case, you can
  include *control variables* which are correlated with these omitted
  causal factors, but which themselves are not causal.

** The role of control variables in multiple regression
*** Definition
A control variable $W$ is a variable that is correlated with, and
controls for, an omitted causal factor in the regression of Y on X,
but which itself does not necessarily have a causal effect on Y.

A control variable is not the object of interest in the study; rather
it is a regressor included to hold constant factors that, if
neglected, could lead to the estimated causal effect of interest to
suffer from omitted variable bias.

*** The test score example
\[TestScore = \underset{(5.6)}{700.2} - \underset{(0.27)}{1.00}STR -
\underset{(0.033)}{0.122}PctEL - \underset{(0.024)}{0.547}LchPct,\,
\bar{R}^2 = 0.773 \]

Where $PctEL=$ percent English learners in the school district,
$LchPct=$ percent of students receiving a free/subsidized lunch.

- Which variable is the variable of interest? $STR$
- Which variables are control variables? Do they have causal
  components? What do they control for?
  - /PctEL/ probably has a direct causal effect (school is tougher if
    you are learning English!).  But it is also a control variable:
    immigrant communities tend to be less affluent and often have
    fewer outside learning opportunities, and /PctEL/ is correlated with
    those omitted causal variables.  /PctEL/ is both a possible causal
    variable and a control variable.
  - /LchPct/ might have a causal effect (eating lunch helps learning);
    it also is correlated with and controls for income-related outside
    learning opportunities.  /LchPct/ is both a possible causal variable
    and a control variable.

*** What makes an effective control variable?
- Three interchangeable statements about what makes
  an effective control variable:
  - An effective control variable is one which, when included in
    the regression, makes the error term uncorrelated with the variable of
    interest.
  - Holding constant the control variable(s), the variable of interest
    is “as if” randomly assigned.
  - Among individuals (entities) with the same value of the control
    variable(s), the variable of interest is uncorrelated with the
    omitted determinants of $Y$.

- Control variables need not be causal, and their coefficients
  generally do not have a causal interpretation.
  - Does the coefficient on /LchPct/ have a causal interpretation?  If
    so, then we should be able to boost test scores (by a lot! Do the
    math!) by simply eliminating the school lunch program, so that
    $LchPct = 0$. But it makes nonsense!

*** Conditional mean independence
We need a mathematical statement of what makes an effective control
variable. This condition is *conditional mean independence*: given the
control variable, the mean of $u_i$ doesn’t depend on the variable of
interest.

Let $X_i$ denote the variable of interest and $W_i$ denote the control
variable(s).  $W$ is an effective control variable if conditional mean
independence holds:
\[ E(u_i|X_i, W_i) = E(u_i|W_i) \]

If $W$ is a control variable, then conditional mean
independence replaces the first least square assumption requiring
$E(u_i | X_i, W_i) = 0$.

Consider the regression model
\[ Y = \beta_0 + \beta_1 X + \beta_2 W + u \]
where $X$ is the variable of interest and $W$ is an effective control
variable so that conditional mean independence holds. In addition,
suppose that the other least square assumptions (except for
homoskedasticity) hold. Then,
- $\beta_1$ has a causal interpretation.
- $\hat{\beta}_1$ is unbiased.
- The coefficient on the control variable, $\hat{\beta}_2$ is
  generally biased.

**** $\beta_1$ has a causal interpretation

The expected change in $Y$ resulting from a change in $X$, holding $W$
constant, is:
\begin{equation*}
\begin{split}
& E(Y|X = x + \Delta x, W = w) - E(Y|X = x, W = w) \\
&= \beta_0 + \beta_1(x + \Delta x) + \beta_2 w + E(u|X = x + \Delta x, W = w) \\
&\text{ } - \beta_0 + \beta_1 x + \beta_2 w + E(u|X = x, W = w) \\
&= \beta_1 \Delta x + \left( E(u|W = w) -  E(u|W = w) \right) \\
&= \beta_1 \Delta x
\end{split}
\end{equation*}
In the second equality, we use conditional mean independence $E(u|X =
x + \Delta x, W = w) = E(u|X = x, W = w) = E(u|W = w)$.

**** $\hat{\beta}_1$ is unbiased and $\hat{\beta}_2$ is biased

For convenience, suppose that $E(u|W) = \gamma_0 + \gamma_1 W$. Thus,
under conditional mean independence, we have
\[ E(u|X,W) = E(u|W) = \gamma_0 + \gamma_1 W \]
Let $v = u - E(u|W)$ so that
\[E(v|X, W) = E(u|X,W) - E(u|W) = 0 \]
Then, it follows that
\[ u = E(u|X,W) + v = \gamma_0 + \gamma_1 W + v \]
where $E(v|X,W) = 0$.

Then, the original model $Y = \beta_0 + \beta_1 X + \beta_2 W + u$
becomes
\begin{equation}
\begin{split}
Y &= \beta_0 + \beta_1 X + \beta_2 W + \gamma_0 + \gamma_1 W + v \notag \\
&= (\beta_0 + \gamma_0) + \beta_1 X + (\beta_2 + \gamma_1) W + v \notag \\
&= \delta_0 + \beta_1 X + \delta_2 W + v
\end{split}
\end{equation}
where $\delta_0 = \beta_0 + \gamma_0$ and $\delta_2 = \beta_2 +
\gamma_2$.

For the new model $Y = \delta_0 + \beta_1 X + \delta_2 W + v$, we can
conclude that
- because $E(v|X,W) = 0$, the new model satisfy the first least square
  assumption so that the OLS estimator of $\delta_0, \beta_1, \text{
  and } \delta_2$ are unbiased.
- because the regressors in both the original and new model are the
  same, the OLS coefficients in the original model satisfy
  $E(\hat{\beta}_1) = \beta_1$ and $E(\hat{\beta}_2) = \delta_2 \neq
  \beta_2$ in general.

In summary, if $W$ is such that conditional mean independence is
satisfied, then:
- The OLS estimator of the effect of interest, $\hat{\beta}_1$, is
  unbiased.
- The OLS estimator of the coefficient on the control variable,
  $\hat{\beta}_2$, is biased. This bias stems from the fact that the
  control variable is correlated with omitted variables in the error term, so
  that is subject to omitted variable bias.
- Thus, we cannot interpret $\hat{\beta}_2$ as measuring any causal
  effects of $W$ on $Y$.

** Model specification in theory and in practice
In theory, when data are available on the omitted variable, the
solution to omitted variable bias is to include the omitted variable
in the regression. In practice, however, deciding whether to include a
particular variable can be difficult and requires judgment.

The following steps are advocated to set up a regression model:
1. a core or base set of regressors should be chosen using a
   combination of expert judgment, economic theory, and knowledge of
   how data were collected. The regression using this base set of
   regressors is referred to as a *base specification*. This step
   involves the following consideration:
   1) identifying the variable of interest.
   2) thinking of the omitted causal effects that could result in omitted
      variable bias.
   3) including those omitted causal effects if you can or, if you
      can’t, include variables correlated with them that serve as
      control variables.  The control variables are effective if the
      conditional mean independence assumption plausibly holds.
2. Also specify a range of plausible *alternative model
   specifications*, which include additional candidate variables.
   1) If the estimates of the coefficients of interest are numerically
      similar across the alternative specifications, then this
      provides evidence that the estimates from your base
      specification are reliable.
   2) If the estimates of the coefficients of interest change
      substantially across specifications, this often provides
      evidence that the original specification had omitted variable
      bias.

** Interpreting the $R^2$ and the adjusted $R^2$ in practice
It is easy to fall into the trap of maximizing the $R^2$ and $\bar{R}^2$, but this
loses sight of our real objective, an unbiased estimator of the class
size effect.
- A high $R^2$ or $\bar{R}^2$ means that the regressors explain the variation in Y.
- A high $R^2$ or $\bar{R}^2$ does not mean that you have eliminated omitted variable bias.
- A high $R^2$ or $\bar{R}^2$ does not mean that you have an unbiased estimator of a causal effect ($\beta_1$).
- A high $R^2$ or $\bar{R}^2$ does not mean that the included
  variables are statistically significant. This must be determined
  using hypotheses tests.


* TODO COMMENT Analysis of the test score data set
The complete regression results are formally reported in Table 7.1.

#+CAPTION:
#+ATTR_LATEX: :width 0.95\textwidth
[[file:img/tab-7-1.png]]
