#+TITLE: Lecture 9: Hypothesis Tests and Confidence Intervals in Multiple Regression
#+AUTHOR: Zheng Tian
#+DATE: 
#+STARTUP: beamer
#+OPTIONS: toc:1 H:2
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [presentation,10pt]
#+BEAMER_THEME: CambridgeUS
#+BEAMER_COLOR_THEME: beaver
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 :ETC
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \newtheorem{mydef}{Definition}
#+LATEX_HEADER: \newtheorem{mythm}{Theorem}
#+LATEX_HEADER: \newcommand{\dx}{\mathrm{d}}
#+LATEX_HEADER: \newcommand{\var}{\mathrm{var}}
#+LATEX_HEADER: \newcommand{\cov}{\mathrm{cov}}
#+LATEX_HEADER: \newcommand{\corr}{\mathrm{corr}}
#+LATEX_HEADER: \newcommand{\pr}{\mathrm{Pr}}
#+LATEX_HEADER: \newcommand{\rarrowd}[1]{\xrightarrow{\text{ \textit #1 }}}
#+LATEX_HEADER: \DeclareMathOperator*{\plim}{plim}
#+LATEX_HEADER: \newcommand{\plimn}{\plim_{n \rightarrow \infty}}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{color}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \setlength{\parskip}{1em}


* COMMENT Introduction
** Introduction
:PROPERTIES:
:BEAMER_opt: shrink
:END:
*** Overview
This lecture presents the methods for testing the hypotheses
concerning the coefficients in a multiple regression model.

\vspace{0.2cm}
Besides the t-statistic that we have learned in Lecture 6, we
introduce a new test statistic, the F-statistic, which is used to test
the joint hypotheses that involve two or more coefficients.

\vspace{0.2cm}
We will also learn some basic ideas of assessing model specification.

*** Learning goals
- Know how to test a hypothesis for a single coefficient using the
  t-statistic.
- Know how to test a joint hypotheses for more than one coefficients
  using the F-statistic.
- Understand the underlying ideas of the F-statistic, especially when
  using the homoskedasticity-only F-statistic.
- Be able to undertake a complete regression analysis with estimation
  and hypothesis testing for a baseline model specification against
  some alternative model specifications.


* Hypothesis Tests and Confidence Intervals For a Single Coefficient
#+TOC: headlines [currentsection]
** The basic multiple regression model
Consider the following model
\begin{equation}
\label{eq:jnt-hyp-mod}
\mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \cdots + \beta_k \mathbf{X}_k + \mathbf{u}
\end{equation}
- $\mathbf{Y}, \mathbf{X}_1, \mathbf{X}_2, \ldots, \mathbf{X}_k, \text{ and } \mathbf{u}$ are $n
  \times 1$ vectors of the dependent variable, regressors, and
  errors
- $\beta_0, \beta_1, \beta_2, \ldots, \text{ and } \beta_k$ are
  parameters.
- $\boldsymbol{\iota}$ is the $n \times 1$ vector of 1s.

** Review of $\var(\hat{\boldsymbol{\beta}}|X)$

- The homoskedasticity-only covariance matrix if $u_i$ is
  homoskedastic
   
  \begin{equation}
  \label{eq:varbhat-hm-1}
  \var(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \sigma^2_u (\mathbf{X}^{\prime} \mathbf{X})^{-1}
  \end{equation}
   
- The heteroskedasticity-robust covariance matrix if $u_i$ is
  heteroskedastic
   
  \begin{equation}
  \label{eq:varbhat-ht-1}
  \var_{\mathrm{h}}(\hat{\boldsymbol{\beta}} | \mathbf{X}) = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \boldsymbol{\Sigma} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
  \end{equation}
   
  where $\boldsymbol{\Sigma} = \mathbf{X}^{\prime} \boldsymbol{\Omega}
  \mathbf{X}$, and $\boldsymbol{\Omega} = \var(\mathbf{u} |
  \mathbf{X})$.

** The multivariate normal distribution of $\hat{\mathbf{\beta}}$

We know that if the least squares assumptions hold,
$\hat{\boldsymbol{\beta}}$ has an asymptotic multivariate normal
distribution as
 
\begin{equation}
\label{eq:normal-bhat-m}
\hat{\boldsymbol{\beta}} \rarrowd{d} N(\boldsymbol{\beta}, \boldsymbol{\Sigma_{\hat{\boldsymbol{\beta}}}})
\end{equation}
 
where $\boldsymbol{\Sigma_{\hat{\boldsymbol{\beta}}}} =
\var(\hat{\boldsymbol{\beta}} | \mathbf{X})$ for which use
Equation (\ref{eq:varbhat-hm-1}) for the homoskedastic case and Equation
(\ref{eq:varbhat-ht-1}) for the heteroskedastic case.

** The estimator of $\var(\hat{\boldsymbol{\beta}}|X)$ 

*** The estimator of $\sigma^2_u$
   
  \begin{equation}
  \label{eq:sigma2u}
  s^2_u = \frac{1}{n-k-1} \sum_{i=1}^n \hat{u}^2_i
  \end{equation}
  Thus, the estimator of the homoskedasticity-only covariance matrix
  is
  \begin{equation}
  \label{eq:hat-vbhat-hm}
  \widehat{\var(\hat{\boldsymbol{\beta}})} = s^2_u (\mathbf{X}^{\prime} \mathbf{X})^{-1}
  \end{equation}

*** The estimator of $\boldsymbol{\Sigma}$
   
  \begin{equation}
  \label{eq:Sigmahat}
  \widehat{\boldsymbol{\Sigma}} = \frac{n}{n-k-1} \sum_{i=1}^n
  \mathbf{X}_i \mathbf{X}_i^{\prime} \hat{u}^2_i
  \end{equation}
  where $\mathbf{X}_i$ is the vector of the i^{th}
  observation of $(k+1)$ regressors. 
  The *heteroskedasticity-consistent (robust) covariance matrix estimator* is
  \begin{equation}
  \label{eq:hat-vbhat-ht}
  \widehat{\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}})} = \left(\mathbf{X}^{\prime} \mathbf{X}\right)^{-1} \widehat{\boldsymbol{\Sigma}} (\mathbf{X}^{\prime} \mathbf{X})^{-1}
  \end{equation}

** The estimator of $SE(\hat{\beta}_j)$

We can get the standard error of $\hat{\beta}_j$ as the
square root of the j^{th} diagonal element of
$\widehat{\var(\hat{\boldsymbol{\beta}})}$ for homoskedasticity and
$\widehat{\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}})}$ for
heteroskedasticity. That is,
- Homoskedasticity-only standard error: $SE(\hat{\beta}_j) =
  \left(\left[\widehat{\var(\hat{\boldsymbol{\beta}})}\right]_{(j,j)}\right)^{\frac{1}{2}}$
- Heteroskedasticity-robust standard error: $SE(\hat{\beta}_j) =
  \left(\left[\widehat{\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}})}\right]_{(j,j)}\right)^{\frac{1}{2}}$

** The t-statistic

We can perform a two-sided hypothesis test as
\[ H_0:\, \beta_j = \beta_{j,0} \text{ vs. } H_1:\, \beta_j \neq
\beta_{j,0} \]

- We still use the t-statistic, computed as
  $t = (\hat{\beta}_j - \beta_{j,0})/SE(\hat{\beta}_j)$,
  where $SE(\hat{\beta}_j)$ is the standard error of $\hat{\beta}_j$.

- Under the null hypothesis, we have, in large samples, $t \overset{a}{\sim} N(0, 1)$.
  Therefore, the p-value can still be computed as $2\varPhi(-|t^{act}|)$.

- The null hypothesis is rejected at the 5% significant level when
  the p-value is less than 0.05, or equivalently, if $|t^{act}| >
  1.96$. (Replace the critical value with 1.64 at the 10% level and 2.58
  at the 1% level.)

** Confidence intervals for a single coefficient

The confidence intervals for a single coefficient can be constructed
as before using the t-statistic.

\vspace{0.2cm}

Given large samples, a 95% two-sided confidence interval for the
coefficient $\beta_j$ is
\[ \left[\hat{\beta}_j - 1.96 SE(\hat{\beta}_j),\; \hat{\beta}_j +
1.96 SE(\hat{\beta}_j)\right] \]

** Application to test scores and the student-teacher ratio

The estimated model can be written as follows
\begin{equation*}
\widehat{TestScore} = \underset{{\displaystyle (8.7)}}{686.0}
- \underset{{\displaystyle (0.43)}}{1.10} \times STR
- \underset{\displaystyle (0.031)}{0.650} \times PctEl
\end{equation*}

- We test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$. The t-statistic
  for this test can be computed as $t = (-1.10-0) / 0.43 = -2.54 <
  -1.96$, and the p-value is $2\Phi(-2.54) = 0.011 < 0.05$. Based on
  either the t-statistic or the p-value, we can reject the null
  hypothesis at the 5% level.

- The confidence interval that contains the
  true value of $\beta_1$ with a 95% probability can be computed as
  $-1.10 \pm 1.96 \times 0.43 = (-1.95, -0.26)$.

** Adding expenditure per pupil to the equation

Now we add a new explanatory variable in the regression, /Expn/, that
is the expenditure per pupil in the district in thousands of dollars.
\begin{equation*}
\widehat{TestScore} = \underset{{\displaystyle (15.5)}}{649.6}
- \underset{\displaystyle (0.48)}{0.29} \times STR
+ \underset{\displaystyle (1.59)}{3.87} \times Expn
- \underset{\displaystyle (0.032)}{0.656} \times PctEl
\end{equation*}

- The magnitude of the coefficient on /STR/ decreases from 1.10 to
  0.29 after /Expn/ is added.
- The standard error of the coefficient on /STR/ increases from 0.43
  to 0.48 after /Expn/ is added.
- Consequently, in the new model, the t-statistic for the coefficient
  becomes $t = -0.29/0.48 = -0.60 > -1.96$ so that we cannot reject
  the zero hypothesis at the 5% level. (neither can we at the 10%
  level).

** How can we interpret such changes?

- The decrease in the magnitude of the coefficient reflects that
  expenditure per pupil is an important factor that carry over some
  influence of student-teacher ratio on test scores. 
  \vspace{0.2cm}

  In other words, holding expenditure per pupil and the percentage of
  English-learners constant, reducing class sizes by hiring more
  teachers have only small effect on test scores
  \vspace{0.2cm}

- The increase in the standard error reflects that /Expn/ and /STR/
  are correlated so that there is imperfect multicollinearity in this
  model. In fact, the correlation coefficient between the two
  variables is 0.48, which is relatively high.


* Tests of Joint Hypotheses
#+TOC: headlines [currentsection]
** The unrestricted model
Consider the following multiple regression model
\begin{equation}
\label{eq:fullmodel}
\mathbf{Y} = \beta_0 \boldsymbol{\iota} + \beta_1 \mathbf{X}_1 + \beta_2 \mathbf{X}_2 + \cdots + \beta_k \mathbf{X}_k + \mathbf{u}
\end{equation}
 
We call Equation (\ref{eq:fullmodel}) as the full model or *the
unrestricted model* because $\beta_0$ to $\beta_k$ can take any value
without restrictions. 

** Joint hypothesis: a case of two zero restrictions

- Question: Are the coefficients on the first two regressors zero? 

- Joint hypotheses
  \[ H_0:\, \beta_1 = 0, \beta_2 = 0, \text{ vs. }
  H_1:\, \text{either } \beta_1 \neq 0 \text{ or } \beta_2 \neq 0 \text{
  (or both)} \]

- This is a joint hypothesis because $\beta_1=0$ and $\beta_2=0$ must
  hold at the same time. So if either of them is invalid, the null
  hypothesis is rejected as a whole.

** The restricted model with two zero restrictions

- If the null hypothesis is true, we have   
  \begin{equation}
  \label{eq:restmodel-1}
  \mathbf{Y} = \beta_0 + \beta_3 \mathbf{X}_3 + \beta_4 \mathbf{X}_4 + \cdots + \beta_k \mathbf{X}_k + \mathbf{u}
  \end{equation}
  We call Equation eqref:eq:restmodel-1 as *the restricted model*
  because we impose two restrictions $\beta_1 = 0$ and $\beta_2 = 0$. 

- To test these two restrictions jointly means that we need to use a
  single statistic to test these restrictions simultaneously. That
  statistic is F-statistic. 
   
** Why not use t-statistic and test individual coefficients one at a time?

- Let us test the null hypothesis above using t-statistics for $\beta_1$
  and $\beta_2$ separately. That is, $t_1$ is the t-statistic for
  $\beta_1 = 0$ and $t_2$ is the t-statistic for $\beta_2 = 0$. \vspace{0.2cm}

- Compute the t-statistics $t_1$ for $\beta_1 = 0$ and $t_2$ for
  $\beta_2 = 0$. We call this "one-at-a-time" testing procedure.

** What's the problem with the one-at-a-time procedure

*** How can we reject the null hypothesis with this procedure?

  Using the one-at-a-time procedure, at the 5% significance level, we
  can reject the null hypothesis of $H_0: \beta_1 = 0 \text{ and }
  \beta_2 = 0$ when either $|t_1| > 1.96$ or $|t_2| > 1.96$ (or
  both). In other words, the null is not rejected only when both
  $|t_1| \leq 1.96$ and $|t_2| \leq 1.96$.

*** What is the probability of committing Type I error?

  Assume $t_1$ and $t_2$ to be independent. Then,
  \[\pr(|t_1| \leq 1.96 \text{ \& } |t_2| \leq 1.96) = \pr(|t_1| \leq
  1.96) \pr(|t_2| \leq 1.96) = 0.95^2 = 90.25\%\]

  So the probability of rejecting the null when it is true is $1 -
  90.25\% = 9.75\%$. We may reject the null hypothesis with a higher
  probability than what we have pre-specified with the significant
  level. 


** Joint hypothesis involving one coefficient for each restriction

*** q restrictions 
\begin{align*}
&H_0: \beta_1 = \beta_{1,0},\ \beta_2 = \beta_{2,0},\ \ldots,\ \beta_q = \beta_{q,0} \text{ versus } \\
&H_1: \text{at least one restriction does not hold}
\end{align*}
 
*** The restricted model

Suppose that we are testing the q zero hypotheses, that is, q
restrictions, $\beta_1 = \beta_2 = \cdots = \beta_q = 0$. The
restricted model is
\begin{equation}
\label{eq:restmodel-2}
\mathbf{Y} = \beta_0 + \beta_{q+1} \mathbf{X}_{q+1} + \beta_{q+2} \mathbf{X}_{q+2} + \cdots + \beta_k \mathbf{X}_k + \mathbf{u}
\end{equation}
 
** Joint linear hypotheses

Joint hypotheses include *linear hypotheses* like the followings

- 1 ::
  \begin{equation*}
  H_0:\, \beta_1 = \beta_2 \text{ vs. } H_1:\, \beta_1 \neq \beta_2
  \end{equation*}

- 2 ::
  \begin{equation*}
  H_0:\, \beta_1 + \beta_2 = 1 \text{ vs. } H_1:\, \beta_1 + \beta_2 \neq 1
  \end{equation*}

- 3 ::
  \begin{align*}
  &H_0: \beta_1 + \beta_2 = 0,\, 2\beta_2 + 4\beta_3 + \beta_4 = 3 \text{ vs. } \\
  &H_1: \text{at least one restriction does not hold}
  \end{align*}

** A general form of joint hypotheses

We can use a matrix form to represent all linear hypotheses regarding
the coefficients in Equation (\ref{eq:fullmodel}) as follows
 
\begin{equation}
\label{eq:jnt-hyp-g}
H_0:\, \mathbf{R}\boldsymbol{\beta} = \mathbf{r} \text{ vs. } H_1: \mathbf{R}\boldsymbol{\beta} \neq \mathbf{r}
\end{equation}
 
where $\mathbf{R}$ is a $q \times (k+1)$ matrix with the *full row rank*,
$\boldsymbol{\beta}$ represent the $k+1$ regressors, including
the intercept, and $\mathbf{r}$ is a $q \times 1$ vector of real
numbers.

** Examples of $\mathbf{R}\boldsymbol{\beta} = \mathbf{r}$
- For $H_0: \beta_1 = 0, \beta_2=0$
  \begin{equation*}
  \mathbf{R} =
  \bordermatrix{~ & \beta_0 & \beta_1 & \beta_2 & \beta_3 & \cdots & \beta_k \cr
  R1 & 0 & 1 & 0 & 0 & \cdots & 0 \cr
  R2 & 0 & 0 & 1 & 0 & \cdots & 0 \cr}
  \text{ and }
  \mathbf{r} =
  \begin{pmatrix}
  0 \\
  0
  \end{pmatrix}
  \end{equation*}
   
- For $H_0: \beta_1 + \beta_2 = 0,\, 2\beta_2 + 4\beta_3 + \beta_4 =
  3,\, \beta_1 = 2 \beta_3 + 1$
  \begin{equation*}
  \mathbf{R} =
  \bordermatrix{~ & \beta_0 & \beta_1 & \beta_2 & \beta_3 & \beta_4 & \cdots & \beta_k \cr
  R1 & 0 & 1 & 1 & 0 & 0 & \cdots & 0 \cr
  R2 & 0 & 0 & 2 & 4 & 1 & \cdots & 0 \cr
  R3 & 0 & 1 & 0 & -2 & 0 & \cdots & 0 \cr}
  \text{ and }
  \mathbf{r} =
  \begin{pmatrix}
  0 \\
  3 \\
  1
  \end{pmatrix}
  \end{equation*}
   
** COMMENT The F-statistic \\ \small Review of the F distribution
Let's first review some properties of F distribution, which is
the probability distribution that the F-statistic follows under the
null hypothesis. \vspace{0.2cm}

Let $W_1 \sim \chi^2(n_1)$, $W_2 \sim \chi^2(n_2)$, and $W_1$ and
$W_2$ are independent. Then the random variable
\[ F = \frac{W_1/n_1}{W_2/n_2}\]
has an F distribution with $(n_1, n_2)$ degrees of freedom, denoted as
$F \sim F(n_1, n_2)$

- If $t \sim t(n)$, then $t^2 \sim F(1, n)$
- As $n_2 \rightarrow \infty$, the $F(n_1, \infty)$ distribution is the
  same as the $\chi^2(n_1)$ distribution, divided by $n_1$. That is
  \[ \text{if } F \sim F(n_1, \infty),\, \text{ then } n_1 F \sim
  \chi^2(n_1), \]
  or
  \[ \text{if } x \sim \chi^2(m),\, \text{ then } \frac{x}{m} \sim
  F(m, \infty) \]

** COMMENT The probability density function of the F distribution
:PROPERTIES:
:BEAMER_opt:
:END:
#+NAME: fig:fdist
#+CAPTION: The density function of F distribution
#+ATTR_LATEX: :width \textwidth :placement
[[file:img/fdist.png]]

** The general form of the F-statistic

To test the null hypothesis
\[ H_0:\, \mathbf{R}\boldsymbol{\beta} = \mathbf{r} \]
 
we compute the F-statistic
\begin{equation}
\label{eq:ftest-gen}
F = \frac{1}{q}(\mathbf{R}\hat{\boldsymbol{\beta}} - \mathbf{r})^{\prime} \left[ \mathbf{R} \widehat{\var(\hat{\boldsymbol{\beta}})} \mathbf{R}^{\prime} \right]^{-1} (\mathbf{R}\hat{\boldsymbol{\beta}} - \mathbf{r})
\end{equation}
 
 - $\hat{\boldsymbol{\beta}}$ is the estimated coefficients by OLS and
   $\widehat{\var(\hat{\boldsymbol{\beta}})}$ is the estimated covariance
   matrix.
 - For homoskedastic errors, we can compute
   $\widehat{\var(\hat{\boldsymbol{\beta}})}$ as in Equation (\ref{eq:hat-vbhat-hm})
 - For heteroskedastic errors, we can compute
   $\widehat{\var_{\mathrm{h}}(\hat{\boldsymbol{\beta}})}$ as in
   Equation (\ref{eq:hat-vbhat-ht})

** The F distribution, the critical value, and the p-value

*** The F distribution
If the least square assumptions hold, under the null hypothesis, the
F-statistic is asymptotically distributed as F distribution with
degree of freedom $(q, \infty)$. That is,
\[ F \overset{a}{\sim} F(q, \infty) \]

*** The critical value and the p-value of F test.
The 5% critical value of the F test using F-statistic is $c_{\alpha}$
such that
\[\pr(F < c_{\alpha}) = 0.95\]
And the p-value of F test can be computed as
 \[ p\text{-value} = \pr(F > F^{act})\]

** The F-statistic when $q=2$

The F-statistic for testing the null hypothesis of $H_0: \beta_1 = 0,
\beta_2 = 0$ can be proved to take the following form,
 
\begin{equation}
\label{eq:ftest-q2}
F = \frac{1}{2}\frac{t_1^2 + t_2^2 - 2 \hat{\rho}_{t_1,t_2}t_1t_2}{1 - \hat{\rho}_{t_1,t_2}^2}
\end{equation}
 
- For simplicity, suppose $t_1$ and $t_2$ are independent so that
  $\hat{\rho}_{t_1,t_2} = 0$. Then $F = \frac{1}{2}(t_1^2 +
  t_2^2)$.
- Under the null hypothesis, both $t_1$ and $t_2$ have standard normal
  distribution asymptotically. Then $t^2_1 + t^2_2$ has a chi-squared
  distribution with 2 degrees of freedom.
- It follows that $F = \frac{1}{2}(t^2_1 + t^2_2)$ has asymptotically
  distributed as $F(2, \infty)$.
- The discussion about F-statistic in Equation (\ref{eq:ftest-q2})
  will become complicated when $\hat{\rho}_{t_1,t_2} \neq 0$.

** TODO COMMENT The homoskedasticity-only F-statistic
When regressor errors are homoskedastic, then we can compute *the
homoskedasticity-only F-statistic* that bears more meaningful
implications for the F tests.

\begin{align*}
H_0:&  \mathbf{Y} = \beta_0 + \beta_{q+1} \mathbf{X}_{q+1} + \beta_{q+2} \mathbf{X}_{q+2} + \cdots + \beta_k \mathbf{X}_k + \mathbf{u} \text{\footnotesize{ (the restricted model)}} \\
H_1:&  \mathbf{Y} = \beta_0 + \beta_{1} \mathbf{X}_{1} + \beta_{2} \mathbf{X}_{2} + \cdots + \beta_k \mathbf{X}_k + \mathbf{u} \text{\footnotesize{ (the unrestricted model)}}
\end{align*}

Then the homoskedasticity-only F-statistic can be computed as
\begin{equation}
\label{eq:ftest-hm}
F = \frac{(RSSR - USSR)/q}{USSR/(n-k-1)}
\end{equation}
where $RSSR$ is the sum of squared residuals of the restricted model
and $USSR$ is the sum of squared residuals of the unrestricted model.

** TODO COMMENT The homoskedasticity-only F-statistic (cont'd)
Since both restricted and unrestricted models have the same
$\mathbf{Y}$, $TSS$ is the same for both models. Therefore, dividing
the numerator and the denominator by $TSS$ and doing simple
manipulations, we get another expression of the homoskedasticity-only
F-statistic in terms of $R^2$ as
\begin{equation}
\label{eq:ftest-hm-r}
F = \frac{(R^2_{unrestrict} - R^2_{restrict})/q}{(1 - R^2_{unrestrict})/(n-k-1)}
\end{equation}

Suppose that all least square assumptions hold, then we have
\[ F \sim F(q, n-k-1) \]

** TODO COMMENT Understanding the homoskedasticity-only F-statistic
We can understand the meaning of the homoskedasticity-only F-statistic
by the following reasoning line
- The unrestricted model have more regressors than the restricted
  model.
- By the properties of the OLS estimation, $SSR$ will decrease whenever
  an additional regressor is included in the model and the
  coefficient on that new regressor is not zero.
- In other words, given the same sample, $R^2$ in the unrestricted
  model will increase when a new regressor is added with a nonzero
  coefficient.
- That means $RSSR \geq USSR$ and $R^2_{unrestrict} > R^2_{restrict}$
  are always true.

** TODO COMMENT Understanding the homoskedasticity-only F-statistic (cont'd)
- However, suppose that the null hypothesis is true. That is, the
  true model is really the restricted one.
- Then, adding other regressors in the model should not increase the
  explanatory power of the model by too much.
- That means that $USSR$ cannot be too much smaller than $RSSR$, or
  $R^2_{unrestrict}$ cannot be too much larger than $R^2_{restrict}$
  if the null hypothesis is true.
- That means $F$ should not be a large positive number under the null
  hypothesis.
- If we compute an F-statistic that is large enough compared with a
  critical value at some significance level, then we can reject the
  null hypothesis.

** TODO COMMENT Transformation of joint hypothesis testing to single hypothesis testing
:PROPERTIES:
:BEAMER_opt: shrink
:END:
For some simple joint hypotheses, we can transform the model so that a
joint hypothesis test is converted to a single hypothesis
test. Consider the following model
\[ \mathbf{Y} = \beta_0 + \beta_1 \mathbf{X}_1 + \beta_2
\mathbf{X}_2 + \mathbf{u} \]
And the null hypothesis is $H_0: \beta_1 = \beta_2$. Then we can rewrite the model as
\begin{equation*}
\mathbf{Y} = \beta_0 + (\beta_1 - \beta_2) \mathbf{X}_1 + \beta_2 (\mathbf{X}_1 + \mathbf{X}_2) + \mathbf{u}
\end{equation*}
Define $\gamma = \beta_1 - \beta_2$ and $\mathbf{W} =
\mathbf{X}_1 + \mathbf{X}_2$. Then the original model becomes
\[ \mathbf{Y} = \beta_0 + \gamma \mathbf{X}_1 + \beta_2
\mathbf{W} + \mathbf{u} \]
Thus, instead of testing $\beta_1 - \beta_2 = 0$, we test $H_0:\, \gamma
= 0$ using the t-statistic computed from the transformed model.

** TODO COMMENT Application to test scores and the student-teacher ratio
:PROPERTIES:
:BEAMER_opt:
:END:
We rewrite the estimated regression model of test scores against
the student-teacher ratio, expenditures per pupil, and the percentage of
English learners below.
 
\begin{equation*}
\widehat{TestScore} = \underset{{\displaystyle (15.5)}}{649.6}
- \underset{\displaystyle (0.48)}{0.29} \times STR
+ \underset{\displaystyle (1.59)}{3.87} \times Expn
- \underset{\displaystyle (0.032)}{0.656} \times PctEl,\, R^2 = 0.4366
\end{equation*}
 
The null hypothesis is $H_0:\, \beta_1 = 0,\,\text{ and } \beta_2 = 0$, and the
alternative hypothesis is $H_1:\, \beta_1 \neq 0\,\text{ or } \beta_2
\neq 0$.
- *The heteroskedasticity-robust F statistic* is 5.43, calculated by the
  computer program using the heteroskedasticity-consistent covariance
  matrix. The critical value of the $F_{2,\infty}$ distribution at the
  5% significance level is 3.00, and 4.61 at the 1% level. Since $F =
  5.43 > 4.61$, we can reject the null hypothesis saying that neither
  the student-teacher ratio nor expenditures per pupil have an effect
  on test scores, holding constant the percentage of English
  learners.

** TODO COMMENT Application to test scores and the student-teacher ratio (cont'd)
- *The homoskedasticity-only F statistic*. To compute the
  homoskedasticity-only F statistic, we need to estimate the
  restricted model by OLS, which yields
   
  \begin{equation*}
  \widehat{TestScore} = \underset{{\displaystyle (1.0)}}{664.7}
  - \underset{\displaystyle (0.032)}{0.671} \times PctEl,\, R^2 = 0.4149
  \end{equation*}
   

  Now we know that the unrestricted $R^2_{\text{unrestricted}}$ is
  0.4366, the restricted $R^2_{\text{restricted}}$ is 0.4149, the
  number of restrictions $q=2$, the number of observations $n = 420$,
  and the number of coefficients in the unrestricted model $k =
  3$. Then, the homoskedasticity-only F statistic is computed as
   
  \[F=\frac{(0.4366 - 0.4149)/2}{(1-0.4366)/(420-3-1)} = 8.01 \]
   
  Because 8.01 exceeds the 1% critical value of 4.61 from the
  $F_{2,\infty}$ distribution, the null hypothesis is rejected.

** TODO COMMENT Warm-up Exercises
:PROPERTIES:
:BEAMER_opt: shrink
:END:
*** The following linear hypothesis can be tested using the F-test with the exception of
- A) :: $\beta_2=1$ and $\beta_3 = \beta_4 / \beta_5$
- B) :: $\beta_2=0$
- C) :: $\beta_1 + \beta_2 = 1$ and $\beta_3 = -2\beta_4$
- D) :: $\beta_0 = \beta_1$ and $\beta_1 = 0$
\pause
Answer: A

*** 11) To test joint linear hypotheses in the multiple regression model, you need to
- A) :: compare the sums of squared residuals from the restricted and unrestricted model.
- B) :: use the heteroskedasticity-robust F-statistic.
- C) :: use several t-statistics and perform tests using the standard normal distribution.
- D) :: compare the adjusted R^{2} for the model which imposes the restrictions, and the unrestricted model.
\pause
Answer:  B

** TODO COMMENT Warm-up exercises
:PROPERTIES:
:BEAMER_opt: shrink
:END:
*** If you reject a joint null hypothesis using the F-test in a multiple hypothesis setting, then
- A) :: a series of t-tests may or may not give you the same conclusion.
- B) :: the regression is always significant.
- C) :: all of the hypotheses are always simultaneously rejected.
- D) :: the F-statistic must be negative.
\pause
Answer:  A
\pause
*** Let R^{2}_{unrestricted} and R^{2}_{restricted} be 0.4366 and 0.4149 respectively. The difference between the unrestricted and the restricted model is that you have imposed two restrictions. There are 420 observations and 3 regressors including the intercept. The homoskedasticity-only F-statistic in this case is
- A) :: 4.61
- B) :: 8.01
- C) :: 10.34
- D) :: 7.71
\pause
Answer:  B
\pause
*** The formulation $\boldsymbol{R\beta} = \boldsymbol{r}$ to test a hypotheses
- A) :: allows for restrictions involving both multiple regression coefficients and single regression coefficients.
- B) :: is F-distributed in large samples.
- C) :: allows only for restrictions involving multiple regression coefficients. 
- D) :: allows for testing linear as well as nonlinear hypotheses.
\pause
Answer:  A
\pause

** TODO COMMENT Check the critical value using the F table
*** F(q, \infty)                                                    :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+CAPTION: The table of critical values of the $F(q, \infty)$ distribution
#+ATTR_LATEX: :width 0.8\textwidth :placement
[[file:img/fdist.png]]

*** F(m, n)                                                         :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.5
:END:
#+CAPTION: The table of critical values at the 5% level of the $F(m, n)$ distribution
#+ATTR_LATEX: :width 0.8\textwidth :placement
[[file:img/Ftrunc.jpg]]

* TODO COMMENT Confidence Sets for multiple coefficients
#+TOC: headlines [currentsection]
** Confidence set: definition
A *95% confidence set* for two or more coefficients is
- a set that contains the true population values of these coefficients
  in 95% of randomly drawn samples.
- Equivalently, the set of coefficient values that cannot be rejected
  at the 5% significance level.
** How to construct a confidence set
Suppose that we construct the confidence set for $\beta_1 =
\beta_{1,0}, \beta_2 = \beta_{2,0}$.

- Let $F_{\beta_1, \beta_2}$ be the heteroskedasticity-robust or
  homoskedasticity-only F-statistic.
- A 95% confidence set is
  \[\{\beta_1, \beta_2:\, F_{\beta_1,\beta_2} < c_F\}\]
  where $c_F$ is the 5% critical value of the $F(2, \infty)$
  distribution, which is close to 3 in this case.
- This set has coverage rate 95% because the test on which it is based
  has the size of 5%. That is, 5% of the time, the test incorrectly
  rejects the null when the null is true, so 95% of the time it does
  not.
- Therefore the confidence set constructed as the nonrejected values
  contains the true value 95% of the time.
** The confidence set based on the F-statistic is an ellipse
According to Equation (\ref{eq:ftest-q2}), the confidence set for $\beta_1, \text{ and } \beta_2$ is
\begin{gather*}
\left\{ \beta_1, \beta_2:\, F = \frac{1}{2}\frac{t^2_1 + t^2_2 - 2 \hat{\rho}_{t_1,t_2}t_1t_2}{1 - \hat{\rho}_{t_1,t_2}^2} \leq 3 \right\}
\end{gather*}
Plugging the formula of $t_1$ and $t_2$, the F-statistic becomes
\begin{equation*}
F = \left[ \left(\frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}\right)^2 + \left(\frac{\hat{\beta}_2 - \beta_{2,0}}{SE(\hat{\beta}_2)}\right)^2 + 2 \hat{\rho}_{t_1,t_2}\left(\frac{\hat{\beta}_1 - \beta_{1,0}}{SE(\hat{\beta}_1)}\right) \left(\frac{\hat{\beta}_2 - \beta_{2,0}}{SE(\hat{\beta}_2)}\right) \right] \leq 3
\end{equation*}
which is an ellipse containing the pairs of values of $\beta_1$ and
$\beta_2$ that cannot be rejected using the F-statistic at the 5%
significance level. See Figure [[fig:fig-7-1]].
** The confidence set: an illustration
#+CAPTION: 95% Confidence Set for Coefficients on STR and Expn
#+NAME: fig:fig-7-1
[[file:img/fig-7-1.png]]

* TODO COMMENT Model Specification for Multiple Regression
#+TOC: headlines [currentsection]
** Omitted variable bias in multiple regression
Omitted variable bias is the bias in the OLS estimator that arises
when one or more included regressors are correlated with an omitted
variable.

\vspace{0.2cm}

For omitted variable bias to arise, two things must be true:
1. At least one of the included regressors must be correlated with the
   omitted variable.
2. The omitted variable must be a determinant of the dependent
   variable, $Y$.

\vspace{0.2cm}

With omitted variable bias, the least square assumption $E(u | X) = 0$
does not hold any more. The OLS estimator $\hat{\beta}$ is biased
no matter how large the sample size is.

** A dilemma regarding to the OLS estimation and omitted variable bias
We want to get an unbiased estimate of the effect on test scores of
changing class size, holding constant factors outside the school
committee’s control – such as outside learning opportunities (museums,
etc), parental involvement in education, etc.

\vspace{0.2cm}

If we could run an experiment, we would randomly assign students
(and teachers) to different sized classes.  Then $STR_i$ would be
independent of all the things that go into $u_i$, so $E(u_i|STR_i) = 0$ and
the OLS slope estimator in the regression of $TestScore_i$ on $STR_i$ will
be an unbiased estimator of the desired causal effect.

** A dilemma, cont'd
But with observational data, $u_i$ depends on additional factors
(museums, parental involvement, knowledge of English etc).
- If you can observe those factors (e.g. /PctEL/), then include them in
  the regression.
- But usually you can’t observe all these omitted causal factors
  (e.g. parental involvement in homework).  In this case, you can
  include *control variables* which are correlated with these omitted
  causal factors, but may not have causal implication.

** Control variables: definition
A control variable $W$ is a variable that is correlated with, and
controls for, an omitted causal factor in the regression of Y on X,
but which itself does not necessarily have a causal effect on Y.

\vspace{0.3cm}

A control variable is not the object of interest in the study; rather
it is a regressor included to hold constant factors that, if
neglected, could lead to the estimated causal effect of interest to
suffer from omitted variable bias.

** The test score example
\[TestScore = \underset{(5.6)}{700.2} - \underset{(0.27)}{1.00}STR -
\underset{(0.033)}{0.122}PctEL - \underset{(0.024)}{0.547}LchPct,\,
\bar{R}^2 = 0.773 \]

Where $PctEL=$ percent English learners in the school district,
$LchPct=$ percent of students receiving a free/subsidized lunch.

- Which variable is the variable of interest? $STR$
- Which variables are control variables? Do they have causal
  components? What do they control for?

** The test score example, cont'd
 - /PctEL/ probably has a direct causal effect (school is tougher if
   you are learning English!).  But it is also a control variable:
   immigrant communities tend to be less affluent and often have
   fewer outside learning opportunities, and /PctEL/ is correlated with
   those omitted causal variables.  /PctEL/ is both a possible causal
   variable and a control variable.
 - /LchPct/ might slightly have a causal effect (eating lunch helps
   learning); But it is correlated with and controls for
   income-related outside learning opportunities.  /LchPct/ is more
   serving as a proxy variable for income than bearing causal effect
   itself.

** What makes an effective control variable?
*** Three interchangeable statements about what makes an effective control variable:
- An effective control variable is one which, when included in
  the regression, makes the error term uncorrelated with the variable of
  interest.
- Holding constant the control variable(s), the variable of interest
  is “as if” randomly assigned.
- Among individuals (entities) with the same value of the control
  variable(s), the variable of interest is uncorrelated with the
  omitted determinants of $Y$.

*** Control variables need not be causal, and their coefficients generally do not have a causal interpretation.
- Does the coefficient on /LchPct/ have a causal interpretation?  If
  so, then we should be able to boost test scores (by a lot! Do the
  math!) by simply eliminating the school lunch program, so that
  $LchPct = 0$. But it makes nonsense. 

** Conditional mean independence: definition
We need a mathematical statement of what makes an effective control
variable. This condition is *conditional mean independence*: given the
control variable, the mean of $u_i$ doesn’t depend on the variable of
interest.

\vspace{0.3cm}

Let $X_i$ denote the variable of interest and $W_i$ denote the control
variable(s).  $W$ is an effective control variable if conditional mean
independence holds:
\[ E(u_i|X_i, W_i) = E(u_i|W_i) \]

If $W$ is a control variable, then conditional mean
independence replaces the first least square assumption requiring
$E(u_i | X_i, W_i) = 0$.

** Implications of conditional mean independence
Consider the regression model
\[ Y = \beta_0 + \beta_1 X + \beta_2 W + u \]
where $X$ is the variable of interest and $W$ is an effective control
variable so that conditional mean independence holds. In addition,
suppose that the other least square assumptions hold. Then,
- $\beta_1$ has a causal interpretation.
- $\hat{\beta}_1$ is unbiased.
- The coefficient on the control variable, $\hat{\beta}_2$ is in
  general biased.

** $\beta_1$ has a causal interpretation
The expected change in $Y$ resulting from a change in $X$, holding $W$
constant, is:
\begin{equation*}
\begin{split}
& E(Y|X = x + \Delta x, W = w) - E(Y|X = x, W = w) \\
&= \beta_0 + \beta_1(x + \Delta x) + \beta_2 w + E(u|X = x + \Delta x, W = w) \\
&\text{ } - \beta_0 + \beta_1 x + \beta_2 w + E(u|X = x, W = w) \\
&= \beta_1 \Delta x + \left( E(u|W = w) -  E(u|W = w) \right) \\
&= \beta_1 \Delta x
\end{split}
\end{equation*}
In the second equality, we use conditional mean independence $E(u|X =
x + \Delta x, W = w) = E(u|X = x, W = w) = E(u|W = w)$.

** $\hat{\beta}_1$ is unbiased and $\hat{\beta}_2$ is biased
For convenience, suppose that $E(u|W) = \gamma_0 + \gamma_1 W$. Thus,
under conditional mean independence, we have
\[ E(u|X,W) = E(u|W) = \gamma_0 + \gamma_1 W \]
Let $v = u - E(u|W)$ so that
\[E(v|X, W) = E(u|X,W) - E(u|W) = 0 \]
Then, it follows that
\[ u = E(u|X,W) + v = \gamma_0 + \gamma_1 W + v \]

Then, the original model $Y = \beta_0 + \beta_1 X + \beta_2 W + u$
becomes
\begin{equation}
\begin{split}
Y &= \beta_0 + \beta_1 X + \beta_2 W + \gamma_0 + \gamma_1 W + v \notag \\
&= (\beta_0 + \gamma_0) + \beta_1 X + (\beta_2 + \gamma_1) W + v \notag \\
&= \delta_0 + \beta_1 X + \delta_2 W + v
\end{split}
\end{equation}
where $\delta_0 = \beta_0 + \gamma_0$ and $\delta_2 = \beta_2 +
\gamma_2$.

** $\hat{\beta}_1$ is unbiased and $\hat{\beta}_2$ is biased (cont'd)
*** For the new model $Y = \delta_0 + \beta_1 X + \delta_2 W + v$, we can conclude that
- because $E(v|X,W) = 0$, the new model satisfy the first least square
  assumption so that the OLS estimator of $\delta_0, \beta_1, \text{
  and } \delta_2$ are unbiased.
- because the regressors in both the original and new model are the
  same, the OLS coefficients in the original model satisfy
  $E(\hat{\beta}_1) = \beta_1$ and $E(\hat{\delta}_2) = \delta_2 \neq
  \beta_2$ in general.

*** In summary, if $W$ is such that conditional mean independence is satisfied, then:
- The OLS estimator of the effect of interest, $\hat{\beta}_1$, is
  unbiased.
- The OLS estimator of the coefficient on the control variable,
  $\hat{\beta}_2$, is biased. Thus, we cannot interpret
  $\hat{\beta}_2$ as measuring any causal effects of $W$ on $Y$. 

** Model specification in theory and in practice
:PROPERTIES:
:BEAMER_opt: shrink
:END:
The following steps are advocated to set up a regression model:
- A core or base set of regressors should be chosen using a
  combination of expert judgment, economic theory, and knowledge of
  how data were collected. The regression using this base set of
  regressors is referred to as a *base specification*. This step
  involves the following consideration:
  1) identifying the variable of interest.
  2) thinking of the omitted causal effects that could result in omitted
     variable bias.
  3) including those omitted causal effects if you can. If you
     can’t, include variables correlated with them that serve as
     control variables.
- Also specify a range of plausible *alternative model
  specifications*, which include additional candidate variables.
  1) If the estimates of the coefficients of interest are numerically
     similar across the alternative specifications, then this
     provides evidence that the estimates from your base
     specification are reliable.
  2) If the estimates of the coefficients of interest change
     substantially across specifications, this often provides
     evidence that the original specification had omitted variable
     bias.

** Interpreting the $R^2$ and the adjusted $R^2$ in practice
It is easy to fall into the trap of maximizing the $R^2$ and $\bar{R}^2$, but this
loses sight of our real objective, an unbiased estimator of the class
size effect.
- A high $R^2$ or $\bar{R}^2$ means that the regressors explain the variation in Y.
- A high $R^2$ or $\bar{R}^2$ does not mean that you have eliminated omitted variable bias.
- A high $R^2$ or $\bar{R}^2$ does not mean that you have an unbiased estimator of a causal effect ($\beta_1$).
- A high $R^2$ or $\bar{R}^2$ does not mean that the included
  variables are statistically significant. This must be determined
  using hypotheses tests.
- To test the overall significance of the regression model, we use the
  F-test with the null $H_0:\, \beta_1 = \cdots = \beta_k = 0$, that
  is, testing all coefficients on regressors excluding the intercept
  to be jointly significant. 

** Analysis of the test score data set
:PROPERTIES:
:BEAMER_opt: shrink
:END:
#+CAPTION:
#+ATTR_LATEX: :width 0.95\textwidth
[[file:img/tab-7-1.png]]
